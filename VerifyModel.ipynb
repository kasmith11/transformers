{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca8cd0a9",
   "metadata": {},
   "source": [
    "# Verifying the SpeechT5 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5342da",
   "metadata": {},
   "source": [
    "I needed to do the following to be able to load the original model:\n",
    "\n",
    "- Clone the https://github.com/microsoft/SpeechT5 repo\n",
    "\n",
    "Install stuff:\n",
    "\n",
    "```\n",
    "pip install editdistance\n",
    "pip install -U sacrebleu==1.5.1\n",
    "\n",
    "git submodule update --init SpeechT5/fairseq\n",
    "cd SpeechT5\n",
    "pip install --editable fairseq/\n",
    "pip install espnet\n",
    "```\n",
    "\n",
    "Put this notebook at the same level as the `SpeechT5` repo.\n",
    "\n",
    "Hack the code:\n",
    "\n",
    "- Copy `speecht5/tasks/speecht5.py` into `fairseq/fairseq/tasks`\n",
    "\n",
    "- To run on CPU: In `speecht5/sequence_generator.py`, comment out where it does `.to(device=\"cuda\")`\n",
    "\n",
    "Additional stuff to download:\n",
    "\n",
    "- `dict.txt` from https://drive.google.com/uc?export=download&id=19hcQ58RHZ6CssxF8Qp6yEF1NW_AXxObK\n",
    "\n",
    "- `tokenizer` from https://drive.google.com/uc?export=download&id=1wClgQjXXoU2lmpbaEa1v2SqMbg7cAutq\n",
    "\n",
    "- `speecht5_base_asr.pt` and `t5_transformer_lm.pt` from https://huggingface.co/ajyy/SpeechT5\n",
    "\n",
    "You also need an input audio file, any WAV at 16 kHz will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1a4d7c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65661728",
   "metadata": {},
   "source": [
    "Set Python path so it can find the `speecht5` and `fairseq` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f30402f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../SpeechT5/SpeechT5\")\n",
    "sys.path.insert(0, \"../SpeechT5/SpeechT5/fairseq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d398e605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ksmith/anaconda3/envs/SpeechT5/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b45926",
   "metadata": {},
   "source": [
    "## Load audio and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c18f317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/AUDIO_DIR/dev_clean/1272/141231/1272-141231-0020.flac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4b474ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/AUDIO_DIR/dev_clean/1272/128104/1272-128104-0000.flac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a855d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((69120,), 16000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "wav_data, cur_sample_rate = sf.read(input_file)\n",
    "wav_data.shape, cur_sample_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e60de",
   "metadata": {},
   "source": [
    "NOTE: The `Wav2Vec2FeatureExtractor` does not make sure the audio file is mono. If it has shape `(2, length)` or even `(1, length)` then the output from the feature extractor is incorrect!\n",
    "\n",
    "The `do_normalize` option is False for the SpeechT5 ASR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9699780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(do_normalize=False, return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ea3da78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 69120])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = feature_extractor(wav_data, sampling_rate=cur_sample_rate, padding=True, return_tensors=\"pt\")\n",
    "inputs[\"input_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1a94286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing padding mask\n",
    "inputs[\"attention_mask\"][:, 40000:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62e522e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 2))\n",
    "plt.plot(inputs[\"input_values\"][0])\n",
    "plt.plot(inputs[\"attention_mask\"][0])\n",
    "plt.ylim(-1, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f594f3",
   "metadata": {},
   "source": [
    "Transform multiple inputs into a single padded batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbb99431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128632,), 16000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file2 = \"/Users/matthijs/Documents/FILES/HuggingFace/S2S/textless/AUDIO_DIR/selfdestruct.wav\"\n",
    "wav_data2, cur_sample_rate2 = sf.read(input_file2)\n",
    "wav_data2.shape, cur_sample_rate2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1daee2cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128632])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs2 = feature_extractor([wav_data, wav_data2], sampling_rate=cur_sample_rate, padding=True, return_tensors=\"pt\")\n",
    "inputs2[\"input_values\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb653d87",
   "metadata": {},
   "source": [
    "The original model used a `padding_mask` as input, where False means no padding. The `Wav2Vec2FeatureExtractor` can return an `attention_mask`, where 1 means no padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d3ab7e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs2[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7163b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs2   # use the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2205900b",
   "metadata": {},
   "source": [
    "## Load the Transformers model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc8c0f8",
   "metadata": {},
   "source": [
    "To convert the original checkpoint weights to Transformers:\n",
    "\n",
    "First download the checkpoint. I used `speecht5_base_asr.pt` from https://huggingface.co/ajyy/SpeechT5\n",
    "\n",
    "Then run the following, using your own `--checkpoint_path` and `--pytorch_dump_folder_path`:\n",
    "\n",
    "```nohighlight\n",
    "cd transformers/src/transformers/models/speecht5\n",
    "\n",
    "python convert_speecht5_original_pytorch_checkpoint_to_pytorch.py \\\n",
    "  --task s2t \\\n",
    "  --checkpoint_path /path/to/SpeechT5/speecht5_base_asr.pt \n",
    "  --pytorch_dump_folder_path /some/other/path\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "552fd307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    SpeechT5Config, \n",
    "    SpeechT5CTCTokenizer,\n",
    "    SpeechT5Processor,\n",
    "    SpeechT5Model, \n",
    "    SpeechT5ForConditionalGeneration, \n",
    "    SpeechT5ForCTC, \n",
    "    Wav2Vec2FeatureExtractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69aa6aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SpeechT5Config()\n",
    "hf_model = SpeechT5Model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ba8f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_base_asr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5df4bac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model = SpeechT5ForConditionalGeneration.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b7dd7606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeechT5ForConditionalGeneration(\n",
       "  (speecht5): SpeechT5Model(\n",
       "    (encoder): SpeechT5EncoderWithSpeechPrenet(\n",
       "      (prenet): SpeechT5SpeechEncoderPrenet(\n",
       "        (feature_encoder): SpeechT5FeatureEncoder(\n",
       "          (conv_layers): ModuleList(\n",
       "            (0): SpeechT5GroupNormConvLayer(\n",
       "              (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "              (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "            )\n",
       "            (1): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (2): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (3): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (4): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (5): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (6): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (feature_projection): SpeechT5FeatureProjection(\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (pos_conv_embed): SpeechT5PositionalConvEmbedding(\n",
       "          (conv): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "          (padding): SpeechT5SamePadLayer()\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (pos_sinusoidal_embed): SpeechT5SinusoidalPositionalEmbedding()\n",
       "      )\n",
       "      (wrapped_encoder): SpeechT5Encoder(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (embed_positions): SpeechT5RelativePositionalEncoding(\n",
       "          (pe_k): Embedding(320, 64)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): SpeechT5DecoderWithTextPrenet(\n",
       "      (prenet): SpeechT5TextDecoderPrenet(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (embed_tokens): Embedding(81, 768, padding_idx=1)\n",
       "        (embed_positions): SpeechT5SinusoidalPositionalEmbedding()\n",
       "      )\n",
       "      (wrapped_decoder): SpeechT5Decoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (text_decoder_postnet): SpeechT5TextDecoderPostnet(\n",
       "    (lm_head): Linear(in_features=768, out_features=81, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "09c1c4b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_base_asr were not used when initializing SpeechT5Model: ['speecht5.encoder.prenet.feature_projection.projection.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.6.conv.weight', 'speecht5.encoder.prenet.feature_projection.projection.bias', 'speecht5.encoder.prenet.feature_encoder.conv_layers.5.conv.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.1.conv.weight', 'speecht5.encoder.prenet.pos_sinusoidal_embed.weights', 'speecht5.encoder.prenet.masked_spec_embed', 'speecht5.encoder.prenet.feature_encoder.conv_layers.2.conv.weight', 'text_decoder_postnet.lm_head.weight', 'speecht5.decoder.prenet.embed_positions.weights', 'speecht5.encoder.prenet.feature_projection.layer_norm.bias', 'speecht5.encoder.prenet.feature_encoder.conv_layers.0.layer_norm.bias', 'speecht5.encoder.prenet.feature_encoder.conv_layers.0.conv.weight', 'speecht5.encoder.prenet.pos_conv_embed.conv.weight_v', 'speecht5.encoder.prenet.feature_encoder.conv_layers.4.conv.weight', 'speecht5.decoder.prenet.embed_tokens.weight', 'speecht5.encoder.prenet.pos_conv_embed.conv.weight_g', 'speecht5.encoder.prenet.feature_encoder.conv_layers.0.layer_norm.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.3.conv.weight', 'speecht5.encoder.prenet.feature_projection.layer_norm.weight', 'speecht5.encoder.prenet.pos_conv_embed.conv.bias']\n",
      "- This IS expected if you are initializing SpeechT5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SpeechT5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Loading should work OK for class `SpeechT5Model` too:\n",
    "hf_model_naked = SpeechT5Model.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7628cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the attention layer weights are correct\n",
    "# for i in range(len(hf_model.speecht5.encoder.layers)):\n",
    "#     print(i, \"k_proj weight\", torch.all(hf_model.speecht5.encoder.layers[i].attention.k_proj.weight == orig_model.encoder.layers[i].self_attn.k_proj.weight))\n",
    "#     print(i, \"k_proj bias\", torch.all(hf_model.speecht5.encoder.layers[i].attention.k_proj.bias == orig_model.encoder.layers[i].self_attn.k_proj.bias))\n",
    "#     print(i, \"v_proj weight\", torch.all(hf_model.speecht5.encoder.layers[i].attention.v_proj.weight == orig_model.encoder.layers[i].self_attn.v_proj.weight))\n",
    "#     print(i, \"v_proj bias\", torch.all(hf_model.speecht5.encoder.layers[i].attention.v_proj.bias == orig_model.encoder.layers[i].self_attn.v_proj.bias))\n",
    "#     print(i, \"q_proj weight\", torch.all(hf_model.speecht5.encoder.layers[i].attention.q_proj.weight == orig_model.encoder.layers[i].self_attn.q_proj.weight))\n",
    "#     print(i, \"q_proj bias\", torch.all(hf_model.speecht5.encoder.layers[i].attention.q_proj.bias == orig_model.encoder.layers[i].self_attn.q_proj.bias))\n",
    "#     print(i, \"out_proj weight\", torch.all(hf_model.speecht5.encoder.layers[i].attention.out_proj.weight == orig_model.encoder.layers[i].self_attn.out_proj.weight))\n",
    "#     print(i, \"out_proj bias\", torch.all(hf_model.speecht5.encoder.layers[i].attention.out_proj.bias == orig_model.encoder.layers[i].self_attn.out_proj.bias))\n",
    "#     print(\"---\")\n",
    "\n",
    "# print(\"pos_emb weight\", torch.all(hf_model.speecht5.encoder.pos_emb.pe_k.weight == orig_model.encoder.pos_emb.pe_k.weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82233c52",
   "metadata": {},
   "source": [
    "Run a single forward pass. This should run the encoder, decoder, and the relevant pre- and postnets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "29bd2f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.encoder.prenet(**inputs)\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7e1f8501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using no attention_mask\n",
    "# with torch.no_grad():\n",
    "#      hf_outputs = hf_model.speech_encoder_prenet(input_values=inputs[\"input_values\"])\n",
    "\n",
    "# type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "359778a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "97a77055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_outputs[\"extract_features\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7db34f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_outputs[\"extract_features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "41afd19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_outputs[\"hidden_states\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "fcef1339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_outputs[\"hidden_states\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d01d920d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 215, 768]), torch.Size([1, 215])]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in hf_outputs if hasattr(x, \"shape\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2d5ff106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.4147, -3.6930, 10.6119,  ...,  1.3639,  2.2972,  0.8101],\n",
       "         [ 8.1118, -4.2742, 12.2622,  ...,  0.1729,  1.6734,  0.6476],\n",
       "         [-2.8165,  2.4560, -1.3008,  ...,  0.9983, -0.4333,  2.5262],\n",
       "         ...,\n",
       "         [ 7.7907, -6.4359,  5.6191,  ...,  0.0352,  3.1737,  0.4928],\n",
       "         [-0.1691, -4.2425,  6.3816,  ..., -1.6755,  1.3843,  0.2407],\n",
       "         [ 2.0105, -3.1966,  7.5800,  ..., -0.5673,  1.7153,  0.5722]]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d24920f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_encoder_input = hf_outputs[0]\n",
    "hf_encoder_attention_mask = hf_outputs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64823660",
   "metadata": {},
   "source": [
    "## Load the original model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1697ab0",
   "metadata": {},
   "source": [
    "Load the dictionary. This adds `<s>, <pad>, </s>, <unk>` tokens to the front and `<mask>` and `<ctc_blank>` to the end. **dict.txt** was [downloaded from here](https://drive.google.com/uc?export=download&id=19hcQ58RHZ6CssxF8Qp6yEF1NW_AXxObK). This is the Vocabulary link from the main SpeechT5 README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24e0f251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size: 81\n"
     ]
    }
   ],
   "source": [
    "from fairseq.data import Dictionary\n",
    "tgt_dict = Dictionary.load(\"/home/ksmith/HuggingFace/SpeechT5/dict.txt\")\n",
    "tgt_dict.add_symbol(\"<mask>\")\n",
    "tgt_dict.add_symbol(\"<ctc_blank>\")\n",
    "print(f\"dictionary size: \" f\"{len(tgt_dict):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85935250",
   "metadata": {},
   "source": [
    "To load the model we need the `SpeechT5Task` object but constructing it is annoying. Fortunately, `build_model` only reads two properties from the task object, so we can fake it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea4b4380",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeTask:\n",
    "    def __init__(self):\n",
    "        self.dicts = { \"text\": tgt_dict }\n",
    "        self.t5_task = \"s2t\"\n",
    "        \n",
    "task = FakeTask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dc6c7a",
   "metadata": {},
   "source": [
    "Load the fine-tuned ASR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a79fbfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from speecht5.models.speecht5 import T5TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9b2818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"../weights/SpeechT5/speecht5_base_asr.pt\")\n",
    "\n",
    "orig_model = T5TransformerModel.build_model(checkpoint[\"cfg\"][\"model\"], task)\n",
    "\n",
    "orig_model.load_state_dict(checkpoint[\"model\"])\n",
    "orig_model = orig_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bda7a201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speecht5.models.speecht5.T5TransformerModel"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(orig_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64d53b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speecht5.models.modules.encoder.TransformerEncoder"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(orig_model.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bad44136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(checkpoint[\"model\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c1faddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fairseq.data.encoders.sentencepiece_bpe.SentencepieceBPE"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fairseq.data import encoders\n",
    "from argparse import Namespace\n",
    "tokenizer = encoders.build_bpe(\n",
    "    Namespace(\n",
    "        bpe='sentencepiece', \n",
    "        sentencepiece_model='/home/ksmith/HuggingFace/SpeechT5/spm_char.model'\n",
    "    )\n",
    ")\n",
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61dd5619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig_model.decoder.layers[0].encoder_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56503f04",
   "metadata": {},
   "source": [
    "## Verify speech encoder prenet output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e2f9ff",
   "metadata": {},
   "source": [
    "This first uses the `speech_encoder_prenet` to convert the raw audio data into embeddings of shape `(batch, sequence_length, 768)`. The sequence length is roughly `number of audio samples / 320`, so there is one vector every 20 ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01638115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 69120])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = inputs[\"input_values\"]\n",
    "source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35b0a089",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_mask = torch.BoolTensor(source.shape).fill_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "94586252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ...,  True,  True,  True]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask = torch.BoolTensor((1 - inputs[\"attention_mask\"]).bool())\n",
    "padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "374cf769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This doesn't work on the original model\n",
    "#padding_mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31d1d2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input, encoder_padding_mask = orig_model.speech_encoder_prenet(\n",
    "    source, padding_mask=padding_mask, mask=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36334be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_input = orig_model.speech_encoder_prenet.feature_extractor(source)\n",
    "# encoder_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26c4a65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 215, 768]), torch.Size([1, 215]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input.shape, encoder_padding_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "058de538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.4147, -3.6930, 10.6119,  ...,  1.3639,  2.2972,  0.8101],\n",
       "         [ 8.1118, -4.2742, 12.2622,  ...,  0.1729,  1.6734,  0.6476],\n",
       "         [-2.8165,  2.4560, -1.3008,  ...,  0.9983, -0.4333,  2.5262],\n",
       "         ...,\n",
       "         [ 7.7907, -6.4359,  5.6191,  ...,  0.0352,  3.1737,  0.4928],\n",
       "         [-0.1691, -4.2425,  6.3816,  ..., -1.6755,  1.3843,  0.2407],\n",
       "         [ 2.0105, -3.1966,  7.5800,  ..., -0.5673,  1.7153,  0.5722]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c07884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dbc6dd",
   "metadata": {},
   "source": [
    "If the weights and model were converted correctly, this should report zero or a very small number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b099ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.max(torch.abs(encoder_input - hf_outputs[\"hidden_states\"]))\n",
    "torch.max(torch.abs(encoder_input - hf_outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5533cb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f82c2ac1c70>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.imshow(torch.abs(encoder_input - hf_outputs[\"hidden_states\"]).numpy()[0] > 1e-5)\n",
    "plt.imshow(torch.abs(encoder_input - hf_outputs[0]).numpy()[0] > 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bf93df",
   "metadata": {},
   "source": [
    "The line that is different is where the padding mask goes from 1 to 0; the original model handles this a little different than we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b92b758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.max(torch.abs(encoder_input - hf_outputs[\"extract_features\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d18286fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.abs(encoder_input - hf_outputs[\"last_hidden_state\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b76dc23",
   "metadata": {},
   "source": [
    "## Verify Transformer encoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "47c86f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it with the original's speech prenet input:\n",
    "# with torch.no_grad():\n",
    "#     encoder_output = orig_model.encoder(encoder_input, encoder_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8b8b4df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it with our input, which is slightly different (see above)\n",
    "with torch.no_grad():\n",
    "    encoder_output = orig_model.encoder(hf_encoder_input, ~hf_encoder_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "41ff76cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_out shape torch.Size([215, 1, 768])\n",
      "encoder_padding_mask shape torch.Size([1, 215])\n",
      "encoder_states []\n",
      "src_tokens []\n",
      "decoder_input [None]\n",
      "encoder_out_for_ctc shape torch.Size([215, 1, 81])\n"
     ]
    }
   ],
   "source": [
    "print(\"encoder_out shape\", encoder_output[\"encoder_out\"][0].shape)\n",
    "print(\"encoder_padding_mask shape\", encoder_output[\"encoder_padding_mask\"][0].shape)\n",
    "print(\"encoder_states\", encoder_output[\"encoder_states\"])  # []\n",
    "print(\"src_tokens\", encoder_output[\"src_tokens\"])  # []\n",
    "print(\"decoder_input\", encoder_output[\"decoder_input\"])  # [None]\n",
    "print(\"encoder_out_for_ctc shape\", encoder_output[\"encoder_out_for_ctc\"][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a38979d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3807, -0.1815, -0.5548,  ...,  0.3614, -0.8483,  0.2265],\n",
       "         [-0.3834, -0.1535, -0.6543,  ...,  0.2798, -0.9170,  0.1041],\n",
       "         [-0.4408, -0.1220, -0.7244,  ...,  0.2857, -0.8230, -0.0304],\n",
       "         ...,\n",
       "         [-0.4740, -0.2646, -0.1816,  ...,  0.2346, -0.3932,  0.2151],\n",
       "         [-0.4529, -0.3011, -0.0495,  ...,  0.1883, -0.3702,  0.3084],\n",
       "         [-0.4112, -0.3328, -0.2101,  ...,  0.2243, -0.3563,  0.1865]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[\"encoder_out\"][0].permute((1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f9b9dda4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -9.6748,  -9.7322,  -9.7790,  ...,  -9.7432,  -9.6151,  13.2914],\n",
       "         [-10.0331, -10.0065, -10.1315,  ..., -10.1021, -10.0029,  13.9544],\n",
       "         [-11.2495, -11.2236, -11.2750,  ..., -11.3193, -11.2077,  15.5090],\n",
       "         ...,\n",
       "         [-11.1690, -11.4812, -11.2688,  ..., -11.4284, -11.0229,   8.8438],\n",
       "         [-10.9783, -11.2885, -11.0654,  ..., -11.2347, -10.7493,   8.8235],\n",
       "         [-10.5381, -10.8706, -10.7067,  ..., -10.7471, -10.3071,   8.5327]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[\"encoder_out_for_ctc\"][0].permute((1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cde6dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use exact same inputs as the original model:\n",
    "# with torch.no_grad():\n",
    "#      hf_outputs = hf_model.speecht5(\n",
    "#          inputs_embeds=encoder_input,\n",
    "#          attention_mask=(~encoder_padding_mask),\n",
    "#      )\n",
    "\n",
    "# type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "28f529a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutput"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.encoder.wrapped_encoder(\n",
    "         hidden_states=hf_encoder_input,\n",
    "         attention_mask=hf_encoder_attention_mask,\n",
    "#          input_values=inputs.input_values,\n",
    "#          attention_mask=inputs.attention_mask,\n",
    "         return_dict=True,\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fb12e71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-0.3807, -0.1815, -0.5548,  ...,  0.3614, -0.8483,  0.2265],\n",
       "         [-0.3834, -0.1535, -0.6543,  ...,  0.2798, -0.9170,  0.1041],\n",
       "         [-0.4408, -0.1220, -0.7244,  ...,  0.2857, -0.8230, -0.0304],\n",
       "         ...,\n",
       "         [-0.4740, -0.2646, -0.1816,  ...,  0.2346, -0.3932,  0.2151],\n",
       "         [-0.4529, -0.3011, -0.0495,  ...,  0.1883, -0.3702,  0.3084],\n",
       "         [-0.4112, -0.3328, -0.2101,  ...,  0.2243, -0.3563,  0.1865]]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "06fb675b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_hidden_state']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8fe9914e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 215, 768])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "69eda6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3807, -0.1815, -0.5548,  ...,  0.3614, -0.8483,  0.2265],\n",
       "         [-0.3834, -0.1535, -0.6543,  ...,  0.2798, -0.9170,  0.1041],\n",
       "         [-0.4408, -0.1220, -0.7244,  ...,  0.2857, -0.8230, -0.0304],\n",
       "         ...,\n",
       "         [-0.4740, -0.2646, -0.1816,  ...,  0.2346, -0.3932,  0.2151],\n",
       "         [-0.4529, -0.3011, -0.0495,  ...,  0.1883, -0.3702,  0.3084],\n",
       "         [-0.4112, -0.3328, -0.2101,  ...,  0.2243, -0.3563,  0.1865]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a747e547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(encoder_output[\"encoder_out\"][0].permute((1, 0, 2)) - hf_outputs[\"last_hidden_state\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7518e152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca490a88",
   "metadata": {},
   "source": [
    "## Verify CTC model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98055be9",
   "metadata": {},
   "source": [
    "This model only needs the encoder portion.\n",
    "\n",
    "This uses the same checkpoint as before: `speecht5_base_asr.pt` from https://huggingface.co/ajyy/SpeechT5\n",
    "\n",
    "Run the following to convert, using your own `--checkpoint_path` and `--pytorch_dump_folder_path`:\n",
    "\n",
    "```nohighlight\n",
    "cd transformers/src/transformers/models/speecht5\n",
    "\n",
    "python convert_speecht5_original_pytorch_checkpoint_to_pytorch.py \\\n",
    "  --task ctc \\\n",
    "  --checkpoint_path /path/to/SpeechT5/speecht5_base_asr.pt \n",
    "  --pytorch_dump_folder_path /some/other/path\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "69ca25a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_model_ctc = SpeechT5ForCTC(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5a32978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_ctc = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_base_ctc\"\n",
    "hf_model_ctc = SpeechT5ForCTC.from_pretrained(model_checkpoint_ctc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b28453ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_base_ctc were not used when initializing SpeechT5Model: ['speecht5.encoder.prenet.feature_projection.projection.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.6.conv.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.5.conv.weight', 'speecht5.encoder.prenet.feature_projection.projection.bias', 'speecht5.encoder.prenet.feature_encoder.conv_layers.1.conv.weight', 'speecht5.encoder.prenet.pos_sinusoidal_embed.weights', 'speecht5.encoder.prenet.masked_spec_embed', 'speecht5.encoder.prenet.feature_encoder.conv_layers.2.conv.weight', 'speecht5.encoder.prenet.feature_projection.layer_norm.bias', 'lm_head.bias', 'speecht5.encoder.prenet.feature_encoder.conv_layers.0.layer_norm.bias', 'speecht5.encoder.prenet.feature_encoder.conv_layers.0.conv.weight', 'speecht5.encoder.prenet.pos_conv_embed.conv.weight_v', 'speecht5.encoder.prenet.feature_encoder.conv_layers.4.conv.weight', 'lm_head.weight', 'speecht5.encoder.prenet.pos_conv_embed.conv.weight_g', 'speecht5.encoder.prenet.feature_encoder.conv_layers.0.layer_norm.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.3.conv.weight', 'speecht5.encoder.prenet.feature_projection.layer_norm.weight', 'speecht5.encoder.prenet.pos_conv_embed.conv.bias']\n",
      "- This IS expected if you are initializing SpeechT5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SpeechT5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of SpeechT5Model were not initialized from the model checkpoint at /Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_base_ctc and are newly initialized: ['speecht5.decoder.wrapped_decoder.layers.4.self_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.2.feed_forward.intermediate_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.5.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.0.feed_forward.output_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.3.feed_forward.intermediate_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.0.feed_forward.intermediate_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.3.feed_forward.output_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.feed_forward.output_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.4.final_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.2.feed_forward.output_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.3.feed_forward.output_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.5.feed_forward.output_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.feed_forward.intermediate_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.final_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.5.final_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.2.feed_forward.intermediate_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.1.final_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.5.feed_forward.intermediate_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.feed_forward.intermediate_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.feed_forward.output_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.0.feed_forward.intermediate_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.feed_forward.intermediate_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.feed_forward.intermediate_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.4.feed_forward.output_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.0.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.4.feed_forward.intermediate_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.2.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.0.final_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.feed_forward.intermediate_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.1.feed_forward.output_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.1.feed_forward.output_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.4.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.2.final_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.0.feed_forward.output_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.2.feed_forward.output_dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# This should also work:\n",
    "hf_model_ctc_naked = SpeechT5Model.from_pretrained(model_checkpoint_ctc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8d81d7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.CausalLMOutput"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the full model:\n",
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model_ctc(**inputs)\n",
    "\n",
    "# Run without attention_mask:\n",
    "# with torch.no_grad():\n",
    "#     hf_outputs = hf_model_ctc(input_values=inputs[\"input_values\"])\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "de48ff1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(encoder_output[\"encoder_out_for_ctc\"][0].permute((1, 0, 2)) - hf_outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "32fabb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(hf_outputs[0], dim=-1, dtype=torch.float32)\n",
    "probs = probs.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "267f80ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[80, 80, 80, 80, 80, 80,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "          4,  4,  4,  4, 46, 80, 80, 16, 80, 80, 12, 12,  6,  6, 80, 80, 80, 80,\n",
       "         80,  4,  4,  4,  6,  6, 11, 11, 13, 13, 13, 80, 16, 16, 80, 80, 80, 80,\n",
       "         12, 12,  6,  6, 80, 80, 80, 80, 80, 80, 80, 80,  4,  4,  4, 80, 80,  7,\n",
       "          9, 14, 14,  4,  4, 80, 24, 80, 80, 80, 80,  5, 80, 13, 13, 13, 13, 13,\n",
       "         80, 22, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,  4,  4,  4, 80, 80,  9, 14,\n",
       "         14, 14, 14, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "90ca3c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tgt_dict.string(probs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5bcf370d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                jusstt   tthhrrruusstt   andd  perrrrry   ndddd\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(probs.shape[0]):\n",
    "    print(tokenizer.decode(tgt_dict.string(probs[i])).replace(\"<ctc_blank>\", \"\"))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edc5fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60229b2c",
   "metadata": {},
   "source": [
    "Calculate loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d6f22609",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model_ctc(\n",
    "         **inputs, \n",
    "         labels=torch.tensor(\n",
    "           [[ 46, 16, 12,  6,  4,  6, 11, 13, 16, 12,  6,  4,  7,  9, 14,  4,\n",
    "         24,  7, 13, 13, 22,  4,  7,  9, 14,  4, 27, 10, 17,  6,  8, 13, 22,  4,\n",
    "          6,  8,  4,  6, 11,  5,  4, 12,  6, 13,  8,  9, 21,  5, 13, ]]\n",
    "         ),\n",
    "         output_hidden_states=True,\n",
    "         return_dict=True,\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5f57c285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(867.8440)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9dd3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b17bb840",
   "metadata": {},
   "source": [
    "## Verify text decoder prenet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9ba48d",
   "metadata": {},
   "source": [
    "First this calls `text_decoder_prenet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c3747a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = inputs.input_values.size(0)\n",
    "beam_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "28dcb259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = torch.tensor([2, 4, 46, 16, 12, 16] * beam_size * batch_size).reshape(beam_size * batch_size, -1)\n",
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e6bd8179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one has padding (token_id = 1)\n",
    "# The results will be different with the HF implementation because\n",
    "# we don't set the attention_mask to 0 for padding tokens\n",
    "# tokens = torch.tensor([2, 4, 46, 16, 1, 12] * beam_size * batch_size).reshape(beam_size * batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f09f744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    prev_output_tokens, tgt_mask, incremental_state = orig_model.text_decoder_prenet(tokens, incremental_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "60cc6f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This returns a sequence length of 1\n",
    "# with torch.no_grad():\n",
    "#     prev_output_tokens, tgt_mask, incremental_state = orig_model.text_decoder_prenet(tokens, incremental_state={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "725cfc62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6, 768])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: misleading name; these are not the actual tokens but their embeddings!\n",
    "prev_output_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "84764c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "81026d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "77134734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.decoder.prenet(\n",
    "         input_ids=tokens,\n",
    "#          attention_mask=hf_encoder_attention_mask,\n",
    "#          attention_mask=inputs.attention_mask,\n",
    "#          past_key_values=[(torch.ones(5, 1, 1), torch.ones(5, 1, 1))],\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8f007852",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeds_hf, decoder_attention_mask = hf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ea4dd4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6, 768])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeds_hf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "21f6193b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(prev_output_tokens - token_embeds_hf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a827caef",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8d7a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fec354d",
   "metadata": {},
   "source": [
    "## Verify Transformer decoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "42a4fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = source.size(0)\n",
    "new_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)\n",
    "encoder_outs = orig_model.encoder.reorder_encoder_out(encoder_output, new_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a98652fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1074,  1.1042,  1.0387,  ...,  0.9619,  1.0463,  0.8678],\n",
       "         [ 0.0890,  0.1702,  0.3139,  ...,  0.8629,  0.7833,  0.9348],\n",
       "         [-0.7402, -0.4274, -0.5774,  ...,  1.3103,  0.5403,  0.7292],\n",
       "         [-0.8579, -0.9214, -0.9064,  ...,  1.0331,  0.8949,  0.9723],\n",
       "         [-0.2291, -0.4457, -0.5671,  ...,  0.9991,  0.8768,  0.9128],\n",
       "         [ 0.7580,  0.5875,  0.4706,  ...,  1.0331,  0.8949,  0.9723]],\n",
       "\n",
       "        [[ 1.1074,  1.1042,  1.0387,  ...,  0.9619,  1.0463,  0.8678],\n",
       "         [ 0.0890,  0.1702,  0.3139,  ...,  0.8629,  0.7833,  0.9348],\n",
       "         [-0.7402, -0.4274, -0.5774,  ...,  1.3103,  0.5403,  0.7292],\n",
       "         [-0.8579, -0.9214, -0.9064,  ...,  1.0331,  0.8949,  0.9723],\n",
       "         [-0.2291, -0.4457, -0.5671,  ...,  0.9991,  0.8768,  0.9128],\n",
       "         [ 0.7580,  0.5875,  0.4706,  ...,  1.0331,  0.8949,  0.9723]],\n",
       "\n",
       "        [[ 1.1074,  1.1042,  1.0387,  ...,  0.9619,  1.0463,  0.8678],\n",
       "         [ 0.0890,  0.1702,  0.3139,  ...,  0.8629,  0.7833,  0.9348],\n",
       "         [-0.7402, -0.4274, -0.5774,  ...,  1.3103,  0.5403,  0.7292],\n",
       "         [-0.8579, -0.9214, -0.9064,  ...,  1.0331,  0.8949,  0.9723],\n",
       "         [-0.2291, -0.4457, -0.5671,  ...,  0.9991,  0.8768,  0.9128],\n",
       "         [ 0.7580,  0.5875,  0.4706,  ...,  1.0331,  0.8949,  0.9723]],\n",
       "\n",
       "        [[ 1.1074,  1.1042,  1.0387,  ...,  0.9619,  1.0463,  0.8678],\n",
       "         [ 0.0890,  0.1702,  0.3139,  ...,  0.8629,  0.7833,  0.9348],\n",
       "         [-0.7402, -0.4274, -0.5774,  ...,  1.3103,  0.5403,  0.7292],\n",
       "         [-0.8579, -0.9214, -0.9064,  ...,  1.0331,  0.8949,  0.9723],\n",
       "         [-0.2291, -0.4457, -0.5671,  ...,  0.9991,  0.8768,  0.9128],\n",
       "         [ 0.7580,  0.5875,  0.4706,  ...,  1.0331,  0.8949,  0.9723]],\n",
       "\n",
       "        [[ 1.1074,  1.1042,  1.0387,  ...,  0.9619,  1.0463,  0.8678],\n",
       "         [ 0.0890,  0.1702,  0.3139,  ...,  0.8629,  0.7833,  0.9348],\n",
       "         [-0.7402, -0.4274, -0.5774,  ...,  1.3103,  0.5403,  0.7292],\n",
       "         [-0.8579, -0.9214, -0.9064,  ...,  1.0331,  0.8949,  0.9723],\n",
       "         [-0.2291, -0.4457, -0.5671,  ...,  0.9991,  0.8768,  0.9128],\n",
       "         [ 0.7580,  0.5875,  0.4706,  ...,  1.0331,  0.8949,  0.9723]]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6229b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    decoder_output, extra = orig_model.decoder(\n",
    "        prev_output_tokens,\n",
    "        tgt_mask,\n",
    "        encoder_out=encoder_outs,\n",
    "        incremental_state=incremental_state,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0a190958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6, 768])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fea0351e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4313, -0.3359, -0.2130,  ..., -0.2689, -0.3691, -0.0400],\n",
       "         [-0.2194, -0.0034, -0.7280,  ..., -0.1523, -0.6537,  0.0089],\n",
       "         [-0.0151, -0.1364, -0.4223,  ...,  0.3197, -0.2469,  0.1624],\n",
       "         [ 0.9742, -0.4636, -1.4084,  ...,  0.2529,  0.2293, -0.4467],\n",
       "         [ 0.6141, -0.4535, -0.1964,  ..., -0.4525, -0.1346, -0.5909],\n",
       "         [ 0.8822, -0.4557, -0.7624,  ...,  0.0964,  0.2632, -0.3568]],\n",
       "\n",
       "        [[-0.4313, -0.3359, -0.2130,  ..., -0.2689, -0.3691, -0.0400],\n",
       "         [-0.2194, -0.0034, -0.7280,  ..., -0.1523, -0.6537,  0.0089],\n",
       "         [-0.0151, -0.1364, -0.4223,  ...,  0.3197, -0.2469,  0.1624],\n",
       "         [ 0.9742, -0.4636, -1.4084,  ...,  0.2529,  0.2293, -0.4467],\n",
       "         [ 0.6141, -0.4535, -0.1964,  ..., -0.4525, -0.1346, -0.5909],\n",
       "         [ 0.8822, -0.4557, -0.7624,  ...,  0.0964,  0.2632, -0.3568]],\n",
       "\n",
       "        [[-0.4313, -0.3359, -0.2130,  ..., -0.2689, -0.3691, -0.0400],\n",
       "         [-0.2194, -0.0034, -0.7280,  ..., -0.1523, -0.6537,  0.0089],\n",
       "         [-0.0151, -0.1364, -0.4223,  ...,  0.3197, -0.2469,  0.1624],\n",
       "         [ 0.9742, -0.4636, -1.4084,  ...,  0.2529,  0.2293, -0.4467],\n",
       "         [ 0.6141, -0.4535, -0.1964,  ..., -0.4525, -0.1346, -0.5909],\n",
       "         [ 0.8822, -0.4557, -0.7624,  ...,  0.0964,  0.2632, -0.3568]],\n",
       "\n",
       "        [[-0.4313, -0.3359, -0.2130,  ..., -0.2689, -0.3691, -0.0400],\n",
       "         [-0.2194, -0.0034, -0.7280,  ..., -0.1523, -0.6537,  0.0089],\n",
       "         [-0.0151, -0.1364, -0.4223,  ...,  0.3197, -0.2469,  0.1624],\n",
       "         [ 0.9742, -0.4636, -1.4084,  ...,  0.2529,  0.2293, -0.4467],\n",
       "         [ 0.6141, -0.4535, -0.1964,  ..., -0.4525, -0.1346, -0.5909],\n",
       "         [ 0.8822, -0.4557, -0.7624,  ...,  0.0964,  0.2632, -0.3568]],\n",
       "\n",
       "        [[-0.4313, -0.3359, -0.2130,  ..., -0.2689, -0.3691, -0.0400],\n",
       "         [-0.2194, -0.0034, -0.7280,  ..., -0.1523, -0.6537,  0.0089],\n",
       "         [-0.0151, -0.1364, -0.4223,  ...,  0.3197, -0.2469,  0.1624],\n",
       "         [ 0.9742, -0.4636, -1.4084,  ...,  0.2529,  0.2293, -0.4467],\n",
       "         [ 0.6141, -0.4535, -0.1964,  ..., -0.4525, -0.1346, -0.5909],\n",
       "         [ 0.8822, -0.4557, -0.7624,  ...,  0.0964,  0.2632, -0.3568]]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "05117956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[x.shape for x in extra[\"attn\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b593692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[x.shape for x in extra[\"inner_states\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9d486f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.decoder.wrapped_decoder(\n",
    "         hidden_states=prev_output_tokens,\n",
    "         attention_mask=decoder_attention_mask,\n",
    "         encoder_hidden_states=encoder_outs[\"encoder_out\"][0].permute((1, 0, 2)),\n",
    "         encoder_attention_mask=hf_encoder_attention_mask.repeat((1, beam_size)).view(beam_size * batch_size, -1),\n",
    "         return_dict=True,\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ea512cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_hidden_state', 'past_key_values']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d82c369a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6, 768])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "eb6282bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4313, -0.3359, -0.2130,  ..., -0.2689, -0.3691, -0.0400],\n",
       "         [-0.2194, -0.0034, -0.7280,  ..., -0.1523, -0.6537,  0.0089],\n",
       "         [-0.0151, -0.1364, -0.4223,  ...,  0.3197, -0.2469,  0.1624],\n",
       "         [ 0.9742, -0.4636, -1.4084,  ...,  0.2529,  0.2293, -0.4467],\n",
       "         [ 0.6141, -0.4535, -0.1964,  ..., -0.4525, -0.1346, -0.5909],\n",
       "         [ 0.8822, -0.4557, -0.7624,  ...,  0.0964,  0.2632, -0.3568]],\n",
       "\n",
       "        [[-0.4313, -0.3359, -0.2130,  ..., -0.2689, -0.3691, -0.0400],\n",
       "         [-0.2194, -0.0034, -0.7280,  ..., -0.1523, -0.6537,  0.0089],\n",
       "         [-0.0151, -0.1364, -0.4223,  ...,  0.3197, -0.2469,  0.1624],\n",
       "         [ 0.9742, -0.4636, -1.4084,  ...,  0.2529,  0.2293, -0.4467],\n",
       "         [ 0.6141, -0.4535, -0.1964,  ..., -0.4525, -0.1346, -0.5909],\n",
       "         [ 0.8822, -0.4557, -0.7624,  ...,  0.0964,  0.2632, -0.3568]],\n",
       "\n",
       "        [[-0.4313, -0.3359, -0.2130,  ..., -0.2689, -0.3691, -0.0400],\n",
       "         [-0.2194, -0.0034, -0.7280,  ..., -0.1523, -0.6537,  0.0089],\n",
       "         [-0.0151, -0.1364, -0.4223,  ...,  0.3197, -0.2469,  0.1624],\n",
       "         [ 0.9742, -0.4636, -1.4084,  ...,  0.2529,  0.2293, -0.4467],\n",
       "         [ 0.6141, -0.4535, -0.1964,  ..., -0.4525, -0.1346, -0.5909],\n",
       "         [ 0.8822, -0.4557, -0.7624,  ...,  0.0964,  0.2632, -0.3568]],\n",
       "\n",
       "        [[-0.4313, -0.3359, -0.2130,  ..., -0.2689, -0.3691, -0.0400],\n",
       "         [-0.2194, -0.0034, -0.7280,  ..., -0.1523, -0.6537,  0.0089],\n",
       "         [-0.0151, -0.1364, -0.4223,  ...,  0.3197, -0.2469,  0.1624],\n",
       "         [ 0.9742, -0.4636, -1.4084,  ...,  0.2529,  0.2293, -0.4467],\n",
       "         [ 0.6141, -0.4535, -0.1964,  ..., -0.4525, -0.1346, -0.5909],\n",
       "         [ 0.8822, -0.4557, -0.7624,  ...,  0.0964,  0.2632, -0.3568]],\n",
       "\n",
       "        [[-0.4313, -0.3359, -0.2130,  ..., -0.2689, -0.3691, -0.0400],\n",
       "         [-0.2194, -0.0034, -0.7280,  ..., -0.1523, -0.6537,  0.0089],\n",
       "         [-0.0151, -0.1364, -0.4223,  ...,  0.3197, -0.2469,  0.1624],\n",
       "         [ 0.9742, -0.4636, -1.4084,  ...,  0.2529,  0.2293, -0.4467],\n",
       "         [ 0.6141, -0.4535, -0.1964,  ..., -0.4525, -0.1346, -0.5909],\n",
       "         [ 0.8822, -0.4557, -0.7624,  ...,  0.0964,  0.2632, -0.3568]]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f3e254cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1325e-06)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(decoder_output - hf_outputs[\"last_hidden_state\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d3d1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d9113f7",
   "metadata": {},
   "source": [
    "## Verify text decoder postnet output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a20be43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    lprobs = orig_model.text_decoder_postnet(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a7a143bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6, 81])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lprobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7d6de14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1037e+01, -2.1807e+01,  1.5759e+00, -2.1707e+01,  1.6257e+01,\n",
       "          7.1087e-02,  1.8325e-01, -1.0547e+00, -1.7248e-01, -1.9979e+00,\n",
       "         -5.4946e-02,  1.3188e-01, -2.4018e-01, -1.1039e+00, -2.6584e+00,\n",
       "         -1.0886e+00, -1.0818e+00, -2.0889e+00, -1.1437e+00, -7.9405e-01,\n",
       "          1.0275e+00,  2.2644e-03, -3.0957e+00, -2.1822e+01, -9.1836e-01,\n",
       "          9.4244e-01, -2.1769e+01, -3.0252e+00, -2.3076e+00, -2.1718e+01,\n",
       "         -2.1757e+01,  4.5374e-01, -2.1858e+01, -2.1725e+01, -2.1715e+01,\n",
       "         -2.1791e+01, -2.1762e+01, -5.5614e+00, -2.1728e+01, -2.1835e+01,\n",
       "         -2.1708e+01, -2.1776e+01, -2.1762e+01, -2.1772e+01, -2.1749e+01,\n",
       "         -2.5837e+00, -1.4410e+00, -2.1696e+01, -2.1731e+01, -2.1853e+01,\n",
       "         -2.1734e+01, -2.1810e+01, -2.1700e+01, -2.1690e+01, -2.1748e+01,\n",
       "         -2.1690e+01, -2.1741e+01, -6.0976e+00, -2.1758e+01, -2.1720e+01,\n",
       "         -2.1772e+01, -2.1833e+01, -2.1756e+01, -2.1735e+01, -2.1747e+01,\n",
       "         -2.1770e+01, -2.1749e+01, -2.1786e+01, -2.1787e+01, -2.1747e+01,\n",
       "         -2.1673e+01, -2.1764e+01, -2.1712e+01, -2.1751e+01, -2.1695e+01,\n",
       "         -2.1751e+01, -2.1708e+01, -2.1734e+01, -2.1717e+01, -2.0313e+01,\n",
       "         -2.1729e+01],\n",
       "        [-1.1664e+01, -1.2805e+01, -7.1837e+00, -1.2664e+01, -2.5586e+00,\n",
       "          4.2616e-01,  3.6004e+00,  2.9132e+00, -3.9726e-01, -3.1469e-01,\n",
       "          2.5877e+00, -3.4749e-01,  1.7275e+00, -4.9179e-01,  4.6530e+00,\n",
       "         -1.9203e+00, -1.7016e-01,  3.5551e+00,  1.9592e+00,  1.2522e+00,\n",
       "          9.3826e-01,  5.3145e+00,  2.8388e+00, -1.2670e+01,  2.6241e-02,\n",
       "          2.0107e+00, -1.2660e+01, -8.8509e-01, -3.7090e+00, -1.2652e+01,\n",
       "         -1.2691e+01, -5.5648e+00, -1.2629e+01, -1.2585e+01, -1.2656e+01,\n",
       "         -1.2678e+01, -1.2688e+01, -5.1685e+00, -1.2610e+01, -1.2673e+01,\n",
       "         -1.2672e+01, -1.2641e+01, -1.2666e+01, -1.2651e+01, -1.2646e+01,\n",
       "         -1.9276e+00,  1.3502e+01, -1.2630e+01, -1.2681e+01, -1.2739e+01,\n",
       "         -1.2703e+01, -1.2684e+01, -1.2634e+01, -1.2566e+01, -1.2623e+01,\n",
       "         -1.2610e+01, -1.2603e+01, -1.6130e+00, -1.2662e+01, -1.2688e+01,\n",
       "         -1.2664e+01, -1.2685e+01, -1.2755e+01, -1.2621e+01, -1.2643e+01,\n",
       "         -1.2657e+01, -1.2668e+01, -1.2618e+01, -1.2693e+01, -1.2697e+01,\n",
       "         -1.2605e+01, -1.2662e+01, -1.2596e+01, -1.2588e+01, -1.2659e+01,\n",
       "         -1.2678e+01, -1.2700e+01, -1.2661e+01, -1.2678e+01, -1.1944e+01,\n",
       "         -1.2601e+01],\n",
       "        [-1.2388e+01, -1.2962e+01, -1.3085e+00, -1.3086e+01,  2.7360e+00,\n",
       "          7.7410e+00, -2.3140e-01,  3.8255e+00,  4.3075e+00, -2.4741e-02,\n",
       "          6.4240e+00, -6.9125e-01,  1.5720e+00, -1.5421e+00, -3.4463e+00,\n",
       "         -4.2847e+00,  1.4289e+01, -4.3191e+00, -4.0551e+00, -3.1863e+00,\n",
       "         -4.3351e+00, -4.2331e+00,  2.2487e+00, -1.3198e+01, -3.2880e+00,\n",
       "         -5.2398e+00, -1.3092e+01, -2.4781e+00, -5.9313e+00, -1.3092e+01,\n",
       "         -1.3182e+01,  2.9716e+00, -1.3002e+01, -1.3064e+01, -1.3078e+01,\n",
       "         -1.3077e+01, -1.3168e+01, -4.4803e+00, -1.3179e+01, -1.3117e+01,\n",
       "         -1.3161e+01, -1.3122e+01, -1.3144e+01, -1.3080e+01, -1.3138e+01,\n",
       "         -6.7845e+00, -1.8506e-01, -1.3142e+01, -1.3094e+01, -1.3119e+01,\n",
       "         -1.3080e+01, -1.3101e+01, -1.3173e+01, -1.2931e+01, -1.3132e+01,\n",
       "         -1.3013e+01, -1.3072e+01, -3.3886e+00, -1.3187e+01, -1.3161e+01,\n",
       "         -1.3091e+01, -1.3060e+01, -1.3198e+01, -1.3102e+01, -1.3160e+01,\n",
       "         -1.3103e+01, -1.3121e+01, -1.3048e+01, -1.3090e+01, -1.3191e+01,\n",
       "         -1.3148e+01, -1.3170e+01, -1.3083e+01, -1.3063e+01, -1.3060e+01,\n",
       "         -1.3024e+01, -1.3103e+01, -1.3153e+01, -1.3139e+01, -1.2866e+01,\n",
       "         -1.3031e+01],\n",
       "        [-1.8274e+01, -1.5212e+01, -5.3120e-01, -1.5214e+01,  2.4778e+00,\n",
       "         -3.4880e+00,  4.9050e+00,  7.8669e-01, -2.2142e+00,  2.0731e+00,\n",
       "          1.0639e+00, -1.3736e+00,  1.8932e+01,  1.6710e+00,  3.5663e+00,\n",
       "          2.5050e+00, -1.9784e+00, -1.9199e+00,  8.5971e-01, -1.2840e+00,\n",
       "         -5.0375e+00,  8.1232e-01, -3.7057e+00, -1.5384e+01,  1.6570e-01,\n",
       "          4.7704e-01, -1.5229e+01, -1.2485e+00, -4.8086e+00, -1.5404e+01,\n",
       "         -1.5355e+01,  2.3393e-01, -1.5352e+01, -1.5368e+01, -1.5295e+01,\n",
       "         -1.5330e+01, -1.5405e+01, -1.7992e+00, -1.5298e+01, -1.5295e+01,\n",
       "         -1.5300e+01, -1.5386e+01, -1.5370e+01, -1.5308e+01, -1.5348e+01,\n",
       "         -5.2306e+00, -4.5142e+00, -1.5541e+01, -1.5385e+01, -1.5270e+01,\n",
       "         -1.5376e+01, -1.5383e+01, -1.5294e+01, -1.5196e+01, -1.5504e+01,\n",
       "         -1.5268e+01, -1.5323e+01,  2.8120e-01, -1.5402e+01, -1.5291e+01,\n",
       "         -1.5379e+01, -1.5352e+01, -1.5320e+01, -1.5290e+01, -1.5414e+01,\n",
       "         -1.5389e+01, -1.5305e+01, -1.5406e+01, -1.5269e+01, -1.5353e+01,\n",
       "         -1.5398e+01, -1.5427e+01, -1.5431e+01, -1.5463e+01, -1.5307e+01,\n",
       "         -1.5297e+01, -1.5190e+01, -1.5409e+01, -1.5393e+01, -1.4910e+01,\n",
       "         -1.5185e+01],\n",
       "        [-1.1422e+01, -1.4533e+01,  3.4843e+00, -1.4172e+01,  9.4429e+00,\n",
       "         -1.6376e+00,  1.8918e+01, -2.1377e+00, -2.7100e+00, -1.0284e+00,\n",
       "         -1.6453e+00, -3.8702e-01,  5.5691e+00, -4.9815e+00, -6.7231e-01,\n",
       "         -3.0831e+00, -2.0137e+00,  3.9318e-01, -3.8844e+00, -2.8566e-02,\n",
       "         -3.6051e+00,  3.5579e-01, -2.4296e+00, -1.4242e+01,  3.0042e+00,\n",
       "         -2.4899e+00, -1.4161e+01, -2.4731e+00,  9.8946e-02, -1.4218e+01,\n",
       "         -1.4226e+01,  3.5717e+00, -1.4208e+01, -1.4299e+01, -1.4194e+01,\n",
       "         -1.4242e+01, -1.4195e+01, -3.6879e+00, -1.4246e+01, -1.4169e+01,\n",
       "         -1.4164e+01, -1.4189e+01, -1.4226e+01, -1.4246e+01, -1.4257e+01,\n",
       "          3.6633e-01, -6.1123e+00, -1.4205e+01, -1.4363e+01, -1.4233e+01,\n",
       "         -1.4284e+01, -1.4276e+01, -1.4210e+01, -1.4115e+01, -1.4263e+01,\n",
       "         -1.4268e+01, -1.4174e+01, -3.4926e-01, -1.4290e+01, -1.4277e+01,\n",
       "         -1.4266e+01, -1.4148e+01, -1.4195e+01, -1.4155e+01, -1.4246e+01,\n",
       "         -1.4232e+01, -1.4221e+01, -1.4341e+01, -1.4202e+01, -1.4227e+01,\n",
       "         -1.4160e+01, -1.4410e+01, -1.4241e+01, -1.4330e+01, -1.4215e+01,\n",
       "         -1.4250e+01, -1.4152e+01, -1.4263e+01, -1.4239e+01, -1.3209e+01,\n",
       "         -1.4203e+01],\n",
       "        [-2.0714e+01, -1.6684e+01,  2.3597e+00, -1.6627e+01,  7.3070e+00,\n",
       "          2.0986e-01,  6.7247e+00,  1.2362e+00, -7.6587e+00,  1.4044e-01,\n",
       "         -4.4320e-01,  1.2462e+00,  1.0353e+01,  1.4858e+00,  6.8508e-01,\n",
       "          3.1815e+00, -7.4359e+00,  6.6236e-01, -5.4236e-01, -3.4805e-01,\n",
       "         -6.7645e+00,  2.1580e+00, -8.8409e+00, -1.6624e+01,  3.2994e+00,\n",
       "         -2.3242e+00, -1.6527e+01, -1.6795e+00, -1.2849e+00, -1.6728e+01,\n",
       "         -1.6766e+01,  1.1037e+00, -1.6704e+01, -1.6714e+01, -1.6506e+01,\n",
       "         -1.6683e+01, -1.6659e+01, -2.6109e+00, -1.6529e+01, -1.6636e+01,\n",
       "         -1.6544e+01, -1.6698e+01, -1.6699e+01, -1.6525e+01, -1.6729e+01,\n",
       "         -5.5754e+00, -4.1031e+00, -1.6800e+01, -1.6741e+01, -1.6584e+01,\n",
       "         -1.6811e+01, -1.6693e+01, -1.6660e+01, -1.6538e+01, -1.6884e+01,\n",
       "         -1.6722e+01, -1.6668e+01, -3.1906e+00, -1.6682e+01, -1.6634e+01,\n",
       "         -1.6678e+01, -1.6737e+01, -1.6513e+01, -1.6670e+01, -1.6735e+01,\n",
       "         -1.6759e+01, -1.6690e+01, -1.6813e+01, -1.6623e+01, -1.6661e+01,\n",
       "         -1.6705e+01, -1.6677e+01, -1.6837e+01, -1.6700e+01, -1.6637e+01,\n",
       "         -1.6623e+01, -1.6590e+01, -1.6717e+01, -1.6810e+01, -1.5199e+01,\n",
       "         -1.6550e+01]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lprobs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ba7571e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    lprobs_hf = hf_model.text_decoder_postnet(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5623a791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6, 81])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lprobs_hf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "64f0c976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1037e+01, -2.1807e+01,  1.5759e+00, -2.1707e+01,  1.6257e+01,\n",
       "          7.1087e-02,  1.8325e-01, -1.0547e+00, -1.7248e-01, -1.9979e+00,\n",
       "         -5.4946e-02,  1.3188e-01, -2.4018e-01, -1.1039e+00, -2.6584e+00,\n",
       "         -1.0886e+00, -1.0818e+00, -2.0889e+00, -1.1437e+00, -7.9405e-01,\n",
       "          1.0275e+00,  2.2644e-03, -3.0957e+00, -2.1822e+01, -9.1836e-01,\n",
       "          9.4244e-01, -2.1769e+01, -3.0252e+00, -2.3076e+00, -2.1718e+01,\n",
       "         -2.1757e+01,  4.5374e-01, -2.1858e+01, -2.1725e+01, -2.1715e+01,\n",
       "         -2.1791e+01, -2.1762e+01, -5.5614e+00, -2.1728e+01, -2.1835e+01,\n",
       "         -2.1708e+01, -2.1776e+01, -2.1762e+01, -2.1772e+01, -2.1749e+01,\n",
       "         -2.5837e+00, -1.4410e+00, -2.1696e+01, -2.1731e+01, -2.1853e+01,\n",
       "         -2.1734e+01, -2.1810e+01, -2.1700e+01, -2.1690e+01, -2.1748e+01,\n",
       "         -2.1690e+01, -2.1741e+01, -6.0976e+00, -2.1758e+01, -2.1720e+01,\n",
       "         -2.1772e+01, -2.1833e+01, -2.1756e+01, -2.1735e+01, -2.1747e+01,\n",
       "         -2.1770e+01, -2.1749e+01, -2.1786e+01, -2.1787e+01, -2.1747e+01,\n",
       "         -2.1673e+01, -2.1764e+01, -2.1712e+01, -2.1751e+01, -2.1695e+01,\n",
       "         -2.1751e+01, -2.1708e+01, -2.1734e+01, -2.1717e+01, -2.0313e+01,\n",
       "         -2.1729e+01],\n",
       "        [-1.1664e+01, -1.2805e+01, -7.1837e+00, -1.2664e+01, -2.5586e+00,\n",
       "          4.2616e-01,  3.6004e+00,  2.9132e+00, -3.9726e-01, -3.1469e-01,\n",
       "          2.5877e+00, -3.4749e-01,  1.7275e+00, -4.9179e-01,  4.6530e+00,\n",
       "         -1.9203e+00, -1.7016e-01,  3.5551e+00,  1.9592e+00,  1.2522e+00,\n",
       "          9.3826e-01,  5.3145e+00,  2.8388e+00, -1.2670e+01,  2.6241e-02,\n",
       "          2.0107e+00, -1.2660e+01, -8.8509e-01, -3.7090e+00, -1.2652e+01,\n",
       "         -1.2691e+01, -5.5648e+00, -1.2629e+01, -1.2585e+01, -1.2656e+01,\n",
       "         -1.2678e+01, -1.2688e+01, -5.1685e+00, -1.2610e+01, -1.2673e+01,\n",
       "         -1.2672e+01, -1.2641e+01, -1.2666e+01, -1.2651e+01, -1.2646e+01,\n",
       "         -1.9276e+00,  1.3502e+01, -1.2630e+01, -1.2681e+01, -1.2739e+01,\n",
       "         -1.2703e+01, -1.2684e+01, -1.2634e+01, -1.2566e+01, -1.2623e+01,\n",
       "         -1.2610e+01, -1.2603e+01, -1.6130e+00, -1.2662e+01, -1.2688e+01,\n",
       "         -1.2664e+01, -1.2685e+01, -1.2755e+01, -1.2621e+01, -1.2643e+01,\n",
       "         -1.2657e+01, -1.2668e+01, -1.2618e+01, -1.2693e+01, -1.2697e+01,\n",
       "         -1.2605e+01, -1.2662e+01, -1.2596e+01, -1.2588e+01, -1.2659e+01,\n",
       "         -1.2678e+01, -1.2700e+01, -1.2661e+01, -1.2678e+01, -1.1944e+01,\n",
       "         -1.2601e+01],\n",
       "        [-1.2388e+01, -1.2962e+01, -1.3085e+00, -1.3086e+01,  2.7360e+00,\n",
       "          7.7410e+00, -2.3140e-01,  3.8255e+00,  4.3075e+00, -2.4741e-02,\n",
       "          6.4240e+00, -6.9125e-01,  1.5720e+00, -1.5421e+00, -3.4463e+00,\n",
       "         -4.2847e+00,  1.4289e+01, -4.3191e+00, -4.0551e+00, -3.1863e+00,\n",
       "         -4.3351e+00, -4.2331e+00,  2.2487e+00, -1.3198e+01, -3.2880e+00,\n",
       "         -5.2398e+00, -1.3092e+01, -2.4781e+00, -5.9313e+00, -1.3092e+01,\n",
       "         -1.3182e+01,  2.9716e+00, -1.3002e+01, -1.3064e+01, -1.3078e+01,\n",
       "         -1.3077e+01, -1.3168e+01, -4.4803e+00, -1.3179e+01, -1.3117e+01,\n",
       "         -1.3161e+01, -1.3122e+01, -1.3144e+01, -1.3080e+01, -1.3138e+01,\n",
       "         -6.7845e+00, -1.8506e-01, -1.3142e+01, -1.3094e+01, -1.3119e+01,\n",
       "         -1.3080e+01, -1.3101e+01, -1.3173e+01, -1.2931e+01, -1.3132e+01,\n",
       "         -1.3013e+01, -1.3072e+01, -3.3886e+00, -1.3187e+01, -1.3161e+01,\n",
       "         -1.3091e+01, -1.3060e+01, -1.3198e+01, -1.3102e+01, -1.3160e+01,\n",
       "         -1.3103e+01, -1.3121e+01, -1.3048e+01, -1.3090e+01, -1.3191e+01,\n",
       "         -1.3148e+01, -1.3170e+01, -1.3083e+01, -1.3063e+01, -1.3060e+01,\n",
       "         -1.3024e+01, -1.3103e+01, -1.3153e+01, -1.3139e+01, -1.2866e+01,\n",
       "         -1.3031e+01],\n",
       "        [-1.8274e+01, -1.5212e+01, -5.3120e-01, -1.5214e+01,  2.4778e+00,\n",
       "         -3.4880e+00,  4.9050e+00,  7.8669e-01, -2.2142e+00,  2.0731e+00,\n",
       "          1.0639e+00, -1.3736e+00,  1.8932e+01,  1.6710e+00,  3.5663e+00,\n",
       "          2.5050e+00, -1.9784e+00, -1.9199e+00,  8.5971e-01, -1.2840e+00,\n",
       "         -5.0375e+00,  8.1232e-01, -3.7057e+00, -1.5384e+01,  1.6570e-01,\n",
       "          4.7704e-01, -1.5229e+01, -1.2485e+00, -4.8086e+00, -1.5404e+01,\n",
       "         -1.5355e+01,  2.3393e-01, -1.5352e+01, -1.5368e+01, -1.5295e+01,\n",
       "         -1.5330e+01, -1.5405e+01, -1.7992e+00, -1.5298e+01, -1.5295e+01,\n",
       "         -1.5300e+01, -1.5386e+01, -1.5370e+01, -1.5308e+01, -1.5348e+01,\n",
       "         -5.2306e+00, -4.5142e+00, -1.5541e+01, -1.5385e+01, -1.5270e+01,\n",
       "         -1.5376e+01, -1.5383e+01, -1.5294e+01, -1.5196e+01, -1.5504e+01,\n",
       "         -1.5268e+01, -1.5323e+01,  2.8120e-01, -1.5402e+01, -1.5291e+01,\n",
       "         -1.5379e+01, -1.5352e+01, -1.5320e+01, -1.5290e+01, -1.5414e+01,\n",
       "         -1.5389e+01, -1.5305e+01, -1.5406e+01, -1.5269e+01, -1.5353e+01,\n",
       "         -1.5398e+01, -1.5427e+01, -1.5431e+01, -1.5463e+01, -1.5307e+01,\n",
       "         -1.5297e+01, -1.5190e+01, -1.5409e+01, -1.5393e+01, -1.4910e+01,\n",
       "         -1.5185e+01],\n",
       "        [-1.1422e+01, -1.4533e+01,  3.4843e+00, -1.4172e+01,  9.4429e+00,\n",
       "         -1.6376e+00,  1.8918e+01, -2.1377e+00, -2.7100e+00, -1.0284e+00,\n",
       "         -1.6453e+00, -3.8702e-01,  5.5691e+00, -4.9815e+00, -6.7231e-01,\n",
       "         -3.0831e+00, -2.0137e+00,  3.9318e-01, -3.8844e+00, -2.8566e-02,\n",
       "         -3.6051e+00,  3.5579e-01, -2.4296e+00, -1.4242e+01,  3.0042e+00,\n",
       "         -2.4899e+00, -1.4161e+01, -2.4731e+00,  9.8946e-02, -1.4218e+01,\n",
       "         -1.4226e+01,  3.5717e+00, -1.4208e+01, -1.4299e+01, -1.4194e+01,\n",
       "         -1.4242e+01, -1.4195e+01, -3.6879e+00, -1.4246e+01, -1.4169e+01,\n",
       "         -1.4164e+01, -1.4189e+01, -1.4226e+01, -1.4246e+01, -1.4257e+01,\n",
       "          3.6633e-01, -6.1123e+00, -1.4205e+01, -1.4363e+01, -1.4233e+01,\n",
       "         -1.4284e+01, -1.4276e+01, -1.4210e+01, -1.4115e+01, -1.4263e+01,\n",
       "         -1.4268e+01, -1.4174e+01, -3.4926e-01, -1.4290e+01, -1.4277e+01,\n",
       "         -1.4266e+01, -1.4148e+01, -1.4195e+01, -1.4155e+01, -1.4246e+01,\n",
       "         -1.4232e+01, -1.4221e+01, -1.4341e+01, -1.4202e+01, -1.4227e+01,\n",
       "         -1.4160e+01, -1.4410e+01, -1.4241e+01, -1.4330e+01, -1.4215e+01,\n",
       "         -1.4250e+01, -1.4152e+01, -1.4263e+01, -1.4239e+01, -1.3209e+01,\n",
       "         -1.4203e+01],\n",
       "        [-2.0714e+01, -1.6684e+01,  2.3597e+00, -1.6627e+01,  7.3070e+00,\n",
       "          2.0986e-01,  6.7247e+00,  1.2362e+00, -7.6587e+00,  1.4044e-01,\n",
       "         -4.4320e-01,  1.2462e+00,  1.0353e+01,  1.4858e+00,  6.8508e-01,\n",
       "          3.1815e+00, -7.4359e+00,  6.6236e-01, -5.4236e-01, -3.4805e-01,\n",
       "         -6.7645e+00,  2.1580e+00, -8.8409e+00, -1.6624e+01,  3.2994e+00,\n",
       "         -2.3242e+00, -1.6527e+01, -1.6795e+00, -1.2849e+00, -1.6728e+01,\n",
       "         -1.6766e+01,  1.1037e+00, -1.6704e+01, -1.6714e+01, -1.6506e+01,\n",
       "         -1.6683e+01, -1.6659e+01, -2.6109e+00, -1.6529e+01, -1.6636e+01,\n",
       "         -1.6544e+01, -1.6698e+01, -1.6699e+01, -1.6525e+01, -1.6729e+01,\n",
       "         -5.5754e+00, -4.1031e+00, -1.6800e+01, -1.6741e+01, -1.6584e+01,\n",
       "         -1.6811e+01, -1.6693e+01, -1.6660e+01, -1.6538e+01, -1.6884e+01,\n",
       "         -1.6722e+01, -1.6668e+01, -3.1906e+00, -1.6682e+01, -1.6634e+01,\n",
       "         -1.6678e+01, -1.6737e+01, -1.6513e+01, -1.6670e+01, -1.6735e+01,\n",
       "         -1.6759e+01, -1.6690e+01, -1.6813e+01, -1.6623e+01, -1.6661e+01,\n",
       "         -1.6705e+01, -1.6677e+01, -1.6837e+01, -1.6700e+01, -1.6637e+01,\n",
       "         -1.6623e+01, -1.6590e+01, -1.6717e+01, -1.6810e+01, -1.5199e+01,\n",
       "         -1.6550e+01]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lprobs_hf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6eeff463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(lprobs - lprobs_hf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24926eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32e98799",
   "metadata": {},
   "source": [
    "## Use the `transformers` generator loop:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c4e9dc",
   "metadata": {},
   "source": [
    "Run the full model to make sure this doesn't give any errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b4b889ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.Seq2SeqLMOutput"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model(\n",
    "         input_values=inputs.input_values,\n",
    "         attention_mask=inputs.attention_mask,\n",
    "         decoder_input_ids=torch.tensor([[3, 4, 5]]),\n",
    "         #decoder_input_ids=torch.tensor([[3, 4, 5], [2, 2, 2]]),  # batch\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "677718c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logits', 'past_key_values', 'encoder_last_hidden_state']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7f2bcec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 81])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d1eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "baf0244b",
   "metadata": {},
   "source": [
    "Also calculate loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1b7bd1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.Seq2SeqLMOutput"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model(\n",
    "         input_values=inputs.input_values,\n",
    "         attention_mask=inputs.attention_mask,\n",
    "         #decoder_input_ids=torch.tensor([[2,  4, 18, 10, 12,  6,  5]]),\n",
    "         labels=torch.tensor([[4, 18, 10, 12,  6,  5, 13]]),\n",
    "         #labels=torch.tensor([[4, 18, 10, 12,  6,  5, 13], [4, 18, 10, 12,  6,  5, 13]]),  # batch\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f34dd766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 81])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4edf2044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.7035)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195961eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f2ce14a",
   "metadata": {},
   "source": [
    "Generator loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "72ccdbf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 69120])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b2fc2baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs = hf_model.generate(inputs.input_values, max_length=100)\n",
    "# hf_outputs = hf_model.generate(inputs.input_values, num_beams=5, max_length=100) #, bos_token_id=2)\n",
    "# hf_outputs = hf_model.generate(torch.rand(1, 10000), num_beams=5, max_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "015c80b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 52])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ce7e1899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  4, 46, 16, 12,  6,  4,  6, 11, 13, 16, 12,  6,  4,  7,  9, 14,  4,\n",
       "         24,  7, 13, 13, 22,  4,  7,  9, 14,  4, 27, 10, 17,  6,  8, 13, 22,  4,\n",
       "          6,  8,  4,  6, 11,  5,  4, 12,  6, 13,  8,  9, 21,  5, 13,  2]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "11fce9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁ j u s t ▁ t h r u s t ▁ a n d ▁ p a r r y ▁ a n d ▁ v i c t o r y ▁ t o ▁ t h e ▁ s t r o n g e r\n",
      "just thrust and parry and victory to the stronger\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(hf_outputs.shape[0]):\n",
    "    print(tgt_dict.string(hf_outputs[i]))\n",
    "    print(tokenizer.decode(tgt_dict.string(hf_outputs[i])))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "997fd48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '<pad>', '</s>', '<unk>', '▁']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tgt_dict[x] for x in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d72119a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a116f698",
   "metadata": {},
   "source": [
    "For comparison, Speech2Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00306c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
    "\n",
    "s2t_model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "s2t_processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "f994828c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthijs/anaconda3/envs/t5/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'dataset_info': token. Will not be supported from version '0.12'.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Reusing dataset librispeech_asr_dummy (/Users/matthijs/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 584, 80])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "inputs = s2t_processor(\n",
    "    ds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], return_tensors=\"pt\"\n",
    ")\n",
    "input_features = inputs.input_features\n",
    "input_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "dc8ef5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 584, 80])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test batch\n",
    "# input_features = torch.tile(input_features, dims=(2, 1, 1))\n",
    "# input_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "daf06a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/transformers/src/transformers/models/speech_to_text/modeling_speech_to_text.py:561: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  input_lengths = (input_lengths - 1) // 2 + 1\n",
      "/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/transformers/src/transformers/generation_utils.py:1296: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 200 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mister quilter is the apostle of the middle classes and we are glad to welcome his gospel'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = s2t_model.generate(inputs=input_features)\n",
    "\n",
    "transcription = s2t_processor.batch_decode(generated_ids)[0]\n",
    "transcription\n",
    "\n",
    "#'mister quilter is the apostle of the middle classes and we are glad to welcome his gospel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "31a21206",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/s2t-small-librispeech-asr were not used when initializing Speech2TextModel: ['lm_head.weight']\n",
      "- This IS expected if you are initializing Speech2TextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Speech2TextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Speech2TextModel were not initialized from the model checkpoint at facebook/s2t-small-librispeech-asr and are newly initialized: ['model.decoder.embed_positions.weights', 'model.encoder.embed_positions.weights']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** has_prefix_module True expects_prefix_module False\n",
      "*** remove_prefix_from_model False add_prefix_to_model True\n"
     ]
    }
   ],
   "source": [
    "from transformers import Speech2TextModel\n",
    "\n",
    "s2t_model = Speech2TextModel.from_pretrained(\"facebook/s2t-small-librispeech-asr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49667e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d50884f2",
   "metadata": {},
   "source": [
    "Test other methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1cd38a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BART, Speech2Text, Wav2Vec2 don't have pruning\n",
    "#hf_model.prune_heads({1: [0, 2], 2: [2,3 ]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc11e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9778431c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(100, 768)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44be3e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=100, bias=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.get_output_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6f954bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(100, 768)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.resize_token_embeddings(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b17c7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(100, 768)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742e8369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0974c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0b945156",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_ctc.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "59c092a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_naked.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2280ef5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65890c5f",
   "metadata": {},
   "source": [
    "# Verify Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfb327a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 32, 11, 10, 12,  4, 10, 12,  4,  7,  4, 15,  8,  9, 21,  4,  5, 37,\n",
       "         7, 18, 24, 15,  5,  4, 10,  9, 24, 16,  6,  4, 12,  6, 13, 10,  9, 21,\n",
       "         4, 17,  8,  9,  6,  7, 10,  9, 10,  9, 21,  4, 12, 24,  5, 17, 10,  7,\n",
       "        15,  4, 17, 11,  7, 13,  7, 17,  6,  5, 13, 12,  4, 26,  3, 41, 39, 23,\n",
       "         4,  9, 16, 18, 25,  5, 13, 12,  4,  3,  4,  3,  4,  3,  4,  7,  9, 14,\n",
       "         4, 20,  8, 13, 14, 12, 26,  2], dtype=torch.int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_str = \"This is a long example input string containing special characters .$?-, numbers 2872 234 12 and words.\"\n",
    "\n",
    "org_tok_outputs = tokenizer.encode(input_str)\n",
    "tgt_dict.encode_line(org_tok_outputs, add_if_not_exist=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53163c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer = SpeechT5CTCTokenizer.from_pretrained('/home/ksmith/HuggingFace/SpeechT5/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a36a1b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dict = hf_tokenizer.get_vocab()\n",
    "    \n",
    "hf_dict[\"<mask>\"] = 79\n",
    "hf_dict[\"<ctc_blank>\"] = 80\n",
    "\n",
    "#save tokenizer to the current directory\n",
    "\n",
    "#hf_tokenizer.save_vocabulary(\"/home/ksmith/HuggingFace/SpeechT5/tokenizer/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38749c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer = SpeechT5CTCTokenizer.from_pretrained(\"/home/ksmith/HuggingFace/SpeechT5/tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a29fb497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 32, 11, 10, 12,  4, 10, 12,  4,  7,  4, 15,  8,  9, 21,  4,  5, 37,\n",
       "         7, 18, 24, 15,  5,  4, 10,  9, 24, 16,  6,  4, 12,  6, 13, 10,  9, 21,\n",
       "         4, 17,  8,  9,  6,  7, 10,  9, 10,  9, 21,  4, 12, 24,  5, 17, 10,  7,\n",
       "        15,  4, 17, 11,  7, 13,  7, 17,  6,  5, 13, 12,  4, 26,  3, 41, 39, 23,\n",
       "         4,  9, 16, 18, 25,  5, 13, 12,  4,  3,  4,  3,  4,  3,  4,  7,  9, 14,\n",
       "         4, 20,  8, 13, 14, 12, 26,  2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(hf_tokenizer(input_str)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8aa6b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

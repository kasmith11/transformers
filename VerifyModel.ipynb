{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca8cd0a9",
   "metadata": {},
   "source": [
    "# Verifying the SpeechT5 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5342da",
   "metadata": {},
   "source": [
    "I needed to do the following to be able to load the original model:\n",
    "\n",
    "- Clone the https://github.com/microsoft/SpeechT5 repo\n",
    "\n",
    "Install stuff:\n",
    "\n",
    "```\n",
    "pip install editdistance\n",
    "pip install -U sacrebleu==1.5.1\n",
    "\n",
    "git submodule update --init SpeechT5/fairseq\n",
    "cd SpeechT5\n",
    "pip install --editable fairseq/\n",
    "pip install espnet\n",
    "```\n",
    "\n",
    "Put this notebook at the same level as the `SpeechT5` repo.\n",
    "\n",
    "Hack the code:\n",
    "\n",
    "- Copy `speecht5/tasks/speecht5.py` into `fairseq/fairseq/tasks`\n",
    "\n",
    "- To run on CPU: In `speecht5/sequence_generator.py`, comment out where it does `.to(device=\"cuda\")`\n",
    "\n",
    "Additional stuff to download:\n",
    "\n",
    "- `dict.txt` from https://drive.google.com/uc?export=download&id=19hcQ58RHZ6CssxF8Qp6yEF1NW_AXxObK\n",
    "\n",
    "- `tokenizer` from https://drive.google.com/uc?export=download&id=1wClgQjXXoU2lmpbaEa1v2SqMbg7cAutq\n",
    "\n",
    "- `speecht5_base_asr.pt` and `t5_transformer_lm.pt` from https://huggingface.co/ajyy/SpeechT5\n",
    "\n",
    "You also need an input audio file, any WAV at 16 kHz will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a4d7c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65661728",
   "metadata": {},
   "source": [
    "Set Python path so it can find the `speecht5` and `fairseq` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f30402f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../SpeechT5/SpeechT5\")\n",
    "sys.path.insert(0, \"../SpeechT5/SpeechT5/fairseq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d398e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b45926",
   "metadata": {},
   "source": [
    "## Load audio and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c18f317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/AUDIO_DIR/dev_clean/1272/141231/1272-141231-0020.flac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a855d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((69120,), 16000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "wav_data, cur_sample_rate = sf.read(input_file)\n",
    "wav_data.shape, cur_sample_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e60de",
   "metadata": {},
   "source": [
    "NOTE: The `Wav2Vec2FeatureExtractor` does not make sure the audio file is mono. If it has shape `(2, length)` or even `(1, length)` then the output from the feature extractor is incorrect!\n",
    "\n",
    "The `do_normalize` option is False for the SpeechT5 ASR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9699780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(do_normalize=False, return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ea3da78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 69120])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = feature_extractor(wav_data, sampling_rate=cur_sample_rate, padding=True, return_tensors=\"pt\")\n",
    "inputs[\"input_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62e522e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAADLCAYAAAAyeNoLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAzrklEQVR4nO3deViU5foH8O8MyyDqALKjKJgLKoq4Ie4mhcvpZKfjMbM0K03T0rBUWrSy0jrW+ZXHNFvUc6xMPbnkgiJupbixqCiRO2osKrIq28zz+wMZGZmBmeGdGRi+n+viknnned+5H2acuedZZUIIASIiIiIJyK0dABEREdkOJhZEREQkGSYWREREJBkmFkRERCQZJhZEREQkGSYWREREJBkmFkRERCQZJhZEREQkGSYWREREJBkmFkRERCQZsyYWBw8exGOPPQY/Pz/IZDJs3ry51nP279+PHj16QKFQoF27dli9erU5QyQiIiIJmTWxKCoqQkhICJYtW2ZQ+UuXLmHUqFEYOnQokpOTMWvWLLz44ovYtWuXOcMkIiIiicgstQmZTCbDpk2bMHr0aL1l5s6di+3btyMlJUVz7KmnnkJubi5iYmIsECURERHVhb21A6gqPj4eERERWsciIyMxa9YsveeUlJSgpKREc1utViMnJwfu7u6QyWTmCpWIiMjmCCFQUFAAPz8/yOWmdWrUq8QiMzMT3t7eWse8vb2Rn5+Pu3fvokmTJtXOWbRoEd577z1LhUhERGTzrl69ilatWpl0br1KLEwRHR2NqKgoze28vDy0bt0aV69ehVKptGJkREREDUt+fj78/f3RvHlzk69RrxILHx8fZGVlaR3LysqCUqnU2VoBAAqFAgqFotpxpVLJxIKIiMgEdRlKUK/WsQgPD0dcXJzWsdjYWISHh1spIiIiIjKGWROLwsJCJCcnIzk5GUDFdNLk5GSkp6cDqOjGmDBhgqb81KlTcfHiRcyZMwe///47vvzyS6xfvx6vvfaaOcMkIiIiiZg1sThx4gRCQ0MRGhoKAIiKikJoaCjmz58PAMjIyNAkGQAQGBiI7du3IzY2FiEhIfj000/xzTffIDIy0pxhEhERkUQsto6FpeTn58PFxQV5eXkcY0FERGQEKT5D69UYCyIiImrYmFgQERGRZJhYEBERkWSYWBAREZFkmFgQERGRZJhYEBERkWSYWBAREZFkmFgQERGRZJhYEBERkWSYWBAREZFkmFgQERGRZJhYEBERkWSYWBAREZFkmFgQERGRZJhYEBERkWSYWBAREZFkmFgQERGRZJhYEBERkWSYWBAREZFkmFgQERGRZJhYEBERkWSYWBAREZFkmFgQERGRZJhYEBERkWSYWBAREZFkmFgQERGRZJhYEBERkWSYWBAREZFkLJJYLFu2DAEBAXByckJYWBiOHTumt+zq1ashk8m0fpycnCwRJhEREdWR2ROLn376CVFRUViwYAESExMREhKCyMhIZGdn6z1HqVQiIyND83PlyhVzh0lEREQSMHti8dlnn2Hy5MmYNGkSOnfujBUrVsDZ2Rnfffed3nNkMhl8fHw0P97e3uYOkxqAGwUleOLLQ5i78RSEENYOh4iIdDBrYlFaWoqEhARERETcf0C5HBEREYiPj9d7XmFhIdq0aQN/f388/vjjOHPmjN6yJSUlyM/P1/oh2/Tovw4gKT0XP524ilWHLls7HCIi0sGsicXNmzehUqmqtTh4e3sjMzNT5zkdO3bEd999hy1btmDt2rVQq9Xo168frl27prP8okWL4OLiovnx9/eXvB5UP9y+U6b5/f1tZ60YCRER6VPvZoWEh4djwoQJ6N69OwYPHoyff/4Znp6e+Oqrr3SWj46ORl5enubn6tWrFo6YLCE1gy1RREQNgb05L+7h4QE7OztkZWVpHc/KyoKPj49B13BwcEBoaCjOnz+v836FQgGFQlHnWKl+m/Cd/plERERUf5i1xcLR0RE9e/ZEXFyc5pharUZcXBzCw8MNuoZKpcLp06fh6+trrjCpAbhRUGLtEIiIyABm7wqJiorC119/jTVr1iA1NRXTpk1DUVERJk2aBACYMGECoqOjNeXff/997N69GxcvXkRiYiKeeeYZXLlyBS+++KK5Q6UGLjH9Np799ij+yCqwdihERI2WWbtCAGDs2LG4ceMG5s+fj8zMTHTv3h0xMTGaAZ3p6emQy+/nN7dv38bkyZORmZkJNzc39OzZE4cPH0bnzp3NHSrVU4Ul5TqPn8sqQHvv5prbf/vyMABgwrfHcOTNYRaJjaiSWl0xBVoul1k5EiLrkgkbWxAgPz8fLi4uyMvLg1KptHY4JIHPYv/AF3Hnqh23l8tw/qORmtsB87Zrfr+8eJRFYrOm1Ix8lKsEuvgp+WFmZWq1wIjPf4VcLsOOVwdAJuPzQQ2TFJ+hZm+xIKoLIYTOpAIAytU2lRMb5U5pOUZ8/isAILS1Kza93N/KETVuNwtLkHavCy7/bjlcnB2sHBGR9dS76aZEVV3NuWvtEOql3CpreiSl51ovkEbs6MVb+MvSX5GYfhtJV3OtHQ5RvcEWC6rXMvKYWOhSXKaydgiN3tiVRyr+/SoeZar7rWcCjbcljQhgiwXVc5Vv3qTt4U8PaN2e979TVoqEqiYVRMTEgsgmrDt+FRsTdC97T9I7fP6mtUNoEG4UlGDCd8cQk5Jh7VDIgphYUIOWV2Wsga1KuHLboDfm1zectEA0BAAbmMTVKu9uGXp/uAcH/7iBqWsTETBvOxLTb1s7LLIAJhbUoC2OSbV2CGb35PLDmLo2EeezufBXfcHJpLXbcKL6vk1/+/IwSsvVVoiGLImJBdVbyQaMtL9y647O43dKdS+q1ZA9sewwtiRfx9ojV6wdCtWQWdjWykCmU+v5Q3zz20ULR0KWxlkhVG+NXnbI5HOX7j2PucODJIzG+gpKyjFzXXKNZYQQXJzJAmRss6iREAIf7fhd533JnB5t89hiQTbp4o1Ca4dgFZ/sSrN2CES4dLPI2iGQFTGxoAatsrX1qwMXtI7vSc22QjTWt3z/hdoLUZ2xUahmNa2Ky8XEbB8TC6qXjNmhNCn9Nhbt1G52VTXi5b7rIjOvGP+Nv4zL/MZZI07tNd2NghKc+TPP2mGQGTGxIJSp1Ei5nqfZnbE+ePRfBw0qJ5MB2QUlZo6mcYhJyUDfRXF4Z8sZDFmyHza2P6FkrtyqOen6X2LDTTqKy1R4c9Np7P09y6yPk3iF005tGRMLwpT/nMBflv6Gtm/usHYoRjt84ZZNj8LPzi82+pxDJi7eNHVtotbtf+3RvflbY1emqnm65AfbG+4U6O8OXcIPR9Px/OoTdUos/733vIRRUUPDxIKwL+2G5vfa3jTrI1v9Zq1WC/T5KM7o8175MUnrdlFJOfb9no2Scv37i+haSfKLuHM2+7etC1uedVO1i+f51cdNvs7Wk39KEQ41UEwsSEv7t3YiLbNhLcS0cNtZa4dgFmO+ijfpvJyiUhQUV6xIqlYLhC+Kw6TVx/Hu1jM6y/8v4Rqe/uaozvt2nM40KQZbJrfhxOLijfvdPFW/cEjN1HR1/YmrCJi3HUOX7EdmnvGteWQZTCyomk93W3fK4s7Txu0r8KeeN5gsE7oR6pOEOvRDd313N45fzsGqw5eRX1yxWNiPx67inI5BsbNrWAp8S/J1k2OwVXIz5BWl5Wp8+9sl/Cf+MorLVCgoLsP1XOvv7HvgjxtGj73alFT7GBNTG8LmbKzYbO/SzSI8/fURtqjVU0wsqF5RqwWmfZ9Ye0EDTJfoOqYoV6mrJTY1dUWYw5gV8dVac6J/Pg2gYtZM+q07iEs17yA9W1RihiWpl++/gIXbzmL+ljMIeicGXd/djf6L9+LtzaexOcmw5K64TPrX18Tvjhk9GPW1n2rfsybu97pPB794s4izc+oprrxJ1dw1wxuUoWqa/26sk9dyJbuWsZ5aeQQnrtzGkjEh+Djmd9y4N3PljciOeMizGYYH+1glrhNXbuPVH5NQWq5GzJnauzl2n82CSi1gZ46v6Q3Uv2L/kP6ae3Rfc+2RdKw9kg6ZDHi8e0u953+6Ow1L957HT1P6IqytOwDgdlEpnl9zHM6Odlj1XB842pv2PXJnSibG9PKvtdzmpOtwcXYw6JoH/7iB20WlcGvqaHAcKw9WX6Pl298uGRQbWRZbLBq5/+nI+H89Z70toYXJva/Vlams00yqVgucuNeN8fqGk5qkAgD+uSsNU9cm4G6p9ZK3rSf/NCipqHT2z3wzRmNdu85kImDedgTM247CknLsS8vGztMZEELobQHYmWL5cScz1yXX+O186b1ZGO/fa6HKvVOK0IWxSErPxaHztzQtVVXN2XgST399BPnFZTV2KaRm1P78n8sqwKyfkjFpleEDPvPuGrczsa4lwn/PLOBYi3qIiUUjVlKuqrF/XWobTlzFzHVJNe5uaAtdpoZM2zVHs7W52OpYxXc2p+Cl/yZobgcv2IVJq45j2veJCIzegaB3YnA1R/cmd7U5cTlHqjA1Xjfg/2rlc/XGvbEIlf6XeA3l92Z83S1V4ZlvjmL9iWs4fOEWur27GzMemElUlSFjlUwZDyLVf3VD/i5kWewKaYRiUjKwKek6Zg7roLfM3VIVmjjaSfJ4peVqjPv6iGYwohDAF+NCJbl2Q6WqIYMqKC6DuuHN+m0wLtwoxLBPDxhUduAn+zB18EPILijGx092g4OdYd/F/r4iHpcXjzKorDGDjPenZWNIRy+991fOWNG1lolKCNgDGP/NESQ+sBHY9lMZGNNT97gHc62bt+/3bAQOCNR7f0m5CvZyOb7cdx7bTukf0N1Y9wWqz5hY2KgylRpxqVnILiiB0skBo0Mr+mfVaqFZCGnXGf0D9zrNj8HJ+Y8a3Gdak9izWVozHLae/BOfP9Ud5WpR7Y1a31bLDcH/Eq4Z3AI0/ftE/PRSeLXjarVA13d3Sx1anSRcuY3gli7WDkMSZSq1wUlFpRX39qEJb+suybfsB3egfWdzisHnPrfqeI0Ji+yBf7Uft2K9kgeTiqrXtqT3t53F83oSi+IyFXosjEWLpo64drvm1pA/84px8UYh2no2M0eYZAJ2hdiob369hKlrEzF/yxnM+ikZJ+9t/BP87i6Dr/F5nDQrLx7X0Sw8+T8n0OP92Gr9rNbIK45cvIUFW1Jwp7S8Ttcxplvp6CXdTeW/mbhqpjkt0LP+RUO08uBFk899Y+MpzXRHQ+gat1BcpsLQJfsxc939roeHg/S3QOiyYEsKnvjykM5ZRjKZDEIIFOkYw3Pt9l2965XUlan/ba/cKtL5d0rNyMedUlWtSUWlhz89gOk/JNbYzUqWw8TCRu16YHDe48sOobhMhTtGDBr87tAl/PfIlTrHUlBc/QN7T2o2CkrKq317lLrFwpA5+E+tPII18Vcsvgzxf+IvVzs24btjFo2hsdltxKDVuirVsYptXGo2Lt+6gy3J91emNHa2xpr4K0hKz0Wcjh18k6/m4u8rdC+sFvGZcS01xpi93rRxDoP/uR+B0TsQ+a+DyLtTVqcp2dtPZaDD2zvx4prj3LbdyphY2Chd3wJWHDB+S+13NqcgYN52zcAvUyRd1b/Q083CErxaZeCY1GMLltaSLFyo0j9ryOh3Kc3fYjstAQ2FJWcKXb5Z88DPf++taBE0dXDsn/cGTD44ULQuC6vpU1RS8eVArRb4OfEakq/marUO5BSV1un6aVkFCHl/Nzq+HYPXN5ysU5fTntRsvFCH5cip7phY2KiT16pvS/x/ddhUavl+45OSSlWXCdZl68k/ETBvO24XlSJ8sfF7Y9RE3/oAlQqrtKaYcwljQ/xcj3fFjL9wy9oh1NnFG4U4a8HkMfL/at6hd8nuPxAwbzuS9Yx5qM0H21ORd6dMbwuFlCpb0racvI6o9ScxetkhdHh7J45clP51sTHhGr799VKdrnHxZhFu1zHZIdNZJLFYtmwZAgIC4OTkhLCwMBw7VnNz74YNGxAUFAQnJyd07doVO3Y0vF03bc2nJi4KZMw0tNCFsUZ11UihtsTDkqJMbE62hHFfH8Gtwoa7PX3enTI8bOSgTSk82HKoa52WNfGmdzcmpEs/rVXn49xrBdlzVrv7ZfKaE2bZAmC7kcv66xK6MBbvbj0DlbmmtZBeZk8sfvrpJ0RFRWHBggVITExESEgIIiMjkZ2te2rT4cOHMW7cOLzwwgtISkrC6NGjMXr0aKSkGD5yujG7XVRqtv0dPjegxUMIgbw7ZRjx+a/4bHca+i/ea5ZYjFFTM+3+B1opLL3Eta5xFvVVzw/2WDsEk4W8b52ZNuZYpbMqSybiV3PuVPvALygpr7W70ZpWH76MJ5cfRmFJ3QZmk3Fkwsy7uISFhaF3797497//DQBQq9Xw9/fHK6+8gnnz5lUrP3bsWBQVFWHbtm2aY3379kX37t2xYsWKWh8vPz8fLi4uyMvLg1KplKweutZ1KC5TYcWBCxjb2x++Lk0keyxTqNUCi3am4us6NiEa4uibw+CtdNJ8G8vIK8bL3yci+d7Mk/ro7PuRcHasPrs6YN72ascMXX/AkGsZ4oUBgRgR7GORJm2ppL4/XLJ1TsytpFyFjm/HWO3xt0zvjxB/VwghEBjN1ldrWTWpN0rL1ejVxg2uzo4oKC5DU4U9SsvVaKrgyguVpPgMNWtiUVpaCmdnZ2zcuBGjR4/WHJ84cSJyc3OxZcuWaue0bt0aUVFRmDVrlubYggULsHnzZpw8Wb2puKSkBCUl95to8/Pz4e/vL2liEXs2q2J6ZGtXvXPAH2Qnl0neBNfFr6I+57ILNQOnFPZys2yKRPf5KJ2QWWURo26tXODsaIeTV/Pg3qz2efaNgdLJXrOLqi7dWrmg6b3ELv5ev3yfwBYoV6mhUguUqQRKylW4UMt4nDbuzrhy6/6gyCYOdvBsroCzox2UTRxgL5fB7t6PvVyGPTpmThBJqbu/KxT2ctjbyWAnl0MIobUtQlvPprWOM/NROqFUpa51EKx7U0c0c7LX+j/w1xA/fPBEMJROdV9zCJAmsTBrmnbz5k2oVCp4e3trHff29sbvv1df9x0AMjMzdZbPzNQ9TWzRokV47733pAlYj++PVvSBGppUADBLv94ZHXs2MKkwv8wHVkY8VWVgLJOKCjUlFYD236zSMT1redSk6hsqULFhXrqJy24TSaG2ltrakgqg+nuMPreKSnHrgeRj68k/0TuwBZ7t28aga1hCg2//iY6ORlRUlOZ2ZYuFlFY80xP707JRXKbG6et5+Pa3S2iusEeBnn67mcPaw7O5AhsTrknSPdBMYY/hwT4Y3sUH9nYylJSrsSnxOlIz8xE9Igjvbj1r8AuTDNOjtSvauDfFsE5eUAvg//b8gYs3ivBM39YIC3RHabkad8tUcHKww7J95xv1vPmRXX0wpIMX/rk7DcWlKozq5gu3po44+2c+VGqBJo52GBbkBWeFPS7dKIKjvRy375Sii58STg52sJfLYG8nR7lKjRNXbuPw+Zu4WViKYZ28cD67EK1bOGPd8auY1D8Ag9p74trtO/j2t0vwUjphQDsPXLt9B/3becDBTo5ytYBKrUa5SkClFtifdsOoDdeIqpozvCOW7ErTLGve3MkeJeVqdPJpDmUTByjs7fD3ni2hUgPlVV53+cVluFVUCvemjujkq0ROUSne3HQag9p7IqeoVNNq92zfNrh0swjhD7mjqKQceXfLkJ5zB939XXEuqxBP9GiJhdvOoomDHQZ38ESgZ1Nk5Zfgyq2K9xsZgE6+ynqVVAA20BXyIHONsWgovv3tEhbe2+HQHI6/FQHP5gqd9wkhMHblEc03Ua/mCmQXWH8mQe8AN2yY2k/nfQ+Oi5gV0R6zIvTvoVITU8ZYhAW2wH9fCMPCbWclWYzM3Pa9PgQB7s5aS1I3BD8nXrPKrJvf5g5FKzdnze3tpzIw/YdEya4f3FKJlOu2u/uslBLfeQQtjNimvbGS4jPUrLNCHB0d0bNnT8TF3V+bQK1WIy4uDuHh1fdJAIDw8HCt8gAQGxurtzxpe2FAIM68F2mWa6d9MFxvUgFULCe8/qVwXF48CpcWjcSxtyLwfH/9mwxZyrop+l87EZ20u91MTSpM9d8XwuBoL8fbf+lk0cc1xe7XBiHQo2mDSyoA4JHO3rUXMoOqSQUAuDSRph+80vcv9pX0ejX5dc5Q/DJjgMUeT0qXF49iUmFBZp9uGhUVha+//hpr1qxBamoqpk2bhqKiIkyaNAkAMGHCBERHR2vKz5w5EzExMfj000/x+++/491338WJEycwY8YMc4dqM8w1wllhb/gsgMoPn/mPdTb4nFXP9TY6JkPYyfV/EP5rbIhZHtNQlcs5K+zt8OucoVaNpTYdvJtbOwSTNdUxI8gadK1jURcuTRyw4pkekl5TH/8WzujaSnszum2v1P9E46Mnulo7hEbH7InF2LFjsWTJEsyfPx/du3dHcnIyYmJiNAM009PTkZFxf250v3798MMPP2DlypUICQnBxo0bsXnzZgQHB5s7VJsi9ZfKXm3cTD73pUFta7zfwU6GnTMHYqiRmzEZYkotj91copHUpmjvpb0bo38LZzzVW9rxQVIZ0M7D2iHUiVwuQ9oHw7FhqnVbPh8c033ho5HYEzW4TtccHuxbp/MNsWV6f83vlcnEqG6+CG7pglPvPir54/0wOUyS66ye1Bvj+tTP/1O2zCJp/IwZM/S2OOzfv7/asTFjxmDMmDFmjsq2fTA6GG9tqvuiYl38lAjxd8UHj5ue2NW2ydK5D0dqfl/wWGe894t0Y0QMGdQU2cUbu85kYf/rQ+r0WL/OGYqBn+wzuPyOmQOrHVv8ZDe8NPghDF2yv06xSG25hb4Vm5PC3g4KIzf8klrVfP/QvIdhJ5ehnZdp233/ONly3SAh/q6a34NbuuDchyPgYFfxt5RqmmMlmQzwq+O6QO29mmH7qwON3uCNpMG/uo16uk/rasfe+Yvh3RIAsGvWIGx/dSA+eqIr5DV0J9QmuKVLtWN+Lk4AgCVjtLsiJoYHmPw4uvi3cK61zFfP9sLlxaMQ4NHU7I9VVeUb84MC6xiHOVizZUdKpg5VT3rnEfQJbGHUOetfqt46Ev6QO4JbKvG3Hi3R0rVuH57hD7lrfn971P0xOp/8vVudrmsIfa9dKSS+/YjJHUa/zhmKA28MQWzUYCYVVlQ/Oh5Jcg8OsHvl4XYI9jNuhK9UH3APpiQezRyxZ/ZgXLt9t1q/fV0SGKLaqE3ILH5+uR/cmjriy/E90MuIZc11JSIOdnJse6V6S5WxHmzen9gvABl5xQj0aIp/9PLHnI2n6vwYxjBkEShDRHTyhltTR5OSgqXjQo1O7sk8mNI1ErMf7YjeAYZ/41o4OliyjH9QB0+tb2dLxoTA2dG+QQ8GNLfzH46wdgg2qdzIheu+HN8DPVpXjC/yaKZ/RpQlje7uh0V/026VcLCT452/dMYz97r+dLWWmNPy8T0luc6/nw4FUDEA/dc5QxEf/TD6tjXsfctaM3+oOiYWNmzZ09r94nK5DNOGPGTQuVIuuOLkYIeDc4Yi9rVB+PfToRjcwVOya9c3Dxs4ALW2wbX2dnKM7OojQUR159/CuvvgSKnUwJVqt786AJcXj8LIrtoDI5eOCzVHWEb50IBZDt5KyyZBHX3q/iUh9rVBcHK4P/PMv4UzfF2aYFItU9b9XJzw+VPdtc4l62JiYcNGdfPFtlcG4HSVUdvBftXHO1iCnVyG9t7N8Zdufg1yHQRDrXzWsG9uo7u3rLVMc0X9GNcgt6HnK8jAD8Auev6fPBbiZ9D5/32hj8ExGWNgew+DppO3ca/ejbl1Rn+smqR/SvfC0boHaHvVsHaNFL4YF4oLH41Eez0tmLWNRTkcPQyPG/D/iSyHiYWNC27pojXwrr58C7YEQz/kpWRvJ0faB8NrLffuX7vUWmb60HZShFRncyKDrB2CZNwt1J3RvcosCikZsybDg1OXu7VyxdCOXljz/P2kZ+HoYEzqH4CNU8PRVs+YqhcHGrbI3bopxs9SOfrmMPw1xK/GtWaCW7rgzZFB6OB9f/bM+pfCsXpSb8RHP2z0Y5L5MbFoZGQyGWbc+8CaGN4G3VpZpwXDEh7tYp0kSmFvh4WP15w4GLICYys307ogTrwdYdJ5+ozqZv51Eixp0d9q/nCeO7zuiZQ5WuXC27obNTjxpcH3uz0TqrwmBrX3wLopfXH8rQg827cNFjzWBb0CWuidMdPLwLFZfdu6Y5iRa9F4K50MKjdl0EPY/dpgLHy8C1ZP6o0+gS0wpKMXfOs4LZXMg4lFIxT1SAfsmjUICx7rorXwTSWPZtZd+nazjpgamqfD9I9RqTo1sCamzJDp6N0cHs0UeNnAsTS1+c/z5mnSt6aa/qq92rjVOg7JkMWbHI2cjtnagIShb1v3WstUFejRFGkfDMflxaO0WmpkMhn6tnWvtjy/vZ3uv4yD3PC6fDOxFxYYsdqusZ4ND8CQjtIvpEfSYmLRCMnlMnT0aQ65XAaZTIb5f+mMMT1bYfdrg7DimR7YOXOQVePr7u9qlW4MKelr2g1wd8aLA2teDbQqJwfj/otOHVJx7TciOxp1nj6DbHCgbU0f0Bun6d6srqp+D9W+CqmDng9pfQ4asJx7aGtXo64JGLcMf5+AFhjSUfv5tpfLENzS8GnqMpkMk/oH4rl+AZpjD65VQ7aP61gQnh9wvw+1vkwB7VGHJcTrs5hZxiVtY3v5Y0284buejri3vLMtD5CtqwCPpvhpSl+MXXlEc8xbqcCBN6TZq8XN2UHyv39zJ3uzJ3lyuQyrJ/VB8IJdKCwpBwCc/2hkLWfp9taoTni0izd6tHZDes4dKcOkBoAtFlQvuUq8C6Q1/PCidpP5pUUjjZ4SF9ra8ARryqC2nHJnoLC27ppdeJPeeQRH34ww6m9X0+Z1PSVOip0c5HXeT8QYHz5R932ZHOzk6PeQB5wc7ODsqPvvOqOeDE4m6TGxoHrJ3k6uNXq9IerXzgOvPNwOrs4O+HXOULO3IqgeWPyprmNVpFzLpL6SyWRwM2E77UHt9bcePPg81MXlxaPw+8IRBg9ylEKov7SJUSs3Z8wZXr1rLuqRDpI+DtUf7AqhemtwB0+0aOqInKJSa4distmPdsRrER1MXqrcmFzkweWqu/u7Yu/swXj40wO1nuvm7IDbd8q0jo0O5doA+tS0loSpaUXC2xEoKlHB20WBnxOvW21H2dbuzlj7QhjcmkrXavjykHa4mnMHPx67qjnG5fttFxMLqtf2RA1Gj4Wx1g6jTiz1BqrW8U25lZv2bINRXX2xIyUDsa8NQjuv5igtVyO7oBiezRWIWn8S209lAKjY1VbqJn1bUlO3iandeO7NFHC/t1TDOB2bCFrSgPbSJzXGDCSlho1dIVSvtWjqiJT3Iqsdr9wLYVRXX60R6I2Zj445/Y72csyu0uS8bHwPpC0cgXZezTX3t3JzhsLeDsue7oG0D4Zj58yB2PbKAIvFbWveHGnYdOLGZvrQdnBzdkBzJ3v8NleagbJUP7HFguq9Zgp7LBwdjN1nMlFcpsL4sDboE9gCSe88AldnByRfzcXqw5e1zmmMkyIm9Q/Qefzloe2QmV+MXgEVLRA1bS6nsLdDJ1/jdsFtrF4a1BZfHbxY7biXBcdDNCSezRVIfOcRzlhqBJhYUIPwbN821QYTVg66a+vRrFr5fbOHWCIss7tRUGJQuU/+3k1v87ydXGbQxlVknAHtPXQmFqQfk4rGgV0h1OC5ODvgxQHa+xkE6Nn3oKE5eimn1jIxswZiTM9WFoiGqhpYw8wQosaMiQXZBFvd3fDB/vr9rw+pVibIR8lvglby4LhcS29XTlQfMbEgm9C1ymZqz/c3bDfGhiDwgZaXAI+meLIHWyfqi59f1l4rZNPLDX+fG6K64hgLshnbXhmAhCu3MdHGZokoneyRX1yuuV21ceLv7AKxqu7+rri8eBRUalHj1t9EjQkTC7IZwS1dENzS9raB3xM1GK+uS8K0IRVLIPtXWZvikye7WSssqoJJBdF9TCyI6jkvpRPWTQnX3J4yqC1uFpbg0S7eXL2QiOodJhZEDUwTRzssHF33jaKIiMyBgzeJiIhIMkwsiIiISDJMLIiIiEgyZk0scnJyMH78eCiVSri6uuKFF15AYWFhjecMGTIEMplM62fq1KnmDJOIiIgkYtbBm+PHj0dGRgZiY2NRVlaGSZMmYcqUKfjhhx9qPG/y5Ml4//33NbednZ1rKE1ERET1hdkSi9TUVMTExOD48ePo1asXAGDp0qUYOXIklixZAj8/P73nOjs7w8fHx1yhERERkZmYrSskPj4erq6umqQCACIiIiCXy3H06NEaz/3+++/h4eGB4OBgREdH486dO+YKk4iIiCRkthaLzMxMeHl5aT+YvT1atGiBzMxMvec9/fTTaNOmDfz8/HDq1CnMnTsXaWlp+Pnnn3WWLykpQUnJ/a2l8/PzpakAERERGc3oxGLevHn4+OOPayyTmppqckBTpkzR/N61a1f4+vpi2LBhuHDhAh566KFq5RctWoT33nvP5McjIiIi6RidWMyePRvPPfdcjWXatm0LHx8fZGdnax0vLy9HTk6OUeMnwsLCAADnz5/XmVhER0cjKipKczs/Px/+/v4GX5+IiIikY3Ri4enpCU9Pz1rLhYeHIzc3FwkJCejZsycAYO/evVCr1ZpkwRDJyckAAF9fX533KxQKKBQKg69HRERE5mO2wZudOnXC8OHDMXnyZBw7dgyHDh3CjBkz8NRTT2lmhFy/fh1BQUE4duwYAODChQtYuHAhEhIScPnyZWzduhUTJkzAoEGD0K0bd3EkIiKq78y6QNb333+PoKAgDBs2DCNHjsSAAQOwcuVKzf1lZWVIS0vTzPpwdHTEnj178OijjyIoKAizZ8/Gk08+iV9++cWcYRIREZFEZEIIYe0gpJSfnw8XFxfk5eVBqVRaOxwiIqIGQ4rPUO4VQkRERJJhYkFERESSYWJBREREkmFiQURERJJhYkFERESSYWJBREREkmFiQURERJJhYkFERESSYWJBREREkmFiQURERJJhYkFERESSYWJBREREkmFiQURERJJhYkFERESSYWJBREREkmFiQURERJJhYkFERESSYWJBREREkmFiQURERJJhYkFERESSYWJBREREkmFiQURERJJhYkFERESSYWJBREREkmFiQURERJJhYkFERESSYWJBREREkmFiQURERJIxW2Lx4Ycfol+/fnB2doarq6tB5wghMH/+fPj6+qJJkyaIiIjAuXPnzBUiERERScxsiUVpaSnGjBmDadOmGXzOJ598gi+++AIrVqzA0aNH0bRpU0RGRqK4uNhcYRIREZGEZEIIYc4HWL16NWbNmoXc3Nwaywkh4Ofnh9mzZ+P1118HAOTl5cHb2xurV6/GU089ZdDj5efnw8XFBXl5eVAqlXUNn4iIqNGQ4jPUXuKYTHbp0iVkZmYiIiJCc8zFxQVhYWGIj4/Xm1iUlJSgpKREczsvLw9AxR+HiIiIDFf52VmXNod6k1hkZmYCALy9vbWOe3t7a+7TZdGiRXjvvfeqHff395c2QCIiokaioKAALi4uJp1rVGIxb948fPzxxzWWSU1NRVBQkEnBmCI6OhpRUVGa22q1Gjk5OXB3d4dMJpPkMfLz8+Hv74+rV682qu6Vxlhv1pl1tlWsc+OoM1C3egshUFBQAD8/P5Mf36jEYvbs2XjuuedqLNO2bVuTAvHx8QEAZGVlwdfXV3M8KysL3bt313ueQqGAQqHQOmboLBRjKZXKRvXirNQY6806Nw6sc+PQGOsMmF5vU1sqKhmVWHh6esLT07NOD6hPYGAgfHx8EBcXp0kk8vPzcfToUaNmlhAREZH1mG26aXp6OpKTk5Geng6VSoXk5GQkJyejsLBQUyYoKAibNm0CAMhkMsyaNQsffPABtm7ditOnT2PChAnw8/PD6NGjzRUmERERSchsgzfnz5+PNWvWaG6HhoYCAPbt24chQ4YAANLS0jSzOABgzpw5KCoqwpQpU5Cbm4sBAwYgJiYGTk5O5grTIAqFAgsWLKjW5WLrGmO9WefGgXVuHBpjnQHr19vs61gQERFR48G9QoiIiEgyTCyIiIhIMkwsiIiISDJMLIiIiEgyTCwMsGzZMgQEBMDJyQlhYWE4duyYtUPS6eDBg3jsscfg5+cHmUyGzZs3a91vyLb0OTk5GD9+PJRKJVxdXfHCCy9oTREGgFOnTmHgwIFwcnKCv78/Pvnkk2qxbNiwAUFBQXByckLXrl2xY8cOyesLVCzp3rt3bzRv3hxeXl4YPXo00tLStMoUFxdj+vTpcHd3R7NmzfDkk08iKytLq0x6ejpGjRoFZ2dneHl54Y033kB5eblWmf3796NHjx5QKBRo164dVq9eXS0eS7xWli9fjm7dumkWvwkPD8fOnTtttr66LF68WDNFvZKt1fvdd9+FTCbT+qm6qrGt1beq69ev45lnnoG7uzuaNGmCrl274sSJE5r7be29LCAgoNpzLZPJMH36dAAN8LkWVKN169YJR0dH8d1334kzZ86IyZMnC1dXV5GVlWXt0KrZsWOHeOutt8TPP/8sAIhNmzZp3b948WLh4uIiNm/eLE6ePCn++te/isDAQHH37l1NmeHDh4uQkBBx5MgR8euvv4p27dqJcePGae7Py8sT3t7eYvz48SIlJUX8+OOPokmTJuKrr77SlDl06JCws7MTn3zyiTh79qx4++23hYODgzh9+rTkdY6MjBSrVq0SKSkpIjk5WYwcOVK0bt1aFBYWaspMnTpV+Pv7i7i4OHHixAnRt29f0a9fP8395eXlIjg4WERERIikpCSxY8cO4eHhIaKjozVlLl68KJydnUVUVJQ4e/asWLp0qbCzsxMxMTGaMpZ6rWzdulVs375d/PHHHyItLU28+eabwsHBQaSkpNhkfR907NgxERAQILp16yZmzpypOW5r9V6wYIHo0qWLyMjI0PzcuHHDZutbKScnR7Rp00Y899xz4ujRo+LixYti165d4vz585oytvZelp2drfU8x8bGCgBi3759QoiG91wzsahFnz59xPTp0zW3VSqV8PPzE4sWLbJiVLV7MLFQq9XCx8dH/POf/9Qcy83NFQqFQvz4449CCCHOnj0rAIjjx49ryuzcuVPIZDJx/fp1IYQQX375pXBzcxMlJSWaMnPnzhUdO3bU3P7HP/4hRo0apRVPWFiYeOmllyStoy7Z2dkCgDhw4IAQoqKODg4OYsOGDZoyqampAoCIj48XQlQkZHK5XGRmZmrKLF++XCiVSk0958yZI7p06aL1WGPHjhWRkZGa29Z8rbi5uYlvvvnG5utbUFAg2rdvL2JjY8XgwYM1iYUt1nvBggUiJCRE5322WN9Kc+fOFQMGDNB7f2N4L5s5c6Z46KGHhFqtbpDPNbtCalBaWoqEhAStrdzlcjkiIiIQHx9vxciMV9u29AAQHx8PV1dX9OrVS1MmIiICcrkcR48e1ZQZNGgQHB0dNWUiIyORlpaG27dva8pUfZzKMpb4m1UuuNaiRQsAQEJCAsrKyrTiCQoKQuvWrbXq3bVrV62ddSMjI5Gfn48zZ85oytRUJ2u9VlQqFdatW4eioiKEh4fbfH2nT5+OUaNGVYvNVut97tw5+Pn5oW3bthg/fjzS09Ntur4AsHXrVvTq1QtjxoyBl5cXQkND8fXXX2vut/X3stLSUqxduxbPP/88ZDJZg3yumVjU4ObNm1CpVEZv5V4fGbItfWZmJry8vLTut7e3R4sWLbTK6LpG1cfQV8bcfzO1Wo1Zs2ahf//+CA4O1sTi6OhYbWO6B+ttap3y8/Nx9+5di79WTp8+jWbNmkGhUGDq1KnYtGkTOnfubLP1BYB169YhMTERixYtqnafLdY7LCwMq1evRkxMDJYvX45Lly5h4MCBKCgosMn6Vrp48SKWL1+O9u3bY9euXZg2bRpeffVVzUrOtv5etnnzZuTm5mo2/GyIz7XZlvQmsrTp06cjJSUFv/32m7VDMbuOHTsiOTkZeXl52LhxIyZOnIgDBw5YOyyzuXr1KmbOnInY2FirL/FvKSNGjND83q1bN4SFhaFNmzZYv349mjRpYsXIzEutVqNXr1746KOPAFRsB5GSkoIVK1Zg4sSJVo7O/L799luMGDGiTtuWWxtbLGrg4eEBOzu7aqNvs7KyNNu8NxRVt6WvqmpdfHx8kJ2drXV/eXk5cnJytMroukbVx9BXxpx/sxkzZmDbtm3Yt28fWrVqpTnu4+OD0tJS5Obm6o2nLnVSKpVo0qSJxV8rjo6OaNeuHXr27IlFixYhJCQEn3/+uc3WNyEhAdnZ2ejRowfs7e1hb2+PAwcO4IsvvoC9vT28vb1tst5Vubq6okOHDjh//rzNPs8A4Ovri86dO2sd69Spk6YbyJbfy65cuYI9e/bgxRdf1BxriM81E4saODo6omfPnoiLi9McU6vViIuLQ3h4uBUjM17VbekrVW5LX1mX8PBw5ObmIiEhQVNm7969UKvVCAsL05Q5ePAgysrKNGViY2PRsWNHuLm5acpUfZzKMub4mwkhMGPGDGzatAl79+5FYGCg1v09e/aEg4ODVjxpaWlIT0/Xqvfp06e13ohiY2OhVCo1b3C11cnarxW1Wo2SkhKbre+wYcNw+vRpzS7JycnJ6NWrF8aPH6/53RbrXVVhYSEuXLgAX19fm32eAaB///7Vpoz/8ccfaNOmDQDbfS8DgFWrVsHLywujRo3SHGuQz7VRQz0boXXr1gmFQiFWr14tzp49K6ZMmSJcXV21Rt/WFwUFBSIpKUkkJSUJAOKzzz4TSUlJ4sqVK0KIiilarq6uYsuWLeLUqVPi8ccf1zlFKzQ0VBw9elT89ttvon379lpTtHJzc4W3t7d49tlnRUpKili3bp1wdnauNkXL3t5eLFmyRKSmpooFCxaYbbrptGnThIuLi9i/f7/WdK07d+5oykydOlW0bt1a7N27V5w4cUKEh4eL8PBwzf2VU7UeffRRkZycLGJiYoSnp6fOqVpvvPGGSE1NFcuWLdM5VcsSr5V58+aJAwcOiEuXLolTp06JefPmCZlMJnbv3m2T9dWn6qwQW6z37Nmzxf79+8WlS5fEoUOHREREhPDw8BDZ2dk2Wd9Kx44dE/b29uLDDz8U586dE99//71wdnYWa9eu1ZSxxfcylUolWrduLebOnVvtvob2XDOxMMDSpUtF69athaOjo+jTp484cuSItUPSad++fQJAtZ+JEycKISqmab3zzjvC29tbKBQKMWzYMJGWlqZ1jVu3bolx48aJZs2aCaVSKSZNmiQKCgq0ypw8eVIMGDBAKBQK0bJlS7F48eJqsaxfv1506NBBODo6ii5duojt27ebpc666gtArFq1SlPm7t274uWXXxZubm7C2dlZPPHEEyIjI0PrOpcvXxYjRowQTZo0ER4eHmL27NmirKxMq8y+fftE9+7dhaOjo2jbtq3WY1SyxGvl+eefF23atBGOjo7C09NTDBs2TJNU2GJ99XkwsbC1eo8dO1b4+voKR0dH0bJlSzF27FittRxsrb5V/fLLLyI4OFgoFAoRFBQkVq5cqXW/Lb6X7dq1SwCoVg8hGt5zzW3TiYiISDIcY0FERESSYWJBREREkmFiQURERJJhYkFERESSYWJBREREkmFiQURERJJhYkFERESSYWJBREREkmFiQURERJJhYkFERESSYWJBREREkmFiQURERJL5f8KejKmjL2b8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 600x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(6, 2))\n",
    "plt.plot(inputs[\"input_values\"][0])\n",
    "plt.ylim(-1, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f594f3",
   "metadata": {},
   "source": [
    "Transform multiple inputs into a single padded batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbb99431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128632,), 16000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file2 = \"/Users/matthijs/Documents/FILES/HuggingFace/S2S/textless/AUDIO_DIR/selfdestruct.wav\"\n",
    "wav_data2, cur_sample_rate2 = sf.read(input_file2)\n",
    "wav_data2.shape, cur_sample_rate2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1daee2cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 128632)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs2 = feature_extractor([wav_data, wav_data2], sampling_rate=cur_sample_rate, padding=True)\n",
    "inputs2[\"input_values\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb653d87",
   "metadata": {},
   "source": [
    "The original model used a `padding_mask` as input, where False means no padding. The `Wav2Vec2FeatureExtractor` can return an `attention_mask`, where 1 means no padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d3ab7e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 1, 1, ..., 0, 0, 0], dtype=int32),\n",
       " array([1, 1, 1, ..., 1, 1, 1], dtype=int32)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs2[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2205900b",
   "metadata": {},
   "source": [
    "## Load the Transformers model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc8c0f8",
   "metadata": {},
   "source": [
    "To convert the original checkpoint weights to Transformers:\n",
    "\n",
    "First download the checkpoint. I used `speecht5_base_asr.pt` from https://huggingface.co/ajyy/SpeechT5\n",
    "\n",
    "Then run the following, using your own `--checkpoint_path` and `--pytorch_dump_folder_path`:\n",
    "\n",
    "```nohighlight\n",
    "cd transformers/src/transformers/models/speecht5\n",
    "\n",
    "python convert_speecht5_original_pytorch_checkpoint_to_pytorch.py \\\n",
    "  --task s2t \\\n",
    "  --checkpoint_path /path/to/SpeechT5/speecht5_base_asr.pt \n",
    "  --pytorch_dump_folder_path /some/other/path\n",
    "```\n",
    "\n",
    "NOTE: The conversion script is currently incomplete, so it prints a warning about \"Unexpected key(s) in state_dict\". This is normal at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "552fd307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    SpeechT5Config, \n",
    "    SpeechT5CTCTokenizer,\n",
    "    SpeechT5Processor,\n",
    "    SpeechT5Model, \n",
    "    SpeechT5ForCTC, \n",
    "    Wav2Vec2FeatureExtractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69aa6aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SpeechT5Config()\n",
    "hf_model = SpeechT5Model(config)\n",
    "#hf_model = SpeechT5ForCTC(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ba8f514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SpeechT5Model were not initialized from the model checkpoint at /Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_base_asr and are newly initialized: ['speecht5.speech_encoder_prenet.embed_positions.weights']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_base_asr\"\n",
    "hf_model = SpeechT5Model.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2884601a",
   "metadata": {},
   "source": [
    "Note that loading should work OK for both the base class `SpeechT5Model` and `SpeechT5ForCTC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5df4bac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hf_model = SpeechT5ForCTC.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7dd7606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeechT5Model(\n",
       "  (speech_encoder_prenet): SpeechT5SpeechEncoderPrenet(\n",
       "    (feature_encoder): SpeechT5FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): SpeechT5GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1): SpeechT5NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (2): SpeechT5NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (3): SpeechT5NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (4): SpeechT5NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5): SpeechT5NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (6): SpeechT5NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): SpeechT5FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (pos_conv_embed): SpeechT5PositionalConvEmbedding(\n",
       "      (conv): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "      (padding): SpeechT5SamePadLayer()\n",
       "      (activation): GELUActivation()\n",
       "    )\n",
       "    (embed_positions): SpeechT5SinusoidalPositionalEmbedding()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82233c52",
   "metadata": {},
   "source": [
    "Run a single forward pass. This should run the encoder, decoder, and the relevant pre- and postnets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29bd2f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.Wav2Vec2BaseModelOutput"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    hf_outputs = hf_model(**inputs)\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "359778a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_hidden_state', 'extract_features']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97a77055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 215, 512])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"extract_features\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7db34f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6318,  0.1265, -0.8733,  ...,  0.2677, -0.3374,  0.0058],\n",
       "         [-1.3335, -0.7106, -0.6797,  ...,  0.0437,  0.1427,  0.5040],\n",
       "         [-1.8898, -0.6588,  0.4209,  ...,  0.0444, -0.2682,  0.5834],\n",
       "         ...,\n",
       "         [-1.0058, -0.7280, -0.7392,  ...,  0.1703, -0.5074, -0.1430],\n",
       "         [ 0.0079, -0.0941, -0.6969,  ..., -0.1035, -0.5059, -0.0286],\n",
       "         [ 0.5507, -0.3121, -0.4363,  ..., -0.1503, -0.2760,  0.1200]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"extract_features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41afd19a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 215, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcef1339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.3469, -3.7927, 10.4827,  ...,  1.3639,  2.2972,  0.8101],\n",
       "         [ 8.8800, -3.5574, 12.9277,  ...,  0.1729,  1.6734,  0.6476],\n",
       "         [-1.9185,  3.3586, -0.4006,  ...,  0.9983, -0.4333,  2.5262],\n",
       "         ...,\n",
       "         [ 7.2030, -5.8764,  6.5531,  ...,  1.0349,  4.1734,  1.4926],\n",
       "         [ 0.1941, -3.2425,  6.6313,  ..., -0.6757,  2.3840,  1.2404],\n",
       "         [ 2.9908, -2.6358,  6.9353,  ...,  0.4324,  2.7151,  1.5720]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a297ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93e86471",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hf_model.speech_encoder_prenet.embed_positions(padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a416d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64823660",
   "metadata": {},
   "source": [
    "## Load the original model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1697ab0",
   "metadata": {},
   "source": [
    "Load the dictionary. This adds `<s>, <pad>, </s>, <unk>` tokens to the front and `<mask>` and `<ctc_blank>` to the end. **dict.txt** was [downloaded from here](https://drive.google.com/uc?export=download&id=19hcQ58RHZ6CssxF8Qp6yEF1NW_AXxObK). This is the Vocabulary link from the main SpeechT5 README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24e0f251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size: 81\n"
     ]
    }
   ],
   "source": [
    "from fairseq.data import Dictionary\n",
    "tgt_dict = Dictionary.load(\"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/DATA_ROOT/dict.txt\")\n",
    "tgt_dict.add_symbol(\"<mask>\")\n",
    "tgt_dict.add_symbol(\"<ctc_blank>\")\n",
    "print(f\"dictionary size: \" f\"{len(tgt_dict):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85935250",
   "metadata": {},
   "source": [
    "To load the model we need the `SpeechT5Task` object but constructing it is annoying. Fortunately, `build_model` only reads two properties from the task object, so we can fake it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea4b4380",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeTask:\n",
    "    def __init__(self):\n",
    "        self.dicts = { \"text\": tgt_dict }\n",
    "        self.t5_task = \"s2t\"\n",
    "        \n",
    "task = FakeTask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dc6c7a",
   "metadata": {},
   "source": [
    "Load the fine-tuned ASR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a79fbfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from speecht5.models.speecht5 import T5TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9b2818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"../weights/SpeechT5/speecht5_base_asr.pt\")\n",
    "\n",
    "orig_model = T5TransformerModel.build_model(checkpoint[\"cfg\"][\"model\"], task)\n",
    "\n",
    "orig_model.load_state_dict(checkpoint[\"model\"])\n",
    "orig_model = orig_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bad44136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(checkpoint[\"model\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c19892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56503f04",
   "metadata": {},
   "source": [
    "## Verify speech encoder prenet output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e2f9ff",
   "metadata": {},
   "source": [
    "This first uses the `speech_encoder_prenet` to convert the raw audio data into embeddings of shape `(batch, sequence_length, 768)`. The sequence length is roughly `number of audio samples / 320`, so there is one vector every 20 ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01638115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 69120])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = inputs[\"input_values\"]\n",
    "source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35b0a089",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_mask = torch.BoolTensor(source.shape).fill_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31d1d2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input, encoder_padding_mask = orig_model.speech_encoder_prenet(\n",
    "    source, padding_mask=padding_mask, mask=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36334be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_input = orig_model.speech_encoder_prenet.feature_extractor(source)\n",
    "# encoder_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26c4a65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 215, 768]), torch.Size([1, 215]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input.shape, encoder_padding_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "058de538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.4147, -3.6930, 10.6119,  ...,  1.3639,  2.2972,  0.8101],\n",
       "         [ 8.1118, -4.2742, 12.2622,  ...,  0.1729,  1.6734,  0.6476],\n",
       "         [-2.8165,  2.4560, -1.3008,  ...,  0.9983, -0.4333,  2.5262],\n",
       "         ...,\n",
       "         [ 8.1539, -5.4359,  5.8689,  ...,  1.0349,  4.1734,  1.4926],\n",
       "         [ 0.8112, -3.6817,  5.7369,  ..., -0.6757,  2.3840,  1.2404],\n",
       "         [ 2.7066, -3.5683,  6.5834,  ...,  0.4324,  2.7151,  1.5720]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c07884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dbc6dd",
   "metadata": {},
   "source": [
    "If the weights and model were converted correctly, this should report zero or a very small number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b099ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9589)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(encoder_input - hf_outputs[\"last_hidden_state\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d18286fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.abs(encoder_input - hf_outputs[\"last_hidden_state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a93e58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b76dc23",
   "metadata": {},
   "source": [
    "## Verify Transformer encoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47c86f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    encoder_output = orig_model.encoder(encoder_input, encoder_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "41ff76cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_out shape torch.Size([215, 1, 768])\n",
      "encoder_padding_mask shape torch.Size([1, 215])\n",
      "encoder_states []\n",
      "src_tokens []\n",
      "decoder_input [None]\n",
      "encoder_out_for_ctc shape torch.Size([215, 1, 81])\n"
     ]
    }
   ],
   "source": [
    "print(\"encoder_out shape\", encoder_output[\"encoder_out\"][0].shape)\n",
    "print(\"encoder_padding_mask shape\", encoder_output[\"encoder_padding_mask\"][0].shape)\n",
    "print(\"encoder_states\", encoder_output[\"encoder_states\"])  # []\n",
    "print(\"src_tokens\", encoder_output[\"src_tokens\"])  # []\n",
    "print(\"decoder_input\", encoder_output[\"decoder_input\"])  # [None]\n",
    "print(\"encoder_out_for_ctc shape\", encoder_output[\"encoder_out_for_ctc\"][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb74f0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compare to SpeechT5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a747e547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b17bb840",
   "metadata": {},
   "source": [
    "## Verify text decoder prenet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9ba48d",
   "metadata": {},
   "source": [
    "First this calls `text_decoder_prenet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "28dcb259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_size = 5\n",
    "tokens = torch.tensor([2] * beam_size).reshape(beam_size, -1)\n",
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e620930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental_state = {}  # no incremental state on first step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f09f744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    prev_output_tokens, tgt_mask, incremental_state = orig_model.text_decoder_prenet(tokens, incremental_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "725cfc62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 768])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_output_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84764c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "81026d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incremental_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "02534868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compare to SpeechT5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bd382e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fec354d",
   "metadata": {},
   "source": [
    "## Verify Transformer decoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "42a4fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, reorganize the dictionary into a dictionary of lists\n",
    "bsz = 1\n",
    "new_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)\n",
    "encoder_outs = orig_model.encoder.reorder_encoder_out(encoder_output, new_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6229b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    decoder_output, extra = orig_model.decoder(\n",
    "        prev_output_tokens,\n",
    "        tgt_mask,\n",
    "        encoder_out=encoder_outs,\n",
    "        incremental_state=incremental_state,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0a190958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 768])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "05117956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([5, 1, 215])]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in extra[\"attn\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b593692b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 5, 768]),\n",
       " torch.Size([1, 5, 768]),\n",
       " torch.Size([1, 5, 768]),\n",
       " torch.Size([1, 5, 768]),\n",
       " torch.Size([1, 5, 768]),\n",
       " torch.Size([1, 5, 768]),\n",
       " torch.Size([1, 5, 768])]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in extra[\"inner_states\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "43910324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f9fd134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.max(np.abs(extra[\"attn\"][0].numpy() - expected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f4d8331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted = [x.numpy() for x in extra[\"inner_states\"]]\n",
    "# for i in range(len(expected)):\n",
    "#     print(np.max(np.abs(predicted[i] - expected[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d634e0a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d9113f7",
   "metadata": {},
   "source": [
    "## Verify text decoder postnet output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a20be43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    lprobs = orig_model.text_decoder_postnet(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a7a143bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 81])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lprobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b5279b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24926eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32e98799",
   "metadata": {},
   "source": [
    "## Use the `transformers` generator loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b889ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19f0f58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf06a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

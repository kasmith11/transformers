{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca8cd0a9",
   "metadata": {},
   "source": [
    "# Verifying the SpeechT5 TTS model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5342da",
   "metadata": {},
   "source": [
    "I needed to do the following to be able to load the original model:\n",
    "\n",
    "- Clone the https://github.com/microsoft/SpeechT5 repo\n",
    "\n",
    "Install stuff:\n",
    "\n",
    "```\n",
    "pip install editdistance\n",
    "pip install -U sacrebleu==1.5.1\n",
    "\n",
    "git submodule update --init SpeechT5/fairseq\n",
    "cd SpeechT5\n",
    "pip install --editable fairseq/\n",
    "pip install espnet\n",
    "```\n",
    "\n",
    "Hack the code:\n",
    "\n",
    "- Copy `speecht5/tasks/speecht5.py` into `fairseq/fairseq/tasks`\n",
    "\n",
    "- `checkpoint_utils.py`: comment out a bunch of the loading stuff because of missing keys in the checkpoint\n",
    "\n",
    "Additional stuff to download:\n",
    "\n",
    "- `dict.txt` from https://drive.google.com/uc?export=download&id=19hcQ58RHZ6CssxF8Qp6yEF1NW_AXxObK\n",
    "\n",
    "- `tokenizer` from https://drive.google.com/uc?export=download&id=1wClgQjXXoU2lmpbaEa1v2SqMbg7cAutq\n",
    "\n",
    "- `xvectors.zip` from https://drive.google.com/uc?export=download&id=16QOUURZBrW7-GYbVG_gXt3mTMlZmQoH0\n",
    "\n",
    "- everything from https://huggingface.co/mechanicalsea/speecht5-tts\n",
    "\n",
    "- https://github.com/kan-bayashi/ParallelWaveGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a4d7c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65661728",
   "metadata": {},
   "source": [
    "Set Python path so it can find the `speecht5` and `fairseq` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f30402f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../SpeechT5/SpeechT5\")\n",
    "sys.path.insert(0, \"../SpeechT5/SpeechT5/fairseq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d398e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60213c02",
   "metadata": {},
   "source": [
    "## Tokenize input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b5ef173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run the original tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bda5792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run our own tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "768371be",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokens = torch.tensor([[\n",
    "    4, 35,  5, 22, 23,  4, 20, 11,  7,  6, 31, 12,  4, 16, 24, 23,  4, 35,\n",
    "         16, 21, 21, 10,  9, 21,  4, 55,  7, 17,  5, 41,  4, 32, 11, 10, 12,  4,\n",
    "         10, 12,  4, 34, 24,  5,  5, 17, 11,  4,  6,  5,  5,  4, 19, 10, 27,  5,\n",
    "          4,  6,  7, 15, 28, 10,  9, 21,  4,  6,  8,  4, 22,  8, 16,  4, 19,  8,\n",
    "         13,  4, 13,  5,  7, 15, 26\n",
    "]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f3f9e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"input_ids\": src_tokens,\n",
    "#    \"attention_mask\": torch.ones_like(src_tokens),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43f10e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test with attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9bfaeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test batch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1389f72a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2205900b",
   "metadata": {},
   "source": [
    "## Load the Transformers model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc8c0f8",
   "metadata": {},
   "source": [
    "To convert the original checkpoint weights to Transformers:\n",
    "\n",
    "First download the checkpoint from https://huggingface.co/mechanicalsea/speecht5-tts\n",
    "\n",
    "Then run the following, using your own `--checkpoint_path` and `--pytorch_dump_folder_path`:\n",
    "\n",
    "```nohighlight\n",
    "cd transformers/src/transformers/models/speecht5\n",
    "\n",
    "python convert_speecht5_original_pytorch_checkpoint_to_pytorch.py \\\n",
    "  --task t2s \\\n",
    "  --checkpoint_path /path/to/SpeechT5/speecht5_tts.pt\n",
    "  --pytorch_dump_folder_path /some/other/path\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "552fd307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    SpeechT5Config, \n",
    "    SpeechT5CTCTokenizer,\n",
    "    SpeechT5Processor,\n",
    "    SpeechT5Model, \n",
    "    SpeechT5ForConditionalGeneration, \n",
    "    SpeechT5ForCTC, \n",
    "    SpeechT5ForTTS,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69aa6aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = SpeechT5Config()\n",
    "# hf_model = SpeechT5Model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ba8f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_tts.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "5df4bac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model = SpeechT5ForTTS.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b7dd7606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeechT5ForTTS(\n",
       "  (speecht5): SpeechT5Model(\n",
       "    (encoder): SpeechT5EncoderWithTextPrenet(\n",
       "      (prenet): SpeechT5TextEncoderPrenet(\n",
       "        (embed_tokens): Embedding(81, 768, padding_idx=1)\n",
       "        (encode_positions): SpeechT5ScaledPositionalEncoding(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (wrapped_encoder): SpeechT5Encoder(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (embed_positions): SpeechT5RelativePositionalEncoding(\n",
       "          (pe_k): Embedding(320, 64)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): SpeechT5DecoderWithSpeechPrenet(\n",
       "      (prenet): SpeechT5SpeechDecoderPrenet(\n",
       "        (layer1): Linear(in_features=80, out_features=256, bias=True)\n",
       "        (layer2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (layer3): Linear(in_features=256, out_features=768, bias=True)\n",
       "        (encode_positions): SpeechT5ScaledPositionalEncoding(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (speaker_embeds_layer): Linear(in_features=1280, out_features=768, bias=True)\n",
       "      )\n",
       "      (wrapped_decoder): SpeechT5Decoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (speech_decoder_postnet): SpeechT5SpeechDecoderPostnet()\n",
       ")"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09c1c4b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_tts.pt were not used when initializing SpeechT5Model: ['speecht5.encoder.prenet.encode_positions.alpha', 'speecht5.encoder.prenet.encode_positions.pe', 'speecht5.encoder.prenet.embed_tokens.weight']\n",
      "- This IS expected if you are initializing SpeechT5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SpeechT5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Loading should work OK for class `SpeechT5Model` too:\n",
    "hf_model_naked = SpeechT5Model.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64823660",
   "metadata": {},
   "source": [
    "## Load the original model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1046bda1",
   "metadata": {},
   "source": [
    "See the script `generate_speech.py` from the SpeechT5 repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1697ab0",
   "metadata": {},
   "source": [
    "Load the dictionary. This adds `<s>, <pad>, </s>, <unk>` tokens to the front and `<mask>` and `<ctc_blank>` to the end. **dict.txt** was [downloaded from here](https://drive.google.com/uc?export=download&id=19hcQ58RHZ6CssxF8Qp6yEF1NW_AXxObK). This is the Vocabulary link from the main SpeechT5 README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24e0f251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size: 81\n"
     ]
    }
   ],
   "source": [
    "from fairseq.data import Dictionary\n",
    "tgt_dict = Dictionary.load(\"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/DATA_ROOT/dict.txt\")\n",
    "tgt_dict.add_symbol(\"<mask>\")\n",
    "tgt_dict.add_symbol(\"<ctc_blank>\")\n",
    "print(f\"dictionary size: \" f\"{len(tgt_dict):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85935250",
   "metadata": {},
   "source": [
    "To load the model we need the `SpeechT5Task` object but constructing it is annoying. Fortunately, `build_model` only reads two properties from the task object, so we can fake it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea4b4380",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeTask:\n",
    "    def __init__(self):\n",
    "        self.dicts = { \"text\": tgt_dict }\n",
    "        self.t5_task = \"t2s\"\n",
    "        \n",
    "task = FakeTask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dc6c7a",
   "metadata": {},
   "source": [
    "Load the fine-tuned ASR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a79fbfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from speecht5.models.speecht5 import T5TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9b2818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"../weights/speecht5-tts/speecht5_tts.pt\")\n",
    "\n",
    "orig_model = T5TransformerModel.build_model(checkpoint[\"cfg\"][\"model\"], task)\n",
    "\n",
    "orig_model.load_state_dict(checkpoint[\"model\"])\n",
    "orig_model = orig_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bda7a201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speecht5.models.speecht5.T5TransformerModel"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(orig_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64d53b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speecht5.models.modules.encoder.TransformerEncoder"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(orig_model.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bad44136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(checkpoint[\"model\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c1faddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fairseq.data.encoders.sentencepiece_bpe.SentencepieceBPE"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fairseq.data import encoders\n",
    "from argparse import Namespace\n",
    "tokenizer = encoders.build_bpe(\n",
    "    Namespace(\n",
    "        bpe='sentencepiece', \n",
    "        sentencepiece_model='/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/MODEL_DIR/spm_char.model'\n",
    "    )\n",
    ")\n",
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd5619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig_model.decoder.layers[0].encoder_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56503f04",
   "metadata": {},
   "source": [
    "## Verify text encoder prenet output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e2f9ff",
   "metadata": {},
   "source": [
    "The `text_encoder_prenet` converts the input tokens into embeddings of shape `(batch, sequence_length, 768)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "5e6cce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    encoder_input, encoder_padding_mask = orig_model.text_encoder_prenet(src_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "26c4a65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 79, 768]), torch.Size([1, 79]))"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input.shape, encoder_padding_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "058de538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0383,  0.0826,  0.0607,  ..., -0.0333, -0.1770,  0.0203],\n",
       "         [ 0.0857,  0.0239,  0.2346,  ...,  0.1588,  0.0084,  0.0664],\n",
       "         [ 0.0644,  0.0361,  0.2011,  ..., -0.0519, -0.1996,  0.0747],\n",
       "         ...,\n",
       "         [ 0.1361,  0.1569, -0.1022,  ...,  0.1605,  0.0147,  0.0804],\n",
       "         [ 0.2444, -0.0554, -0.0303,  ...,  0.0666,  0.0143, -0.0211],\n",
       "         [ 0.0408, -0.0834,  0.1128,  ...,  0.1641, -0.0404,  0.0649]]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3c07884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c95ff6",
   "metadata": {},
   "source": [
    "Run Hugging Face model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "fef986c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.encoder.prenet(**inputs)\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "5ef358b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([79, 768])]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in hf_outputs if hasattr(x, \"shape\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "003cb256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0383,  0.0826,  0.0607,  ..., -0.0333, -0.1770,  0.0203],\n",
       "        [ 0.0857,  0.0239,  0.2346,  ...,  0.1588,  0.0084,  0.0664],\n",
       "        [ 0.0644,  0.0361,  0.2011,  ..., -0.0519, -0.1996,  0.0747],\n",
       "        ...,\n",
       "        [ 0.1361,  0.1569, -0.1022,  ...,  0.1605,  0.0147,  0.0804],\n",
       "        [ 0.2444, -0.0554, -0.0303,  ...,  0.0666,  0.0143, -0.0211],\n",
       "        [ 0.0408, -0.0834,  0.1128,  ...,  0.1641, -0.0404,  0.0649]])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "c25d959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_encoder_input = hf_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dbc6dd",
   "metadata": {},
   "source": [
    "If the weights and model were converted correctly, this should report zero or a very small number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "4b099ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(encoder_input - hf_outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "5533cb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9a6ba51a60>"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(torch.abs(encoder_input - hf_outputs[0]).numpy()[0] > 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b76dc23",
   "metadata": {},
   "source": [
    "## Verify Transformer encoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "47c86f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it with the original's speech prenet input:\n",
    "with torch.no_grad():\n",
    "    encoder_output = orig_model.encoder(encoder_input, encoder_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "8b8b4df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run it with our input, which is slightly different (see above)\n",
    "# with torch.no_grad():\n",
    "#     encoder_output = orig_model.encoder(hf_encoder_input, ~hf_encoder_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "41ff76cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_out shape torch.Size([79, 1, 768])\n",
      "encoder_padding_mask shape torch.Size([1, 79])\n",
      "encoder_states []\n",
      "src_tokens []\n",
      "decoder_input [None]\n",
      "encoder_out_for_ctc shape torch.Size([79, 1, 81])\n"
     ]
    }
   ],
   "source": [
    "print(\"encoder_out shape\", encoder_output[\"encoder_out\"][0].shape)\n",
    "print(\"encoder_padding_mask shape\", encoder_output[\"encoder_padding_mask\"][0].shape)\n",
    "print(\"encoder_states\", encoder_output[\"encoder_states\"])  # []\n",
    "print(\"src_tokens\", encoder_output[\"src_tokens\"])  # []\n",
    "print(\"decoder_input\", encoder_output[\"decoder_input\"])  # [None]\n",
    "print(\"encoder_out_for_ctc shape\", encoder_output[\"encoder_out_for_ctc\"][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "a38979d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1216, -0.1223, -0.0161,  ...,  0.1346,  0.0016, -0.0969],\n",
       "         [-0.4308,  0.2260,  0.2156,  ..., -0.1011,  0.0140, -0.4570],\n",
       "         [-0.5038,  0.2883,  0.1684,  ...,  0.0300, -0.0678, -0.4736],\n",
       "         ...,\n",
       "         [ 0.4783, -0.1476, -0.0570,  ..., -0.2642, -0.6181,  0.2321],\n",
       "         [ 0.4313, -0.2115, -0.1195,  ..., -0.4216, -0.4153,  0.2950],\n",
       "         [ 0.0910, -0.0379,  0.1903,  ...,  0.0070, -0.1593, -0.0444]]])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[\"encoder_out\"][0].permute((1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "f9b9dda4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0323,  0.0410,  0.0620,  ...,  0.0133,  0.0218, -0.0187],\n",
       "         [ 0.0523, -0.0873, -0.0304,  ..., -0.0184, -0.0473,  0.0077],\n",
       "         [ 0.0937, -0.0637, -0.0121,  ..., -0.0029, -0.0065, -0.0139],\n",
       "         ...,\n",
       "         [-0.1147,  0.0520,  0.0286,  ...,  0.0440,  0.0449,  0.1282],\n",
       "         [-0.1129,  0.0273, -0.0115,  ...,  0.0530,  0.0472,  0.1208],\n",
       "         [ 0.0075, -0.0139,  0.0873,  ...,  0.0442, -0.0180, -0.0587]]])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[\"encoder_out_for_ctc\"][0].permute((1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "cde6dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use exact same inputs as the original model:\n",
    "# with torch.no_grad():\n",
    "#      hf_outputs = hf_model.speecht5(\n",
    "#          inputs_embeds=encoder_input,\n",
    "#          attention_mask=(~encoder_padding_mask),\n",
    "#      )\n",
    "\n",
    "# type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "79eeca25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 79, 768])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_encoder_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "28f529a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutput"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.encoder.wrapped_encoder(\n",
    "         hidden_states=hf_encoder_input,\n",
    "         attention_mask=torch.ones_like(src_tokens),\n",
    "         return_dict=True,\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "fb12e71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[ 0.1216, -0.1223, -0.0161,  ...,  0.1346,  0.0016, -0.0969],\n",
       "         [-0.4308,  0.2260,  0.2156,  ..., -0.1011,  0.0140, -0.4570],\n",
       "         [-0.5038,  0.2883,  0.1684,  ...,  0.0300, -0.0678, -0.4736],\n",
       "         ...,\n",
       "         [ 0.4783, -0.1476, -0.0570,  ..., -0.2642, -0.6181,  0.2321],\n",
       "         [ 0.4313, -0.2115, -0.1195,  ..., -0.4216, -0.4153,  0.2950],\n",
       "         [ 0.0910, -0.0379,  0.1903,  ...,  0.0070, -0.1593, -0.0444]]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "06fb675b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_hidden_state']"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "8fe9914e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 79, 768])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "69eda6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1216, -0.1223, -0.0161,  ...,  0.1346,  0.0016, -0.0969],\n",
       "         [-0.4308,  0.2260,  0.2156,  ..., -0.1011,  0.0140, -0.4570],\n",
       "         [-0.5038,  0.2883,  0.1684,  ...,  0.0300, -0.0678, -0.4736],\n",
       "         ...,\n",
       "         [ 0.4783, -0.1476, -0.0570,  ..., -0.2642, -0.6181,  0.2321],\n",
       "         [ 0.4313, -0.2115, -0.1195,  ..., -0.4216, -0.4153,  0.2950],\n",
       "         [ 0.0910, -0.0379,  0.1903,  ...,  0.0070, -0.1593, -0.0444]]])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "a747e547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(encoder_output[\"encoder_out\"][0].permute((1, 0, 2)) - hf_outputs[\"last_hidden_state\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5288ddf",
   "metadata": {},
   "source": [
    "## Full encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "23a1ee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = src_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "4f1d6413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 79])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test batch\n",
    "inputs = torch.tile(src_tokens, (2, 1))\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "c7b0566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    encoder_out = orig_model.forward_text_encoder(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "4d29d4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1216, -0.1223, -0.0161,  ...,  0.1346,  0.0016, -0.0969],\n",
       "         [-0.4308,  0.2260,  0.2156,  ..., -0.1011,  0.0140, -0.4570],\n",
       "         [-0.5038,  0.2883,  0.1684,  ...,  0.0300, -0.0678, -0.4736],\n",
       "         ...,\n",
       "         [ 0.4783, -0.1476, -0.0570,  ..., -0.2642, -0.6181,  0.2321],\n",
       "         [ 0.4313, -0.2115, -0.1195,  ..., -0.4216, -0.4153,  0.2950],\n",
       "         [ 0.0910, -0.0379,  0.1903,  ...,  0.0070, -0.1593, -0.0444]]])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[\"encoder_out\"][0].permute((1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "65d6394b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 79, 768])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[\"encoder_out\"][0].permute((1, 0, 2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "bb53d9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: what happened to my batch here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "c1e9d096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutput"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.encoder(\n",
    "         input_values=src_tokens,\n",
    "         attention_mask=torch.ones_like(src_tokens),\n",
    "         return_dict=True,\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "f710165e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 79, 768])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "6856a736",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(encoder_output[\"encoder_out\"][0].permute((1, 0, 2)) - hf_outputs[\"last_hidden_state\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35f8df5",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Speaker embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b91b4e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#         if spkembs is not None and self.spk_embed_integration_type != \"pre\":\n",
    "#             encoder_out[\"encoder_out\"] = [self._integrate_with_spk_embed(\n",
    "#                 encoder_out[\"encoder_out\"][0].transpose(0, 1), spkembs\n",
    "#             ).transpose(0, 1)]\n",
    "#             spkembs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16506443",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a45c546",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cbf488",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#         threshold = kwargs.get(\"threshold\", 0.5)\n",
    "#         minlenratio = kwargs.get(\"threshold\", 0.0)\n",
    "\n",
    "#         if source is None:\n",
    "#             assert src_tokens.size(0) == 1\n",
    "#             encoder_out = self.forward_text_encoder(src_tokens)\n",
    "#             maxlenratio = kwargs.get(\"threshold\", 20.0)\n",
    "#         else:\n",
    "#             assert source.size(0) == 1\n",
    "#             encoder_out = self.forward_encoder(source, padding_mask=kwargs[\"padding_mask\"])\n",
    "#             maxlenratio = kwargs.get(\"threshold\", 10.0)\n",
    "\n",
    "#         if spkembs is not None and self.spk_embed_integration_type != \"pre\":\n",
    "#             encoder_out[\"encoder_out\"] = [self._integrate_with_spk_embed(\n",
    "#                 encoder_out[\"encoder_out\"][0].transpose(0, 1), spkembs\n",
    "#             ).transpose(0, 1)]\n",
    "#             spkembs = None\n",
    "\n",
    "#         maxlen = int(encoder_out[\"encoder_out\"][0].size(0) * maxlenratio / self.reduction_factor)\n",
    "#         minlen = int(encoder_out[\"encoder_out\"][0].size(0) * minlenratio / self.reduction_factor)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bac9e919",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "spkembs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17bb840",
   "metadata": {},
   "source": [
    "## Verify speech decoder prenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "c38ad5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#idx = 0\n",
    "#outs, probs = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "f7153188",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 80])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys = encoder_out[\"encoder_out\"][0].new_zeros(1, 1, orig_model.speech_decoder_postnet.odim)\n",
    "ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "1fb08819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: just for testing\n",
    "# ys = torch.randn(7, 3, 80)\n",
    "# spkembs = torch.randn(7, 512)\n",
    "\n",
    "ys = torch.randn(1, 1, 80)\n",
    "spkembs = torch.randn(1, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "b2fe6ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    decoder_in, _ = orig_model.speech_decoder_prenet(ys, spkembs=spkembs)\n",
    "    decoder_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "92cc8add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000e+00, 1.7975e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 7.4122e-02, 0.0000e+00, 0.0000e+00,\n",
       "          1.1482e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.4703e-01,\n",
       "          0.0000e+00, 9.3797e-02, 0.0000e+00, 6.3784e-02, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          3.8182e-01, 2.1554e-01, 0.0000e+00, 5.3059e-02, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 1.1463e-01, 3.5560e-04, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 5.0874e-01, 0.0000e+00, 0.0000e+00,\n",
       "          2.5047e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0639e-01,\n",
       "          6.5273e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.0920e-01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 5.3465e-02, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 2.7185e-03, 1.5815e-01, 0.0000e+00,\n",
       "          5.2077e-02, 0.0000e+00, 0.0000e+00, 1.1143e-01, 3.8944e-02,\n",
       "          0.0000e+00, 9.6145e-02, 9.3250e-02, 4.4204e-01, 1.1393e-02,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1384e+00, 0.0000e+00,\n",
       "          0.0000e+00, 4.9253e-02, 0.0000e+00, 8.1604e-02, 1.8291e-01,\n",
       "          7.4722e-03, 0.0000e+00, 0.0000e+00, 6.3315e-02, 2.2268e-02,\n",
       "          0.0000e+00, 0.0000e+00, 7.9960e-01, 3.0705e-02, 9.4842e-02,\n",
       "          2.1094e-01, 1.8503e-01, 9.3882e-02, 0.0000e+00, 0.0000e+00,\n",
       "          2.6141e-02, 3.4008e-02, 6.5128e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 3.7294e-02, 1.8159e-01, 0.0000e+00,\n",
       "          6.7977e-01, 0.0000e+00, 0.0000e+00, 2.3861e-01, 0.0000e+00,\n",
       "          0.0000e+00, 1.0266e-01, 1.0299e-01, 2.8547e-01, 9.0559e-02,\n",
       "          0.0000e+00, 1.7904e-01, 0.0000e+00, 0.0000e+00, 4.0568e-01,\n",
       "          2.9458e-02, 0.0000e+00, 0.0000e+00, 4.0523e-02, 0.0000e+00,\n",
       "          0.0000e+00, 5.2730e-01, 1.6431e-01, 2.2185e-01, 1.9909e-02,\n",
       "          0.0000e+00, 4.3047e-01, 1.5327e-01, 0.0000e+00, 4.5677e-03,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 6.4155e-02, 4.8400e-02,\n",
       "          0.0000e+00, 0.0000e+00, 1.3735e-01, 0.0000e+00, 2.3006e-01,\n",
       "          0.0000e+00, 1.0369e-01, 2.5638e-01, 6.8451e-02, 0.0000e+00,\n",
       "          3.1199e-02, 0.0000e+00, 3.1417e-01, 0.0000e+00, 1.6541e-01,\n",
       "          5.1964e-01, 4.5333e-01, 3.2347e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 9.0176e-02, 3.2579e-01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          1.3206e-01, 3.2417e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 5.7089e-01, 1.0331e-01, 3.5906e-03, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 1.6355e-01, 0.0000e+00, 0.0000e+00, 1.0744e-01,\n",
       "          2.0468e-01, 5.7429e-01, 2.5638e-02, 0.0000e+00, 2.0402e-01,\n",
       "          4.0188e-01, 0.0000e+00, 2.7203e-01, 0.0000e+00, 0.0000e+00,\n",
       "          2.6649e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.5688e-03,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.2983e-01,\n",
       "          0.0000e+00, 1.6263e-03, 0.0000e+00, 1.7660e+00, 0.0000e+00,\n",
       "          0.0000e+00, 5.6358e-02, 3.6454e-01, 1.9625e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 2.5521e-02, 0.0000e+00, 6.4330e-02, 4.3158e-02,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 6.3296e-02, 9.4919e-01,\n",
       "          0.0000e+00, 1.2273e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          1.3192e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          3.5394e-01, 0.0000e+00, 0.0000e+00, 2.4007e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 4.5934e-02, 3.5549e-01,\n",
       "          2.8243e-01, 2.3980e-01, 4.4139e-02, 1.6934e-01, 1.4239e-01,\n",
       "          0.0000e+00, 4.4819e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          8.2865e-02, 0.0000e+00, 2.9959e-01, 0.0000e+00, 0.0000e+00,\n",
       "          3.4069e-02, 2.3302e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          1.5824e-01, 1.2965e+00, 0.0000e+00, 0.0000e+00, 1.9931e-01,\n",
       "          0.0000e+00, 0.0000e+00, 2.2008e-01, 1.1169e-01, 3.2628e-01,\n",
       "          6.6817e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          9.9854e-03, 3.3815e-01, 2.0262e-01, 0.0000e+00, 1.2701e+00,\n",
       "          0.0000e+00, 2.4959e-01, 0.0000e+00, 3.8275e-02, 0.0000e+00,\n",
       "          0.0000e+00, 3.0770e-02, 0.0000e+00, 1.1792e-01, 0.0000e+00,\n",
       "          1.0012e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6228e-01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4452e-01, 3.3744e-01,\n",
       "          1.6626e-01, 6.1493e-03, 0.0000e+00, 7.7551e-02, 0.0000e+00,\n",
       "          2.1119e-01, 4.3668e-01, 0.0000e+00, 0.0000e+00, 7.8611e-03,\n",
       "          3.1339e-01, 2.1635e-01, 7.0589e-01, 0.0000e+00, 1.7207e-01,\n",
       "          1.6965e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 1.4860e-01, 0.0000e+00, 0.0000e+00,\n",
       "          2.6766e-02, 1.5952e-01, 4.3239e-01, 0.0000e+00, 7.8864e-02,\n",
       "          0.0000e+00, 6.2602e-01, 0.0000e+00, 2.5769e-01, 5.1633e-02,\n",
       "          3.0697e-03, 0.0000e+00, 4.2874e-02, 1.8877e-02, 0.0000e+00,\n",
       "          6.9102e-02, 0.0000e+00, 0.0000e+00, 1.0309e-01, 0.0000e+00,\n",
       "          1.6816e-01, 0.0000e+00, 5.3028e-02, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 1.1318e-01, 0.0000e+00, 8.1529e-01,\n",
       "          0.0000e+00, 2.9628e-01, 9.7020e-02, 3.9156e-01, 0.0000e+00,\n",
       "          0.0000e+00, 9.0686e-02, 0.0000e+00, 1.1769e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.3597e-02,\n",
       "          7.1843e-02, 7.5074e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 3.0654e-01, 4.9163e-02, 1.1653e-01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9293e-01, 3.0886e-01,\n",
       "          2.1195e-01, 0.0000e+00, 0.0000e+00, 3.4224e-01, 9.6731e-02,\n",
       "          0.0000e+00, 1.6310e-01, 0.0000e+00, 0.0000e+00, 1.9962e-01,\n",
       "          2.7216e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.7836e-02,\n",
       "          1.2014e-01, 3.8296e-02, 2.4984e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1524e-01, 0.0000e+00,\n",
       "          9.4829e-02, 0.0000e+00, 0.0000e+00, 2.1255e-01, 0.0000e+00,\n",
       "          0.0000e+00, 2.8127e-01, 0.0000e+00, 3.6208e-01, 6.6282e-02,\n",
       "          0.0000e+00, 2.4042e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          3.7651e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.4726e-03,\n",
       "          1.7859e-01, 2.1535e-01, 5.4034e-02, 0.0000e+00, 2.2938e-02,\n",
       "          0.0000e+00, 1.5104e-01, 1.7476e-01, 0.0000e+00, 1.3459e-01,\n",
       "          8.0246e-03, 0.0000e+00, 0.0000e+00, 5.6821e-02, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.0403e-02,\n",
       "          3.6154e+00, 0.0000e+00, 9.0111e-02, 0.0000e+00, 0.0000e+00,\n",
       "          1.4218e-01, 1.9937e-02, 3.4414e-01, 1.7816e-01, 0.0000e+00,\n",
       "          2.8412e-01, 3.8429e-01, 0.0000e+00, 8.3302e-02, 0.0000e+00,\n",
       "          6.3207e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          2.0546e-01, 0.0000e+00, 1.8305e-01, 0.0000e+00, 8.6033e-01,\n",
       "          8.3322e-03, 1.3390e-01, 2.9294e-01, 8.4851e-02, 1.3683e-01,\n",
       "          0.0000e+00, 8.0625e-01, 1.2125e-01, 0.0000e+00, 1.8276e-01,\n",
       "          5.4692e-01, 0.0000e+00, 2.0579e-01, 0.0000e+00, 6.2198e-01,\n",
       "          0.0000e+00, 0.0000e+00, 9.8102e-02, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 2.9202e-01, 4.2258e-02, 0.0000e+00, 0.0000e+00,\n",
       "          3.0283e-01, 0.0000e+00, 3.5893e-02, 1.8429e-01, 1.1312e-01,\n",
       "          0.0000e+00, 0.0000e+00, 3.1852e-02, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 1.4450e-01, 9.6117e-02, 0.0000e+00, 0.0000e+00,\n",
       "          3.0153e-01, 3.0070e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 1.2823e-01, 2.9247e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 5.7786e-01, 2.3550e-01, 0.0000e+00, 4.5260e-01,\n",
       "          6.7457e-02, 1.7295e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 1.7057e-01, 0.0000e+00, 1.1447e-01,\n",
       "          2.9199e-01, 6.1754e-02, 0.0000e+00, 1.0719e+00, 1.0021e-01,\n",
       "          1.1388e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          2.9698e-02, 4.9517e-01, 6.5369e-02, 0.0000e+00, 0.0000e+00,\n",
       "          5.9837e-02, 0.0000e+00, 2.3112e-01, 1.5998e-02, 0.0000e+00,\n",
       "          8.5030e-01, 0.0000e+00, 2.8823e-01, 0.0000e+00, 1.1316e-01,\n",
       "          0.0000e+00, 2.9959e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1542e-01, 0.0000e+00,\n",
       "          0.0000e+00, 5.1407e-01, 4.9949e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 1.5982e-01, 6.9599e-02, 0.0000e+00, 1.7431e-01,\n",
       "          0.0000e+00, 0.0000e+00, 1.7963e-01, 0.0000e+00, 0.0000e+00,\n",
       "          2.1036e-01, 3.3349e-01, 3.9981e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 3.7285e-01, 0.0000e+00,\n",
       "          1.2651e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          1.2365e-01, 1.1468e-01, 0.0000e+00, 2.4592e-01, 0.0000e+00,\n",
       "          1.4001e-01, 0.0000e+00, 3.6370e-02, 0.0000e+00, 0.0000e+00,\n",
       "          8.8020e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 2.1029e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 1.1403e+00, 2.2335e-01, 3.2571e-01, 5.6850e-02,\n",
       "          4.1145e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 2.2016e-01, 0.0000e+00, 1.0958e+00,\n",
       "          0.0000e+00, 0.0000e+00, 5.2481e-01, 4.0561e-01, 0.0000e+00,\n",
       "          7.7737e-02, 0.0000e+00, 1.0356e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 8.0046e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 3.9923e-01, 0.0000e+00, 2.8024e-01,\n",
       "          0.0000e+00, 2.7186e-01, 0.0000e+00, 3.0490e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 2.9669e-02, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0007e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 4.9041e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 6.8368e-02, 0.0000e+00,\n",
       "          0.0000e+00, 3.0009e-01, 0.0000e+00, 1.8064e-01, 1.7598e-01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          2.6604e-03, 4.9866e-01, 0.0000e+00, 0.0000e+00, 2.0953e-01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          6.6886e-02, 0.0000e+00, 0.0000e+00, 8.3564e-02, 1.1810e-01,\n",
       "          2.2510e-01, 0.0000e+00, 0.0000e+00, 1.5721e-02, 3.6067e-02,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.1535e-01,\n",
       "          0.0000e+00, 6.0603e-01, 0.0000e+00, 2.7730e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 1.6084e-01, 0.0000e+00, 1.3138e-01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00]]])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5612ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "77134734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.decoder.prenet(\n",
    "         input_values=ys,\n",
    "         speaker_embeddings=spkembs,\n",
    "     )\n",
    "\n",
    "hf_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "dc63edfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(decoder_in - hf_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8d7a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fec354d",
   "metadata": {},
   "source": [
    "## Verify Transformer decoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "6a0d800f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_output_tokens = decoder_in[:, -1:]\n",
    "prev_output_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "4f068be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "910d8ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental_state = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "6229b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    decoder_output, extra = orig_model.decoder(\n",
    "        prev_output_tokens,\n",
    "        tgt_mask,\n",
    "        encoder_out=encoder_out,\n",
    "        incremental_state=incremental_state,\n",
    "        alignment_layer=-1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "0a190958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "fea0351e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05117956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[x.shape for x in extra[\"attn\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b593692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[x.shape for x in extra[\"inner_states\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "9d486f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.decoder.wrapped_decoder(\n",
    "         hidden_states=prev_output_tokens,\n",
    "         #attention_mask=decoder_attention_mask,\n",
    "         encoder_hidden_states=encoder_out[\"encoder_out\"][0].permute((1, 0, 2)),\n",
    "         #encoder_attention_mask=hf_encoder_attention_mask.repeat((1, beam_size)).view(beam_size * batch_size, -1),\n",
    "         return_dict=True,\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "ea512cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_hidden_state', 'past_key_values']"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "d82c369a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "eb6282bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_outputs[\"last_hidden_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "f3e254cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(decoder_output - hf_outputs[\"last_hidden_state\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d3d1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1126f36a",
   "metadata": {},
   "source": [
    "## Full decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "7d9bc375",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    encoder_out = orig_model.forward_text_encoder(src_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "633f383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = torch.randn(1, 3, 80)\n",
    "spkembs = torch.randn(1, 512)\n",
    "incremental_states = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "8f0f7d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_in, _ = orig_model.speech_decoder_prenet(ys, spkembs=spkembs)\n",
    "z, extra = orig_model.decoder(\n",
    "    decoder_in[:,-1:], None, encoder_out, incremental_states, alignment_layer=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "b75d5f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.decoder(\n",
    "         input_values=ys,\n",
    "         #attention_mask=decoder_attention_mask,\n",
    "         encoder_hidden_states=encoder_out[\"encoder_out\"][0].permute((1, 0, 2)),\n",
    "         #encoder_attention_mask=hf_encoder_attention_mask.repeat((1, beam_size)).view(beam_size * batch_size, -1),\n",
    "         speaker_embeddings=spkembs,\n",
    "         return_dict=True,         \n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "0c7ec875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(z - hf_outputs[\"last_hidden_state\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4da874c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d9113f7",
   "metadata": {},
   "source": [
    "## Verify speech decoder postnet output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46465deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#             outs += [self.speech_decoder_postnet.feat_out(z[0, -1]).view(self.reduction_factor, self.speech_decoder_postnet.odim)]  # [(r, odim), ...]\n",
    "#             probs += [torch.sigmoid(self.speech_decoder_postnet.prob_out(z[0, -1]))]  # [(r), ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20be43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    lprobs = orig_model.text_decoder_postnet(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a143bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lprobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6de14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lprobs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7571e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    lprobs_hf = hf_model.text_decoder_postnet(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5623a791",
   "metadata": {},
   "outputs": [],
   "source": [
    "lprobs_hf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f0c976",
   "metadata": {},
   "outputs": [],
   "source": [
    "lprobs_hf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeff463",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(torch.abs(lprobs - lprobs_hf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24926eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32e98799",
   "metadata": {},
   "source": [
    "## Use the `transformers` generator loop:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c4e9dc",
   "metadata": {},
   "source": [
    "Run the full model to make sure this doesn't give any errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b889ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model(\n",
    "         input_values=inputs.input_values,\n",
    "         attention_mask=inputs.attention_mask,\n",
    "         decoder_input_ids=torch.tensor([[3, 4, 5]]),\n",
    "         #decoder_input_ids=torch.tensor([[3, 4, 5], [2, 2, 2]]),  # batch\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677718c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2bcec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d1eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "baf0244b",
   "metadata": {},
   "source": [
    "Also calculate loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7bd1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model(\n",
    "         input_values=inputs.input_values,\n",
    "         attention_mask=inputs.attention_mask,\n",
    "         #decoder_input_ids=torch.tensor([[2,  4, 18, 10, 12,  6,  5]]),\n",
    "         labels=torch.tensor([[4, 18, 10, 12,  6,  5, 13]]),\n",
    "         #labels=torch.tensor([[4, 18, 10, 12,  6,  5, 13], [4, 18, 10, 12,  6,  5, 13]]),  # batch\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34dd766",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edf2044",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195961eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f2ce14a",
   "metadata": {},
   "source": [
    "Generator loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ccdbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.input_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fc2baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs = hf_model.generate(inputs.input_values, max_length=100)\n",
    "# hf_outputs = hf_model.generate(inputs.input_values, num_beams=5, max_length=100) #, bos_token_id=2)\n",
    "# hf_outputs = hf_model.generate(torch.rand(1, 10000), num_beams=5, max_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015c80b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7e1899",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fce9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(hf_outputs.shape[0]):\n",
    "    print(tgt_dict.string(hf_outputs[i]))\n",
    "    print(tokenizer.decode(tgt_dict.string(hf_outputs[i])))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997fd48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[tgt_dict[x] for x in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d72119a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a116f698",
   "metadata": {},
   "source": [
    "For comparison, Speech2Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00306c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
    "\n",
    "s2t_model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "s2t_processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f994828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "inputs = s2t_processor(\n",
    "    ds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], return_tensors=\"pt\"\n",
    ")\n",
    "input_features = inputs.input_features\n",
    "input_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8ef5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test batch\n",
    "# input_features = torch.tile(input_features, dims=(2, 1, 1))\n",
    "# input_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf06a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = s2t_model.generate(inputs=input_features)\n",
    "\n",
    "transcription = s2t_processor.batch_decode(generated_ids)[0]\n",
    "transcription\n",
    "\n",
    "#'mister quilter is the apostle of the middle classes and we are glad to welcome his gospel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a21206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Speech2TextModel\n",
    "\n",
    "s2t_model = Speech2TextModel.from_pretrained(\"facebook/s2t-small-librispeech-asr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49667e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d50884f2",
   "metadata": {},
   "source": [
    "Test other methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd38a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BART, Speech2Text, Wav2Vec2 don't have pruning\n",
    "#hf_model.prune_heads({1: [0, 2], 2: [2,3 ]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc11e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9778431c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be3e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.get_output_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f954bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.resize_token_embeddings(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b17c7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742e8369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0974c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b945156",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_ctc.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c092a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_naked.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2280ef5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

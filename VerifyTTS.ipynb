{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca8cd0a9",
   "metadata": {},
   "source": [
    "# Verifying the SpeechT5 TTS model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5342da",
   "metadata": {},
   "source": [
    "I needed to do the following to be able to load the original model:\n",
    "\n",
    "- Clone the https://github.com/microsoft/SpeechT5 repo\n",
    "\n",
    "Install stuff:\n",
    "\n",
    "```\n",
    "pip install editdistance\n",
    "pip install -U sacrebleu==1.5.1\n",
    "\n",
    "git submodule update --init SpeechT5/fairseq\n",
    "cd SpeechT5\n",
    "pip install --editable fairseq/\n",
    "pip install espnet\n",
    "```\n",
    "\n",
    "Hack the code:\n",
    "\n",
    "- Copy `speecht5/tasks/speecht5.py` into `fairseq/fairseq/tasks`\n",
    "\n",
    "- `checkpoint_utils.py`: comment out a bunch of the loading stuff because of missing keys in the checkpoint\n",
    "\n",
    "Additional stuff to download:\n",
    "\n",
    "- `dict.txt` from https://drive.google.com/uc?export=download&id=19hcQ58RHZ6CssxF8Qp6yEF1NW_AXxObK\n",
    "\n",
    "- `tokenizer` from https://drive.google.com/uc?export=download&id=1wClgQjXXoU2lmpbaEa1v2SqMbg7cAutq\n",
    "\n",
    "- `xvectors.zip` from https://drive.google.com/uc?export=download&id=16QOUURZBrW7-GYbVG_gXt3mTMlZmQoH0\n",
    "\n",
    "- everything from https://huggingface.co/mechanicalsea/speecht5-tts\n",
    "\n",
    "- https://github.com/kan-bayashi/ParallelWaveGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a4d7c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65661728",
   "metadata": {},
   "source": [
    "Set Python path so it can find the `speecht5` and `fairseq` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f30402f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../SpeechT5/SpeechT5\")\n",
    "sys.path.insert(0, \"../SpeechT5/SpeechT5/fairseq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d398e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60213c02",
   "metadata": {},
   "source": [
    "## Tokenize input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55c775d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run the original tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb5a1140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run our own tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "768371be",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokens = torch.tensor([[\n",
    "    4, 35,  5, 22, 23,  4, 20, 11,  7,  6, 31, 12,  4, 16, 24, 23,  4, 35,\n",
    "         16, 21, 21, 10,  9, 21,  4, 55,  7, 17,  5, 41,  4, 32, 11, 10, 12,  4,\n",
    "         10, 12,  4, 34, 24,  5,  5, 17, 11,  4,  6,  5,  5,  4, 19, 10, 27,  5,\n",
    "          4,  6,  7, 15, 28, 10,  9, 21,  4,  6,  8,  4, 22,  8, 16,  4, 19,  8,\n",
    "         13,  4, 13,  5,  7, 15, 26\n",
    "]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f3f9e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"input_ids\": src_tokens,\n",
    "#    \"attention_mask\": torch.ones_like(src_tokens),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4472260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test with attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9bfaeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test batch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1389f72a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2205900b",
   "metadata": {},
   "source": [
    "## Load the Transformers model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc8c0f8",
   "metadata": {},
   "source": [
    "To convert the original checkpoint weights to Transformers:\n",
    "\n",
    "First download the checkpoint from https://huggingface.co/mechanicalsea/speecht5-tts\n",
    "\n",
    "Then run the following, using your own `--checkpoint_path` and `--pytorch_dump_folder_path`:\n",
    "\n",
    "```nohighlight\n",
    "cd transformers/src/transformers/models/speecht5\n",
    "\n",
    "python convert_speecht5_original_pytorch_checkpoint_to_pytorch.py \\\n",
    "  --task t2s \\\n",
    "  --checkpoint_path /path/to/SpeechT5/speecht5_tts.pt\n",
    "  --pytorch_dump_folder_path /some/other/path\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "552fd307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    SpeechT5Config, \n",
    "    SpeechT5CTCTokenizer,\n",
    "    SpeechT5Processor,\n",
    "    SpeechT5Model, \n",
    "    SpeechT5ForConditionalGeneration, \n",
    "    SpeechT5ForCTC, \n",
    "    SpeechT5ForTTS,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69aa6aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = SpeechT5Config()\n",
    "# hf_model = SpeechT5Model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ba8f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_tts.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5df4bac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model = SpeechT5ForTTS.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7dd7606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeechT5ForTTS(\n",
       "  (speecht5): SpeechT5Model(\n",
       "    (encoder): SpeechT5EncoderWithTextPrenet(\n",
       "      (prenet): SpeechT5TextEncoderPrenet(\n",
       "        (embed_tokens): Embedding(81, 768, padding_idx=1)\n",
       "        (encode_positions): SpeechT5ScaledPositionalEncoding(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (wrapped_encoder): SpeechT5Encoder(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (embed_positions): SpeechT5RelativePositionalEncoding(\n",
       "          (pe_k): Embedding(320, 64)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): SpeechT5DecoderWithSpeechPrenet(\n",
       "      (prenet): SpeechT5SpeechDecoderPrenet()\n",
       "      (wrapped_decoder): SpeechT5Decoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (speech_decoder_postnet): SpeechT5SpeechDecoderPostnet()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09c1c4b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_tts.pt were not used when initializing SpeechT5Model: ['speecht5.encoder.prenet.encode_positions.alpha', 'speecht5.encoder.prenet.encode_positions.pe', 'speecht5.encoder.prenet.embed_tokens.weight']\n",
      "- This IS expected if you are initializing SpeechT5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SpeechT5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Loading should work OK for class `SpeechT5Model` too:\n",
    "hf_model_naked = SpeechT5Model.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64823660",
   "metadata": {},
   "source": [
    "## Load the original model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1046bda1",
   "metadata": {},
   "source": [
    "See the script `generate_speech.py` from the SpeechT5 repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1697ab0",
   "metadata": {},
   "source": [
    "Load the dictionary. This adds `<s>, <pad>, </s>, <unk>` tokens to the front and `<mask>` and `<ctc_blank>` to the end. **dict.txt** was [downloaded from here](https://drive.google.com/uc?export=download&id=19hcQ58RHZ6CssxF8Qp6yEF1NW_AXxObK). This is the Vocabulary link from the main SpeechT5 README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24e0f251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size: 81\n"
     ]
    }
   ],
   "source": [
    "from fairseq.data import Dictionary\n",
    "tgt_dict = Dictionary.load(\"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/DATA_ROOT/dict.txt\")\n",
    "tgt_dict.add_symbol(\"<mask>\")\n",
    "tgt_dict.add_symbol(\"<ctc_blank>\")\n",
    "print(f\"dictionary size: \" f\"{len(tgt_dict):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85935250",
   "metadata": {},
   "source": [
    "To load the model we need the `SpeechT5Task` object but constructing it is annoying. Fortunately, `build_model` only reads two properties from the task object, so we can fake it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea4b4380",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeTask:\n",
    "    def __init__(self):\n",
    "        self.dicts = { \"text\": tgt_dict }\n",
    "        self.t5_task = \"t2s\"\n",
    "        \n",
    "task = FakeTask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dc6c7a",
   "metadata": {},
   "source": [
    "Load the fine-tuned ASR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a79fbfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from speecht5.models.speecht5 import T5TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9b2818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"../weights/speecht5-tts/speecht5_tts.pt\")\n",
    "\n",
    "orig_model = T5TransformerModel.build_model(checkpoint[\"cfg\"][\"model\"], task)\n",
    "\n",
    "orig_model.load_state_dict(checkpoint[\"model\"])\n",
    "orig_model = orig_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bda7a201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speecht5.models.speecht5.T5TransformerModel"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(orig_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64d53b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speecht5.models.modules.encoder.TransformerEncoder"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(orig_model.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bad44136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(checkpoint[\"model\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c1faddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fairseq.data.encoders.sentencepiece_bpe.SentencepieceBPE"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fairseq.data import encoders\n",
    "from argparse import Namespace\n",
    "tokenizer = encoders.build_bpe(\n",
    "    Namespace(\n",
    "        bpe='sentencepiece', \n",
    "        sentencepiece_model='/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/MODEL_DIR/spm_char.model'\n",
    "    )\n",
    ")\n",
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd5619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig_model.decoder.layers[0].encoder_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56503f04",
   "metadata": {},
   "source": [
    "## Verify text encoder prenet output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e2f9ff",
   "metadata": {},
   "source": [
    "The `text_encoder_prenet` converts the input tokens into embeddings of shape `(batch, sequence_length, 768)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e6cce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    encoder_input, encoder_padding_mask = orig_model.text_encoder_prenet(src_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26c4a65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 79, 768]), torch.Size([1, 79]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input.shape, encoder_padding_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "058de538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0383,  0.0826,  0.0607,  ..., -0.0333, -0.1770,  0.0203],\n",
       "         [ 0.0857,  0.0239,  0.2346,  ...,  0.1588,  0.0084,  0.0664],\n",
       "         [ 0.0644,  0.0361,  0.2011,  ..., -0.0519, -0.1996,  0.0747],\n",
       "         ...,\n",
       "         [ 0.1361,  0.1569, -0.1022,  ...,  0.1605,  0.0147,  0.0804],\n",
       "         [ 0.2444, -0.0554, -0.0303,  ...,  0.0666,  0.0143, -0.0211],\n",
       "         [ 0.0408, -0.0834,  0.1128,  ...,  0.1641, -0.0404,  0.0649]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c07884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0283cd36",
   "metadata": {},
   "source": [
    "Run Hugging Face model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69fdd0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.encoder.prenet(**inputs)\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ecdde1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([79, 768])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in hf_outputs if hasattr(x, \"shape\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1008b25c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0383,  0.0826,  0.0607,  ..., -0.0333, -0.1770,  0.0203],\n",
       "        [ 0.0857,  0.0239,  0.2346,  ...,  0.1588,  0.0084,  0.0664],\n",
       "        [ 0.0644,  0.0361,  0.2011,  ..., -0.0519, -0.1996,  0.0747],\n",
       "        ...,\n",
       "        [ 0.1361,  0.1569, -0.1022,  ...,  0.1605,  0.0147,  0.0804],\n",
       "        [ 0.2444, -0.0554, -0.0303,  ...,  0.0666,  0.0143, -0.0211],\n",
       "        [ 0.0408, -0.0834,  0.1128,  ...,  0.1641, -0.0404,  0.0649]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88415dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_encoder_input = hf_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dbc6dd",
   "metadata": {},
   "source": [
    "If the weights and model were converted correctly, this should report zero or a very small number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b099ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(encoder_input - hf_outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5533cb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9b6ffc79a0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAABjCAYAAAA7ImJCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAPsElEQVR4nO3de1BUdRsH8O+uy644uLsqlwXloqkhKmSg66ZO7+RORI5ZOY3D0AxeqtHQNKxRahL7o3Cmme5GUxk2Y0Xa5K0UI1SKBjARUrQQlcRUwMvALqbc9nn/8OW874raS8JZ2P1+Zs4MnN+P3ef5dTx9Z/ecXY2ICIiIiIhUovV0AURERORbGD6IiIhIVQwfREREpCqGDyIiIlIVwwcRERGpiuGDiIiIVMXwQURERKpi+CAiIiJVMXwQERGRqhg+iIiISFW9Fj7Wr1+PqKgoDBw4EFarFQcOHOitpyIiIqJ+pFfCx1dffYX09HRkZmbi0KFDiIuLQ2JiIhoaGnrj6YiIiKgf0fTGF8tZrVZMnjwZ77//PgDA5XIhPDwcy5Ytw+rVq2/7ty6XC+fOncPgwYOh0Wh6ujQiIiLqBSICp9OJsLAwaLW3f21D19NP3trairKyMmRkZCj7tFot7HY7iouLu8xvaWlBS0uL8vvZs2cRExPT02URERGRCs6cOYMRI0bcdk6Ph4+LFy+io6MDISEhbvtDQkLw+++/d5mflZWFV199tcv+6XgYOvj1dHlERETUC9rRhiLswuDBg/92bo+Hj+7KyMhAenq68rvD4UB4eDh08INOw/BBRETUL/znIo7/55KJHg8fgYGBGDBgAOrr693219fXw2KxdJlvMBhgMBh6ugwiIiLqo3r8bhe9Xo/4+HgUFBQo+1wuFwoKCmCz2Xr66YiIiKif6ZW3XdLT05GamoqEhARMmTIFb7/9Nq5cuYIFCxb0xtMRERFRP9Ir4WPevHm4cOEC1qxZg7q6Otxzzz3Iy8vrchEqERER+Z5e+ZyPO+FwOGAymfAvzOEFp0RERP1Eu7RhP7ajqakJRqPxtnP53S5ERESkKoYPIiIiUhXDBxEREamK4YOIiIhUxfBBREREqmL4ICIiIlUxfBAREZGqGD6IiIhIVQwfREREpCqGDyIiIlIVwwcRERGpiuGDiIiIVMXwQURERKpi+CAiIiJVMXwQERGRqhg+iIiISFUMH0RERKQqhg8iIiJSFcMHERERqYrhg4iIiFTF8EFERESq6lb4WLt2LTQajdsWHR2tjF+7dg1paWkYNmwYAgICMHfuXNTX1/d40URERNR/dfuVj/Hjx+P8+fPKVlRUpIw9//zz2LlzJ7Zs2YLCwkKcO3cOjz/+eI8WTERERP2brtt/oNPBYrF02d/U1IQNGzbgiy++wAMPPAAAyMnJwbhx41BSUoKpU6feebVERETU73X7lY/q6mqEhYVh1KhRSElJQW1tLQCgrKwMbW1tsNvtytzo6GhERESguLj4lo/X0tICh8PhthEREZH36lb4sFqt2LhxI/Ly8pCdnY2amhrMmDEDTqcTdXV10Ov1MJvNbn8TEhKCurq6Wz5mVlYWTCaTsoWHh/+jRoiIiKh/6NbbLklJScrPsbGxsFqtiIyMxObNm+Hv7/+PCsjIyEB6erryu8PhYAAhIiLyYnd0q63ZbMbYsWNx4sQJWCwWtLa2orGx0W1OfX39Ta8R6WQwGGA0Gt02IiIi8l53FD6am5tx8uRJhIaGIj4+Hn5+figoKFDGq6qqUFtbC5vNdseFEhERkXfo1tsuL7zwAmbPno3IyEicO3cOmZmZGDBgAJKTk2EymbBo0SKkp6dj6NChMBqNWLZsGWw2G+90ISIiIkW3wseff/6J5ORkXLp0CUFBQZg+fTpKSkoQFBQEAHjrrbeg1Woxd+5ctLS0IDExER988EGvFE5ERET9k0ZExNNF/C+HwwGTyYR/YQ50Gj9Pl0NERET/h3Zpw35sR1NT099ev8nvdiEiIiJVMXwQERGRqhg+iIiISFUMH0RERKQqhg8iIiJSFcMHERERqYrhg4iIiFTF8EFERESqYvggIiIiVTF8EBERkaq69d0uauj8tPd2tAF96oPfiYiI6Fba0Qbgv/8fv50+Fz6cTicAoAi7PFwJERERdZfT6YTJZLrtnD73xXIulwtVVVWIiYnBmTNn/vbLabyRw+FAeHi4z/YPcA18vX+Aa+Dr/QNcA6B/rYGIwOl0IiwsDFrt7a/q6HOvfGi1WgwfPhwAYDQa+/xi9yZf7x/gGvh6/wDXwNf7B7gGQP9Zg797xaMTLzglIiIiVTF8EBERkar6ZPgwGAzIzMyEwWDwdCke4ev9A1wDX+8f4Br4ev8A1wDw3jXocxecEhERkXfrk698EBERkfdi+CAiIiJVMXwQERGRqhg+iIiISFUMH0RERKSqPhc+1q9fj6ioKAwcOBBWqxUHDhzwdEk95scff8Ts2bMRFhYGjUaDbdu2uY2LCNasWYPQ0FD4+/vDbrejurrabc7ly5eRkpICo9EIs9mMRYsWobm5WcUu/rmsrCxMnjwZgwcPRnBwMB599FFUVVW5zbl27RrS0tIwbNgwBAQEYO7cuaivr3ebU1tbi1mzZmHQoEEIDg7Giy++iPb2djVb+Ueys7MRGxurfFKhzWbD7t27lXFv7v1m1q1bB41GgxUrVij7vH0N1q5dC41G47ZFR0cr497ef6ezZ8/iySefxLBhw+Dv74+JEyfi4MGDyri3nwujoqK6HAcajQZpaWkAfOQ4kD4kNzdX9Hq9fPrpp3L06FF5+umnxWw2S319vadL6xG7du2Sl19+Wb755hsBIFu3bnUbX7dunZhMJtm2bZv8+uuv8sgjj8jIkSPl6tWrypyHHnpI4uLipKSkRH766ScZPXq0JCcnq9zJP5OYmCg5OTlSWVkpFRUV8vDDD0tERIQ0NzcrcxYvXizh4eFSUFAgBw8elKlTp8p9992njLe3t8uECRPEbrdLeXm57Nq1SwIDAyUjI8MTLXXLjh075LvvvpPjx49LVVWVvPTSS+Ln5yeVlZUi4t293+jAgQMSFRUlsbGxsnz5cmW/t69BZmamjB8/Xs6fP69sFy5cUMa9vX8RkcuXL0tkZKTMnz9fSktL5dSpU7Jnzx45ceKEMsfbz4UNDQ1ux0B+fr4AkH379omIbxwHfSp8TJkyRdLS0pTfOzo6JCwsTLKysjxYVe+4MXy4XC6xWCzyxhtvKPsaGxvFYDDIl19+KSIix44dEwDyyy+/KHN2794tGo1Gzp49q1rtPaWhoUEASGFhoYhc79fPz0+2bNmizPntt98EgBQXF4vI9QCn1Wqlrq5OmZOdnS1Go1FaWlrUbaAHDBkyRD755BOf6t3pdMqYMWMkPz9f7r//fiV8+MIaZGZmSlxc3E3HfKF/EZFVq1bJ9OnTbznui+fC5cuXy1133SUul8tnjoM+87ZLa2srysrKYLfblX1arRZ2ux3FxcUerEwdNTU1qKurc+vfZDLBarUq/RcXF8NsNiMhIUGZY7fbodVqUVpaqnrNd6qpqQkAMHToUABAWVkZ2tra3NYgOjoaERERbmswceJEhISEKHMSExPhcDhw9OhRFau/Mx0dHcjNzcWVK1dgs9l8qve0tDTMmjXLrVfAd/77V1dXIywsDKNGjUJKSgpqa2sB+E7/O3bsQEJCAp544gkEBwdj0qRJ+Pjjj5VxXzsXtra2YtOmTVi4cCE0Go3PHAd9JnxcvHgRHR0dbosJACEhIairq/NQVerp7PF2/dfV1SE4ONhtXKfTYejQof1ujVwuF1asWIFp06ZhwoQJAK73p9frYTab3ebeuAY3W6POsb7uyJEjCAgIgMFgwOLFi7F161bExMT4RO8AkJubi0OHDiErK6vLmC+sgdVqxcaNG5GXl4fs7GzU1NRgxowZcDqdPtE/AJw6dQrZ2dkYM2YM9uzZgyVLluC5557DZ599BsD3zoXbtm1DY2Mj5s+fD8A3/h0AgM7TBZBvSktLQ2VlJYqKijxdiqruvvtuVFRUoKmpCV9//TVSU1NRWFjo6bJUcebMGSxfvhz5+fkYOHCgp8vxiKSkJOXn2NhYWK1WREZGYvPmzfD39/dgZepxuVxISEjA66+/DgCYNGkSKisr8eGHHyI1NdXD1alvw4YNSEpKQlhYmKdLUVWfeeUjMDAQAwYM6HJFb319PSwWi4eqUk9nj7fr32KxoKGhwW28vb0dly9f7ldrtHTpUnz77bfYt28fRowYoey3WCxobW1FY2Oj2/wb1+Bma9Q51tfp9XqMHj0a8fHxyMrKQlxcHN555x2f6L2srAwNDQ249957odPpoNPpUFhYiHfffRc6nQ4hISFevwY3MpvNGDt2LE6cOOETxwAAhIaGIiYmxm3fuHHjlLeffOlcePr0afzwww946qmnlH2+chz0mfCh1+sRHx+PgoICZZ/L5UJBQQFsNpsHK1PHyJEjYbFY3Pp3OBwoLS1V+rfZbGhsbERZWZkyZ+/evXC5XLBararX3F0igqVLl2Lr1q3Yu3cvRo4c6TYeHx8PPz8/tzWoqqpCbW2t2xocOXLE7cSTn58Po9HY5YTWH7hcLrS0tPhE7zNnzsSRI0dQUVGhbAkJCUhJSVF+9vY1uFFzczNOnjyJ0NBQnzgGAGDatGldbrE/fvw4IiMjAfjGubBTTk4OgoODMWvWLGWfrxwHfepul9zcXDEYDLJx40Y5duyYPPPMM2I2m92u6O3PnE6nlJeXS3l5uQCQN998U8rLy+X06dMicv32MrPZLNu3b5fDhw/LnDlzbnp72aRJk6S0tFSKiopkzJgx/eb2siVLlojJZJL9+/e73Wb2119/KXMWL14sERERsnfvXjl48KDYbDax2WzKeOctZg8++KBUVFRIXl6eBAUF9YtbzFavXi2FhYVSU1Mjhw8fltWrV4tGo5Hvv/9eRLy791v537tdRLx/DVauXCn79++Xmpoa+fnnn8Vut0tgYKA0NDSIiPf3L3L9NmudTievvfaaVFdXy+effy6DBg2STZs2KXO8/Vwocv1uzoiICFm1alWXMV84DvpU+BARee+99yQiIkL0er1MmTJFSkpKPF1Sj9m3b58A6LKlpqaKyPVbzF555RUJCQkRg8EgM2fOlKqqKrfHuHTpkiQnJ0tAQIAYjUZZsGCBOJ1OD3TTfTfrHYDk5OQoc65evSrPPvusDBkyRAYNGiSPPfaYnD9/3u1x/vjjD0lKShJ/f38JDAyUlStXSltbm8rddN/ChQslMjJS9Hq9BAUFycyZM5XgIeLdvd/KjeHD29dg3rx5EhoaKnq9XoYPHy7z5s1z+3wLb++/086dO2XChAliMBgkOjpaPvroI7dxbz8Xiojs2bNHAHTpS8Q3jgONiIhHXnIhIiIin9RnrvkgIiIi38DwQURERKpi+CAiIiJVMXwQERGRqhg+iIiISFUMH0RERKQqhg8iIiJSFcMHERERqYrhg4iIiFTF8EFERESqYvggIiIiVf0bMCaeJiUgBP8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(torch.abs(encoder_input - hf_outputs[0]).numpy()[0] > 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b76dc23",
   "metadata": {},
   "source": [
    "## Verify Transformer encoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47c86f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it with the original's speech prenet input:\n",
    "with torch.no_grad():\n",
    "    encoder_output = orig_model.encoder(encoder_input, encoder_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b8b4df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run it with our input, which is slightly different (see above)\n",
    "# with torch.no_grad():\n",
    "#     encoder_output = orig_model.encoder(hf_encoder_input, ~hf_encoder_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41ff76cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_out shape torch.Size([79, 1, 768])\n",
      "encoder_padding_mask shape torch.Size([1, 79])\n",
      "encoder_states []\n",
      "src_tokens []\n",
      "decoder_input [None]\n",
      "encoder_out_for_ctc shape torch.Size([79, 1, 81])\n"
     ]
    }
   ],
   "source": [
    "print(\"encoder_out shape\", encoder_output[\"encoder_out\"][0].shape)\n",
    "print(\"encoder_padding_mask shape\", encoder_output[\"encoder_padding_mask\"][0].shape)\n",
    "print(\"encoder_states\", encoder_output[\"encoder_states\"])  # []\n",
    "print(\"src_tokens\", encoder_output[\"src_tokens\"])  # []\n",
    "print(\"decoder_input\", encoder_output[\"decoder_input\"])  # [None]\n",
    "print(\"encoder_out_for_ctc shape\", encoder_output[\"encoder_out_for_ctc\"][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a38979d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1216, -0.1223, -0.0161,  ...,  0.1346,  0.0016, -0.0969],\n",
       "         [-0.4308,  0.2260,  0.2156,  ..., -0.1011,  0.0140, -0.4570],\n",
       "         [-0.5038,  0.2883,  0.1684,  ...,  0.0300, -0.0678, -0.4736],\n",
       "         ...,\n",
       "         [ 0.4783, -0.1476, -0.0570,  ..., -0.2642, -0.6181,  0.2321],\n",
       "         [ 0.4313, -0.2115, -0.1195,  ..., -0.4216, -0.4153,  0.2950],\n",
       "         [ 0.0910, -0.0379,  0.1903,  ...,  0.0070, -0.1593, -0.0444]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[\"encoder_out\"][0].permute((1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9b9dda4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0323,  0.0410,  0.0620,  ...,  0.0133,  0.0218, -0.0187],\n",
       "         [ 0.0523, -0.0873, -0.0304,  ..., -0.0184, -0.0473,  0.0077],\n",
       "         [ 0.0937, -0.0637, -0.0121,  ..., -0.0029, -0.0065, -0.0139],\n",
       "         ...,\n",
       "         [-0.1147,  0.0520,  0.0286,  ...,  0.0440,  0.0449,  0.1282],\n",
       "         [-0.1129,  0.0273, -0.0115,  ...,  0.0530,  0.0472,  0.1208],\n",
       "         [ 0.0075, -0.0139,  0.0873,  ...,  0.0442, -0.0180, -0.0587]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[\"encoder_out_for_ctc\"][0].permute((1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cde6dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use exact same inputs as the original model:\n",
    "# with torch.no_grad():\n",
    "#      hf_outputs = hf_model.speecht5(\n",
    "#          inputs_embeds=encoder_input,\n",
    "#          attention_mask=(~encoder_padding_mask),\n",
    "#      )\n",
    "\n",
    "# type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7089f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 79, 768])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_encoder_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "28f529a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutput"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.encoder.wrapped_encoder(\n",
    "         hidden_states=hf_encoder_input,\n",
    "         attention_mask=torch.ones_like(src_tokens),\n",
    "         return_dict=True,\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb12e71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[ 0.1216, -0.1223, -0.0161,  ...,  0.1346,  0.0016, -0.0969],\n",
       "         [-0.4308,  0.2260,  0.2156,  ..., -0.1011,  0.0140, -0.4570],\n",
       "         [-0.5038,  0.2883,  0.1684,  ...,  0.0300, -0.0678, -0.4736],\n",
       "         ...,\n",
       "         [ 0.4783, -0.1476, -0.0570,  ..., -0.2642, -0.6181,  0.2321],\n",
       "         [ 0.4313, -0.2115, -0.1195,  ..., -0.4216, -0.4153,  0.2950],\n",
       "         [ 0.0910, -0.0379,  0.1903,  ...,  0.0070, -0.1593, -0.0444]]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06fb675b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_hidden_state']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8fe9914e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 79, 768])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69eda6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1216, -0.1223, -0.0161,  ...,  0.1346,  0.0016, -0.0969],\n",
       "         [-0.4308,  0.2260,  0.2156,  ..., -0.1011,  0.0140, -0.4570],\n",
       "         [-0.5038,  0.2883,  0.1684,  ...,  0.0300, -0.0678, -0.4736],\n",
       "         ...,\n",
       "         [ 0.4783, -0.1476, -0.0570,  ..., -0.2642, -0.6181,  0.2321],\n",
       "         [ 0.4313, -0.2115, -0.1195,  ..., -0.4216, -0.4153,  0.2950],\n",
       "         [ 0.0910, -0.0379,  0.1903,  ...,  0.0070, -0.1593, -0.0444]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a747e547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(encoder_output[\"encoder_out\"][0].permute((1, 0, 2)) - hf_outputs[\"last_hidden_state\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a956189",
   "metadata": {},
   "source": [
    "## Full encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "50670bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    encoder_out = orig_model.forward_text_encoder(src_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "651a1a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1216, -0.1223, -0.0161,  ...,  0.1346,  0.0016, -0.0969],\n",
       "         [-0.4308,  0.2260,  0.2156,  ..., -0.1011,  0.0140, -0.4570],\n",
       "         [-0.5038,  0.2883,  0.1684,  ...,  0.0300, -0.0678, -0.4736],\n",
       "         ...,\n",
       "         [ 0.4783, -0.1476, -0.0570,  ..., -0.2642, -0.6181,  0.2321],\n",
       "         [ 0.4313, -0.2115, -0.1195,  ..., -0.4216, -0.4153,  0.2950],\n",
       "         [ 0.0910, -0.0379,  0.1903,  ...,  0.0070, -0.1593, -0.0444]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[\"encoder_out\"][0].permute((1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "845b24a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutput"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.encoder(\n",
    "         input_values=src_tokens,\n",
    "         attention_mask=torch.ones_like(src_tokens),\n",
    "         return_dict=True,\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f02b64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(encoder_output[\"encoder_out\"][0].permute((1, 0, 2)) - hf_outputs[\"last_hidden_state\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b1688e",
   "metadata": {},
   "source": [
    "## Speaker embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12cac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#         if spkembs is not None and self.spk_embed_integration_type != \"pre\":\n",
    "#             encoder_out[\"encoder_out\"] = [self._integrate_with_spk_embed(\n",
    "#                 encoder_out[\"encoder_out\"][0].transpose(0, 1), spkembs\n",
    "#             ).transpose(0, 1)]\n",
    "#             spkembs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40681457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decaaa43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c68a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#         threshold = kwargs.get(\"threshold\", 0.5)\n",
    "#         minlenratio = kwargs.get(\"threshold\", 0.0)\n",
    "\n",
    "#         if source is None:\n",
    "#             assert src_tokens.size(0) == 1\n",
    "#             encoder_out = self.forward_text_encoder(src_tokens)\n",
    "#             maxlenratio = kwargs.get(\"threshold\", 20.0)\n",
    "#         else:\n",
    "#             assert source.size(0) == 1\n",
    "#             encoder_out = self.forward_encoder(source, padding_mask=kwargs[\"padding_mask\"])\n",
    "#             maxlenratio = kwargs.get(\"threshold\", 10.0)\n",
    "\n",
    "#         if spkembs is not None and self.spk_embed_integration_type != \"pre\":\n",
    "#             encoder_out[\"encoder_out\"] = [self._integrate_with_spk_embed(\n",
    "#                 encoder_out[\"encoder_out\"][0].transpose(0, 1), spkembs\n",
    "#             ).transpose(0, 1)]\n",
    "#             spkembs = None\n",
    "\n",
    "#         maxlen = int(encoder_out[\"encoder_out\"][0].size(0) * maxlenratio / self.reduction_factor)\n",
    "#         minlen = int(encoder_out[\"encoder_out\"][0].size(0) * minlenratio / self.reduction_factor)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17bb840",
   "metadata": {},
   "source": [
    "## Verify speech decoder prenet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9ba48d",
   "metadata": {},
   "source": [
    "First this calls `speech_decoder_prenet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3acc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#         idx = 0\n",
    "#         ys = encoder_out[\"encoder_out\"][0].new_zeros(1, 1, self.speech_decoder_postnet.odim)\n",
    "#         outs, probs = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49421f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#             decoder_in, _ = self.speech_decoder_prenet(ys, spkembs=spkembs)\n",
    "#             z, extra = self.decoder(decoder_in[:,-1:], None, encoder_out, incremental_states, alignment_layer=-1)\n",
    "#             outs += [self.speech_decoder_postnet.feat_out(z[0, -1]).view(self.reduction_factor, self.speech_decoder_postnet.odim)]  # [(r, odim), ...]\n",
    "#             probs += [torch.sigmoid(self.speech_decoder_postnet.prob_out(z[0, -1]))]  # [(r), ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3747a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = inputs.input_values.size(0)\n",
    "beam_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dcb259",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.tensor([2, 4, 46, 16, 12, 16] * beam_size * batch_size).reshape(beam_size * batch_size, -1)\n",
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bd8179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one has padding (token_id = 1)\n",
    "# The results will be different with the HF implementation because\n",
    "# we don't set the attention_mask to 0 for padding tokens\n",
    "# tokens = torch.tensor([2, 4, 46, 16, 1, 12] * beam_size * batch_size).reshape(beam_size * batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09f744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    prev_output_tokens, tgt_mask, incremental_state = orig_model.text_decoder_prenet(tokens, incremental_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cc6f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This returns a sequence length of 1\n",
    "# with torch.no_grad():\n",
    "#     prev_output_tokens, tgt_mask, incremental_state = orig_model.text_decoder_prenet(tokens, incremental_state={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725cfc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: misleading name; these are not the actual tokens but their embeddings!\n",
    "prev_output_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84764c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81026d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77134734",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.decoder.prenet(\n",
    "         input_ids=tokens,\n",
    "#          attention_mask=hf_encoder_attention_mask,\n",
    "#          attention_mask=inputs.attention_mask,\n",
    "#          past_key_values=[(torch.ones(5, 1, 1), torch.ones(5, 1, 1))],\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f007852",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeds_hf, decoder_attention_mask = hf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4dd4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeds_hf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f6193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(torch.abs(prev_output_tokens - token_embeds_hf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a827caef",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8d7a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fec354d",
   "metadata": {},
   "source": [
    "## Verify Transformer decoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a4fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = source.size(0)\n",
    "new_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)\n",
    "encoder_outs = orig_model.encoder.reorder_encoder_out(encoder_output, new_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98652fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6229b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    decoder_output, extra = orig_model.decoder(\n",
    "        prev_output_tokens,\n",
    "        tgt_mask,\n",
    "        encoder_out=encoder_outs,\n",
    "        incremental_state=incremental_state,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a190958",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea0351e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05117956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[x.shape for x in extra[\"attn\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b593692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[x.shape for x in extra[\"inner_states\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d486f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.decoder.wrapped_decoder(\n",
    "         hidden_states=prev_output_tokens,\n",
    "         attention_mask=decoder_attention_mask,\n",
    "         encoder_hidden_states=encoder_outs[\"encoder_out\"][0].permute((1, 0, 2)),\n",
    "         encoder_attention_mask=hf_encoder_attention_mask.repeat((1, beam_size)).view(beam_size * batch_size, -1),\n",
    "         return_dict=True,\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea512cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82c369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs[\"last_hidden_state\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6282bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs[\"last_hidden_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e254cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(torch.abs(decoder_output - hf_outputs[\"last_hidden_state\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d3d1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d9113f7",
   "metadata": {},
   "source": [
    "## Verify speech decoder postnet output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20be43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    lprobs = orig_model.text_decoder_postnet(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a143bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lprobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6de14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lprobs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7571e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    lprobs_hf = hf_model.text_decoder_postnet(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5623a791",
   "metadata": {},
   "outputs": [],
   "source": [
    "lprobs_hf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f0c976",
   "metadata": {},
   "outputs": [],
   "source": [
    "lprobs_hf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeff463",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(torch.abs(lprobs - lprobs_hf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24926eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32e98799",
   "metadata": {},
   "source": [
    "## Use the `transformers` generator loop:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c4e9dc",
   "metadata": {},
   "source": [
    "Run the full model to make sure this doesn't give any errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b889ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model(\n",
    "         input_values=inputs.input_values,\n",
    "         attention_mask=inputs.attention_mask,\n",
    "         decoder_input_ids=torch.tensor([[3, 4, 5]]),\n",
    "         #decoder_input_ids=torch.tensor([[3, 4, 5], [2, 2, 2]]),  # batch\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677718c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2bcec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d1eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "baf0244b",
   "metadata": {},
   "source": [
    "Also calculate loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7bd1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model(\n",
    "         input_values=inputs.input_values,\n",
    "         attention_mask=inputs.attention_mask,\n",
    "         #decoder_input_ids=torch.tensor([[2,  4, 18, 10, 12,  6,  5]]),\n",
    "         labels=torch.tensor([[4, 18, 10, 12,  6,  5, 13]]),\n",
    "         #labels=torch.tensor([[4, 18, 10, 12,  6,  5, 13], [4, 18, 10, 12,  6,  5, 13]]),  # batch\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34dd766",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edf2044",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195961eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f2ce14a",
   "metadata": {},
   "source": [
    "Generator loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ccdbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.input_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fc2baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs = hf_model.generate(inputs.input_values, max_length=100)\n",
    "# hf_outputs = hf_model.generate(inputs.input_values, num_beams=5, max_length=100) #, bos_token_id=2)\n",
    "# hf_outputs = hf_model.generate(torch.rand(1, 10000), num_beams=5, max_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015c80b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7e1899",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fce9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(hf_outputs.shape[0]):\n",
    "    print(tgt_dict.string(hf_outputs[i]))\n",
    "    print(tokenizer.decode(tgt_dict.string(hf_outputs[i])))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997fd48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[tgt_dict[x] for x in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d72119a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a116f698",
   "metadata": {},
   "source": [
    "For comparison, Speech2Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00306c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
    "\n",
    "s2t_model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "s2t_processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f994828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "inputs = s2t_processor(\n",
    "    ds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], return_tensors=\"pt\"\n",
    ")\n",
    "input_features = inputs.input_features\n",
    "input_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8ef5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test batch\n",
    "# input_features = torch.tile(input_features, dims=(2, 1, 1))\n",
    "# input_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf06a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = s2t_model.generate(inputs=input_features)\n",
    "\n",
    "transcription = s2t_processor.batch_decode(generated_ids)[0]\n",
    "transcription\n",
    "\n",
    "#'mister quilter is the apostle of the middle classes and we are glad to welcome his gospel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a21206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Speech2TextModel\n",
    "\n",
    "s2t_model = Speech2TextModel.from_pretrained(\"facebook/s2t-small-librispeech-asr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49667e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d50884f2",
   "metadata": {},
   "source": [
    "Test other methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd38a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BART, Speech2Text, Wav2Vec2 don't have pruning\n",
    "#hf_model.prune_heads({1: [0, 2], 2: [2,3 ]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc11e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9778431c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be3e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.get_output_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f954bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.resize_token_embeddings(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b17c7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742e8369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0974c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b945156",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_ctc.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c092a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_naked.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2280ef5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
